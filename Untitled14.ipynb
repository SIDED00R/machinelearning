{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeDcPkdcM/ivH1zXxBS7B0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqc9qhDuR1WI",
        "outputId": "9f756397-74a2-42ae-932f-a983bf0d786c"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('resnet')\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "path2data = '/content/resnet/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "# load dataset\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
        "        '''\n",
        "        block: 'BasicBlock' or 'BottleNeck'\n",
        "        num_block: [n, n, n, n]\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels=64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        x = self.conv3_x(output)\n",
        "        x = self.conv4_x(x)\n",
        "        x = self.conv5_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model=ResNet(BottleNeck,[3,8,36,3])\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at resnet\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch: 1 | batch staus: 0/5000 ( 0%) | Loss:  2.331322\n",
            "train epoch: 1 | batch staus: 640/5000 ( 13%) | Loss:  4.031568\n",
            "train epoch: 1 | batch staus: 1280/5000 ( 25%) | Loss:  2.743551\n",
            "train epoch: 1 | batch staus: 1920/5000 ( 38%) | Loss:  3.310320\n",
            "train epoch: 1 | batch staus: 2560/5000 ( 51%) | Loss:  3.203821\n",
            "train epoch: 1 | batch staus: 3200/5000 ( 63%) | Loss:  3.438390\n",
            "train epoch: 1 | batch staus: 3840/5000 ( 76%) | Loss:  3.435224\n",
            "train epoch: 1 | batch staus: 4480/5000 ( 89%) | Loss:  2.590989\n",
            "Training time: 1m 11s\n",
            "=======\n",
            " test set: average loss:  0.0545, Accuracy: 1377/8000(17%)\n",
            "Testing time: 1m 46s\n",
            "train epoch: 2 | batch staus: 0/5000 ( 0%) | Loss:  2.590879\n",
            "train epoch: 2 | batch staus: 640/5000 ( 13%) | Loss:  2.258070\n",
            "train epoch: 2 | batch staus: 1280/5000 ( 25%) | Loss:  2.535842\n",
            "train epoch: 2 | batch staus: 1920/5000 ( 38%) | Loss:  2.247748\n",
            "train epoch: 2 | batch staus: 2560/5000 ( 51%) | Loss:  2.297128\n",
            "train epoch: 2 | batch staus: 3200/5000 ( 63%) | Loss:  2.091692\n",
            "train epoch: 2 | batch staus: 3840/5000 ( 76%) | Loss:  2.392713\n",
            "train epoch: 2 | batch staus: 4480/5000 ( 89%) | Loss:  2.223287\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0540, Accuracy: 1415/8000(18%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 3 | batch staus: 0/5000 ( 0%) | Loss:  3.257495\n",
            "train epoch: 3 | batch staus: 640/5000 ( 13%) | Loss:  2.540758\n",
            "train epoch: 3 | batch staus: 1280/5000 ( 25%) | Loss:  1.943455\n",
            "train epoch: 3 | batch staus: 1920/5000 ( 38%) | Loss:  2.090447\n",
            "train epoch: 3 | batch staus: 2560/5000 ( 51%) | Loss:  1.962317\n",
            "train epoch: 3 | batch staus: 3200/5000 ( 63%) | Loss:  2.062810\n",
            "train epoch: 3 | batch staus: 3840/5000 ( 76%) | Loss:  2.013297\n",
            "train epoch: 3 | batch staus: 4480/5000 ( 89%) | Loss:  1.984980\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0541, Accuracy: 1918/8000(24%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 4 | batch staus: 0/5000 ( 0%) | Loss:  2.820733\n",
            "train epoch: 4 | batch staus: 640/5000 ( 13%) | Loss:  2.194956\n",
            "train epoch: 4 | batch staus: 1280/5000 ( 25%) | Loss:  2.102003\n",
            "train epoch: 4 | batch staus: 1920/5000 ( 38%) | Loss:  1.885089\n",
            "train epoch: 4 | batch staus: 2560/5000 ( 51%) | Loss:  1.720760\n",
            "train epoch: 4 | batch staus: 3200/5000 ( 63%) | Loss:  1.965590\n",
            "train epoch: 4 | batch staus: 3840/5000 ( 76%) | Loss:  1.839429\n",
            "train epoch: 4 | batch staus: 4480/5000 ( 89%) | Loss:  1.828427\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0614, Accuracy: 1552/8000(19%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 5 | batch staus: 0/5000 ( 0%) | Loss:  2.948933\n",
            "train epoch: 5 | batch staus: 640/5000 ( 13%) | Loss:  2.111667\n",
            "train epoch: 5 | batch staus: 1280/5000 ( 25%) | Loss:  2.314490\n",
            "train epoch: 5 | batch staus: 1920/5000 ( 38%) | Loss:  1.964667\n",
            "train epoch: 5 | batch staus: 2560/5000 ( 51%) | Loss:  1.857520\n",
            "train epoch: 5 | batch staus: 3200/5000 ( 63%) | Loss:  1.965981\n",
            "train epoch: 5 | batch staus: 3840/5000 ( 76%) | Loss:  1.857432\n",
            "train epoch: 5 | batch staus: 4480/5000 ( 89%) | Loss:  1.928068\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0312, Accuracy: 2085/8000(26%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 6 | batch staus: 0/5000 ( 0%) | Loss:  1.832222\n",
            "train epoch: 6 | batch staus: 640/5000 ( 13%) | Loss:  1.890592\n",
            "train epoch: 6 | batch staus: 1280/5000 ( 25%) | Loss:  1.619859\n",
            "train epoch: 6 | batch staus: 1920/5000 ( 38%) | Loss:  1.707371\n",
            "train epoch: 6 | batch staus: 2560/5000 ( 51%) | Loss:  1.605989\n",
            "train epoch: 6 | batch staus: 3200/5000 ( 63%) | Loss:  1.887966\n",
            "train epoch: 6 | batch staus: 3840/5000 ( 76%) | Loss:  1.676167\n",
            "train epoch: 6 | batch staus: 4480/5000 ( 89%) | Loss:  1.762562\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0326, Accuracy: 2408/8000(30%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 7 | batch staus: 0/5000 ( 0%) | Loss:  2.137860\n",
            "train epoch: 7 | batch staus: 640/5000 ( 13%) | Loss:  1.732152\n",
            "train epoch: 7 | batch staus: 1280/5000 ( 25%) | Loss:  1.909194\n",
            "train epoch: 7 | batch staus: 1920/5000 ( 38%) | Loss:  1.576883\n",
            "train epoch: 7 | batch staus: 2560/5000 ( 51%) | Loss:  1.732866\n",
            "train epoch: 7 | batch staus: 3200/5000 ( 63%) | Loss:  1.671412\n",
            "train epoch: 7 | batch staus: 3840/5000 ( 76%) | Loss:  1.611624\n",
            "train epoch: 7 | batch staus: 4480/5000 ( 89%) | Loss:  1.726112\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0286, Accuracy: 2311/8000(29%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 8 | batch staus: 0/5000 ( 0%) | Loss:  1.544107\n",
            "train epoch: 8 | batch staus: 640/5000 ( 13%) | Loss:  1.741370\n",
            "train epoch: 8 | batch staus: 1280/5000 ( 25%) | Loss:  1.583126\n",
            "train epoch: 8 | batch staus: 1920/5000 ( 38%) | Loss:  1.594024\n",
            "train epoch: 8 | batch staus: 2560/5000 ( 51%) | Loss:  1.584832\n",
            "train epoch: 8 | batch staus: 3200/5000 ( 63%) | Loss:  1.812343\n",
            "train epoch: 8 | batch staus: 3840/5000 ( 76%) | Loss:  1.680972\n",
            "train epoch: 8 | batch staus: 4480/5000 ( 89%) | Loss:  1.446429\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0385, Accuracy: 1969/8000(25%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 9 | batch staus: 0/5000 ( 0%) | Loss:  1.965502\n",
            "train epoch: 9 | batch staus: 640/5000 ( 13%) | Loss:  1.729743\n",
            "train epoch: 9 | batch staus: 1280/5000 ( 25%) | Loss:  1.590187\n",
            "train epoch: 9 | batch staus: 1920/5000 ( 38%) | Loss:  1.539933\n",
            "train epoch: 9 | batch staus: 2560/5000 ( 51%) | Loss:  1.407260\n",
            "train epoch: 9 | batch staus: 3200/5000 ( 63%) | Loss:  1.781002\n",
            "train epoch: 9 | batch staus: 3840/5000 ( 76%) | Loss:  1.639080\n",
            "train epoch: 9 | batch staus: 4480/5000 ( 89%) | Loss:  1.720285\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0321, Accuracy: 2209/8000(28%)\n",
            "Testing time: 1m 46s\n",
            "total time: 15m  50s \n",
            " model was trained on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "KhZeU89PoMlx",
        "outputId": "b7466491-d1dc-4055-d999-98fe8b03761a"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('inceptionv4')\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "path2data = '/content/inceptionv4/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # bias=Fasle, because BN after conv includes bias.\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            BasicConv2d(3, 32, 3, stride=2, padding=0), # 149 x 149 x 32\n",
        "            BasicConv2d(32, 32, 3, stride=1, padding=0), # 147 x 147 x 32\n",
        "            BasicConv2d(32, 64, 3, stride=1, padding=1), # 147 x 147 x 64 \n",
        "        )\n",
        "\n",
        "        self.branch3x3_conv = BasicConv2d(64, 96, 3, stride=2, padding=0) # 73x73x96\n",
        "\n",
        "        #  kernel_size=4: 피쳐맵 크기 73, kernel_size=3: 피쳐맵 크기 74\n",
        "        self.branch3x3_pool = nn.MaxPool2d(4, stride=2, padding=1) # 73x73x64\n",
        "\n",
        "        self.branch7x7a = nn.Sequential(\n",
        "            BasicConv2d(160, 64, 1, stride=1, padding=0),\n",
        "            BasicConv2d(64, 96, 3, stride=1, padding=0)\n",
        "        ) # 71x71x96\n",
        "\n",
        "        self.branch7x7b = nn.Sequential(\n",
        "            BasicConv2d(160, 64, 1, stride=1, padding=0),\n",
        "            BasicConv2d(64, 64, (7,1), stride=1, padding=(3,0)),\n",
        "            BasicConv2d(64, 64, (1,7), stride=1, padding=(0,3)),\n",
        "            BasicConv2d(64, 96, 3, stride=1, padding=0)\n",
        "        ) # 71x71x96\n",
        "\n",
        "        self.branchpoola = BasicConv2d(192, 192, 3, stride=2, padding=0) # 35x35x192\n",
        "\n",
        "        #  kernel_size=4: 피쳐맵 크기 73, kernel_size=3: 피쳐맵 크기 74\n",
        "        self.branchpoolb = nn.MaxPool2d(4, 2, 1) # 35x35x192\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.cat((self.branch3x3_conv(x), self.branch3x3_pool(x)), dim=1)\n",
        "        x = torch.cat((self.branch7x7a(x), self.branch7x7b(x)), dim=1)\n",
        "        x = torch.cat((self.branchpoola(x), self.branchpoolb(x)), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Inception_Resnet_A(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 32, 1, stride=1, padding=0)\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, 1, stride=1, padding=0),\n",
        "            BasicConv2d(32, 32, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, 1, stride=1, padding=0),\n",
        "            BasicConv2d(32, 48, 3, stride=1, padding=1),\n",
        "            BasicConv2d(48, 64, 3, stride=1, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.reduction1x1 = nn.Conv2d(128, 384, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 384, 1, stride=1, padding=0)\n",
        "        self.bn = nn.BatchNorm2d(384)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch3x3(x), self.branch3x3stack(x)), dim=1)\n",
        "        x = self.reduction1x1(x)\n",
        "        x = self.bn(x_shortcut + x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Inception_Resnet_B(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 192, 1, stride=1, padding=0)\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 128, 1, stride=1, padding=0),\n",
        "            BasicConv2d(128, 160, (1,7), stride=1, padding=(0,3)),\n",
        "            BasicConv2d(160, 192, (7,1), stride=1, padding=(3,0))\n",
        "        )\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(384, 1152, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 1152, 1, stride=1, padding=0)\n",
        "        self.bn = nn.BatchNorm2d(1152)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch7x7(x)), dim=1)\n",
        "        x = self.reduction1x1(x) * 0.1\n",
        "        x = self.bn(x + x_shortcut)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Inception_Resnet_C(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 192, 1, stride=1, padding=0)\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 192, 1, stride=1, padding=0),\n",
        "            BasicConv2d(192, 224, (1,3), stride=1, padding=(0,1)),\n",
        "            BasicConv2d(224, 256, (3,1), stride=1, padding=(1,0))\n",
        "        )\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(448, 2144, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 2144, 1, stride=1, padding=0) # 2144\n",
        "        self.bn = nn.BatchNorm2d(2144)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch3x3(x)), dim=1)\n",
        "        x = self.reduction1x1(x) * 0.1\n",
        "        x = self.bn(x_shortcut + x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class ReductionA(nn.Module):\n",
        "    def __init__(self, in_channels, k, l, m, n):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(3, 2)\n",
        "        self.branch3x3 = BasicConv2d(in_channels, n, 3, stride=2, padding=0)\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, k, 1, stride=1, padding=0),\n",
        "            BasicConv2d(k, l, 3, stride=1, padding=1),\n",
        "            BasicConv2d(l, m, 3, stride=2, padding=0)\n",
        "        )\n",
        "\n",
        "        self.output_channels = in_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.branchpool(x), self.branch3x3(x), self.branch3x3stack(x)), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReductionB(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(3, 2)\n",
        "        self.branch3x3a = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 384, 3, stride=2, padding=0)\n",
        "        )\n",
        "        self.branch3x3b = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 288, 3, stride=2, padding=0)\n",
        "        )\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 288, 3, stride=1, padding=1),\n",
        "            BasicConv2d(288, 320, 3, stride=2, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.branchpool(x), self.branch3x3a(x), self.branch3x3b(x), self.branch3x3stack(x)), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionResNetV2(nn.Module):\n",
        "    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        blocks.append(Stem())\n",
        "        for i in range(A):\n",
        "            blocks.append(Inception_Resnet_A(384))\n",
        "        blocks.append(ReductionA(384, k, l, m, n))\n",
        "        for i in range(B):\n",
        "            blocks.append(Inception_Resnet_B(1152))\n",
        "        blocks.append(ReductionB(1152))\n",
        "        for i in range(C):\n",
        "            blocks.append(Inception_Resnet_C(2144))\n",
        "\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        # drop out\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.linear = nn.Linear(2144, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    # define weight initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "model = InceptionResNetV2(10, 20, 10).to(device)\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 8):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at inceptionv4; to attempt to forcibly remount, call drive.mount(\"inceptionv4\", force_remount=True).\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-329903366c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-329903366c35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mouput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-329903366c35>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-329903366c35>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranchpoola\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranchpoolb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Got 10 and 9 (The offending index is 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7miS1Urpd0",
        "outputId": "5bf9712b-ff46-46ae-9a28-0154717d86b8"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('densenet')\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "path2data = '/content/densenet/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class BottleNeck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        inner_channels = 4 * growth_rate\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(inner_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n",
        "\n",
        "\n",
        "# Transition Block: reduce feature map size and number of channels\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "\n",
        "# DenseNet\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.growth_rate = growth_rate\n",
        "        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, inner_channels, 7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(3, 2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for i in range(len(nblocks)-1):\n",
        "            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n",
        "            inner_channels += growth_rate * nblocks[i]\n",
        "            out_channels = int(reduction * inner_channels)\n",
        "            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels \n",
        "        \n",
        "        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('relu', nn.ReLU())\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(inner_channels, num_classes)\n",
        "\n",
        "        # weight initialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.features(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def _make_dense_block(self, nblock, inner_channels):\n",
        "        dense_block = nn.Sequential()\n",
        "        for i in range(nblock):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n",
        "            inner_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def DenseNet_121():\n",
        "    return DenseNet([6, 12, 24, 6])\n",
        "model = DenseNet_121()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at densenet; to attempt to forcibly remount, call drive.mount(\"densenet\", force_remount=True).\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train epoch: 1 | batch staus: 0/5000 ( 0%) | Loss:  2.308819\n",
            "train epoch: 1 | batch staus: 640/5000 ( 13%) | Loss:  2.285026\n",
            "train epoch: 1 | batch staus: 1280/5000 ( 25%) | Loss:  2.239379\n",
            "train epoch: 1 | batch staus: 1920/5000 ( 38%) | Loss:  2.206953\n",
            "train epoch: 1 | batch staus: 2560/5000 ( 51%) | Loss:  2.212089\n",
            "train epoch: 1 | batch staus: 3200/5000 ( 63%) | Loss:  2.165587\n",
            "train epoch: 1 | batch staus: 3840/5000 ( 76%) | Loss:  2.131694\n",
            "train epoch: 1 | batch staus: 4480/5000 ( 89%) | Loss:  2.056072\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0324, Accuracy: 2179/8000(27%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 2 | batch staus: 0/5000 ( 0%) | Loss:  2.066060\n",
            "train epoch: 2 | batch staus: 640/5000 ( 13%) | Loss:  2.033450\n",
            "train epoch: 2 | batch staus: 1280/5000 ( 25%) | Loss:  1.940072\n",
            "train epoch: 2 | batch staus: 1920/5000 ( 38%) | Loss:  1.943460\n",
            "train epoch: 2 | batch staus: 2560/5000 ( 51%) | Loss:  1.921139\n",
            "train epoch: 2 | batch staus: 3200/5000 ( 63%) | Loss:  1.887898\n",
            "train epoch: 2 | batch staus: 3840/5000 ( 76%) | Loss:  1.870813\n",
            "train epoch: 2 | batch staus: 4480/5000 ( 89%) | Loss:  1.905525\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0325, Accuracy: 1826/8000(23%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 3 | batch staus: 0/5000 ( 0%) | Loss:  1.898758\n",
            "train epoch: 3 | batch staus: 640/5000 ( 13%) | Loss:  1.781021\n",
            "train epoch: 3 | batch staus: 1280/5000 ( 25%) | Loss:  1.844861\n",
            "train epoch: 3 | batch staus: 1920/5000 ( 38%) | Loss:  1.842050\n",
            "train epoch: 3 | batch staus: 2560/5000 ( 51%) | Loss:  1.866751\n",
            "train epoch: 3 | batch staus: 3200/5000 ( 63%) | Loss:  1.807223\n",
            "train epoch: 3 | batch staus: 3840/5000 ( 76%) | Loss:  1.786574\n",
            "train epoch: 3 | batch staus: 4480/5000 ( 89%) | Loss:  1.743847\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0295, Accuracy: 2381/8000(30%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 4 | batch staus: 0/5000 ( 0%) | Loss:  1.790169\n",
            "train epoch: 4 | batch staus: 640/5000 ( 13%) | Loss:  1.777617\n",
            "train epoch: 4 | batch staus: 1280/5000 ( 25%) | Loss:  1.700735\n",
            "train epoch: 4 | batch staus: 1920/5000 ( 38%) | Loss:  1.816743\n",
            "train epoch: 4 | batch staus: 2560/5000 ( 51%) | Loss:  1.857927\n",
            "train epoch: 4 | batch staus: 3200/5000 ( 63%) | Loss:  1.609918\n",
            "train epoch: 4 | batch staus: 3840/5000 ( 76%) | Loss:  1.789644\n",
            "train epoch: 4 | batch staus: 4480/5000 ( 89%) | Loss:  1.620402\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0389, Accuracy: 1771/8000(22%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 5 | batch staus: 0/5000 ( 0%) | Loss:  1.660998\n",
            "train epoch: 5 | batch staus: 640/5000 ( 13%) | Loss:  1.470306\n",
            "train epoch: 5 | batch staus: 1280/5000 ( 25%) | Loss:  1.792032\n",
            "train epoch: 5 | batch staus: 1920/5000 ( 38%) | Loss:  1.765656\n",
            "train epoch: 5 | batch staus: 2560/5000 ( 51%) | Loss:  1.782431\n",
            "train epoch: 5 | batch staus: 3200/5000 ( 63%) | Loss:  1.619244\n",
            "train epoch: 5 | batch staus: 3840/5000 ( 76%) | Loss:  1.518321\n",
            "train epoch: 5 | batch staus: 4480/5000 ( 89%) | Loss:  1.672554\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0289, Accuracy: 2212/8000(28%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 6 | batch staus: 0/5000 ( 0%) | Loss:  1.685555\n",
            "train epoch: 6 | batch staus: 640/5000 ( 13%) | Loss:  1.633986\n",
            "train epoch: 6 | batch staus: 1280/5000 ( 25%) | Loss:  1.589686\n",
            "train epoch: 6 | batch staus: 1920/5000 ( 38%) | Loss:  1.656419\n",
            "train epoch: 6 | batch staus: 2560/5000 ( 51%) | Loss:  1.672683\n",
            "train epoch: 6 | batch staus: 3200/5000 ( 63%) | Loss:  1.661108\n",
            "train epoch: 6 | batch staus: 3840/5000 ( 76%) | Loss:  1.688173\n",
            "train epoch: 6 | batch staus: 4480/5000 ( 89%) | Loss:  1.527275\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0274, Accuracy: 2546/8000(32%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 7 | batch staus: 0/5000 ( 0%) | Loss:  1.610536\n",
            "train epoch: 7 | batch staus: 640/5000 ( 13%) | Loss:  1.711881\n",
            "train epoch: 7 | batch staus: 1280/5000 ( 25%) | Loss:  1.526184\n",
            "train epoch: 7 | batch staus: 1920/5000 ( 38%) | Loss:  1.712016\n",
            "train epoch: 7 | batch staus: 2560/5000 ( 51%) | Loss:  1.783898\n",
            "train epoch: 7 | batch staus: 3200/5000 ( 63%) | Loss:  1.779588\n",
            "train epoch: 7 | batch staus: 3840/5000 ( 76%) | Loss:  1.631873\n",
            "train epoch: 7 | batch staus: 4480/5000 ( 89%) | Loss:  1.637943\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0271, Accuracy: 2703/8000(34%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 8 | batch staus: 0/5000 ( 0%) | Loss:  1.591097\n",
            "train epoch: 8 | batch staus: 640/5000 ( 13%) | Loss:  1.693871\n",
            "train epoch: 8 | batch staus: 1280/5000 ( 25%) | Loss:  1.514482\n",
            "train epoch: 8 | batch staus: 1920/5000 ( 38%) | Loss:  1.492260\n",
            "train epoch: 8 | batch staus: 2560/5000 ( 51%) | Loss:  1.473762\n",
            "train epoch: 8 | batch staus: 3200/5000 ( 63%) | Loss:  1.561167\n",
            "train epoch: 8 | batch staus: 3840/5000 ( 76%) | Loss:  1.431160\n",
            "train epoch: 8 | batch staus: 4480/5000 ( 89%) | Loss:  1.499656\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0283, Accuracy: 2404/8000(30%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 9 | batch staus: 0/5000 ( 0%) | Loss:  1.578632\n",
            "train epoch: 9 | batch staus: 640/5000 ( 13%) | Loss:  1.537735\n",
            "train epoch: 9 | batch staus: 1280/5000 ( 25%) | Loss:  1.589549\n",
            "train epoch: 9 | batch staus: 1920/5000 ( 38%) | Loss:  1.514706\n",
            "train epoch: 9 | batch staus: 2560/5000 ( 51%) | Loss:  1.493783\n",
            "train epoch: 9 | batch staus: 3200/5000 ( 63%) | Loss:  1.686725\n",
            "train epoch: 9 | batch staus: 3840/5000 ( 76%) | Loss:  1.470229\n",
            "train epoch: 9 | batch staus: 4480/5000 ( 89%) | Loss:  1.579228\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0290, Accuracy: 2518/8000(31%)\n",
            "Testing time: 0m 24s\n",
            "total time: 3m  34s \n",
            " model was trained on cuda!\n"
          ]
        }
      ]
    }
  ]
}