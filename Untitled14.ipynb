{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMY+vOVWrgSTN59Bl86xNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqc9qhDuR1WI",
        "outputId": "9f756397-74a2-42ae-932f-a983bf0d786c"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('resnet')\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "path2data = '/content/resnet/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "# load dataset\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
        "        '''\n",
        "        block: 'BasicBlock' or 'BottleNeck'\n",
        "        num_block: [n, n, n, n]\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels=64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        x = self.conv3_x(output)\n",
        "        x = self.conv4_x(x)\n",
        "        x = self.conv5_x(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model=ResNet(BottleNeck,[3,8,36,3])\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at resnet\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch: 1 | batch staus: 0/5000 ( 0%) | Loss:  2.331322\n",
            "train epoch: 1 | batch staus: 640/5000 ( 13%) | Loss:  4.031568\n",
            "train epoch: 1 | batch staus: 1280/5000 ( 25%) | Loss:  2.743551\n",
            "train epoch: 1 | batch staus: 1920/5000 ( 38%) | Loss:  3.310320\n",
            "train epoch: 1 | batch staus: 2560/5000 ( 51%) | Loss:  3.203821\n",
            "train epoch: 1 | batch staus: 3200/5000 ( 63%) | Loss:  3.438390\n",
            "train epoch: 1 | batch staus: 3840/5000 ( 76%) | Loss:  3.435224\n",
            "train epoch: 1 | batch staus: 4480/5000 ( 89%) | Loss:  2.590989\n",
            "Training time: 1m 11s\n",
            "=======\n",
            " test set: average loss:  0.0545, Accuracy: 1377/8000(17%)\n",
            "Testing time: 1m 46s\n",
            "train epoch: 2 | batch staus: 0/5000 ( 0%) | Loss:  2.590879\n",
            "train epoch: 2 | batch staus: 640/5000 ( 13%) | Loss:  2.258070\n",
            "train epoch: 2 | batch staus: 1280/5000 ( 25%) | Loss:  2.535842\n",
            "train epoch: 2 | batch staus: 1920/5000 ( 38%) | Loss:  2.247748\n",
            "train epoch: 2 | batch staus: 2560/5000 ( 51%) | Loss:  2.297128\n",
            "train epoch: 2 | batch staus: 3200/5000 ( 63%) | Loss:  2.091692\n",
            "train epoch: 2 | batch staus: 3840/5000 ( 76%) | Loss:  2.392713\n",
            "train epoch: 2 | batch staus: 4480/5000 ( 89%) | Loss:  2.223287\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0540, Accuracy: 1415/8000(18%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 3 | batch staus: 0/5000 ( 0%) | Loss:  3.257495\n",
            "train epoch: 3 | batch staus: 640/5000 ( 13%) | Loss:  2.540758\n",
            "train epoch: 3 | batch staus: 1280/5000 ( 25%) | Loss:  1.943455\n",
            "train epoch: 3 | batch staus: 1920/5000 ( 38%) | Loss:  2.090447\n",
            "train epoch: 3 | batch staus: 2560/5000 ( 51%) | Loss:  1.962317\n",
            "train epoch: 3 | batch staus: 3200/5000 ( 63%) | Loss:  2.062810\n",
            "train epoch: 3 | batch staus: 3840/5000 ( 76%) | Loss:  2.013297\n",
            "train epoch: 3 | batch staus: 4480/5000 ( 89%) | Loss:  1.984980\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0541, Accuracy: 1918/8000(24%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 4 | batch staus: 0/5000 ( 0%) | Loss:  2.820733\n",
            "train epoch: 4 | batch staus: 640/5000 ( 13%) | Loss:  2.194956\n",
            "train epoch: 4 | batch staus: 1280/5000 ( 25%) | Loss:  2.102003\n",
            "train epoch: 4 | batch staus: 1920/5000 ( 38%) | Loss:  1.885089\n",
            "train epoch: 4 | batch staus: 2560/5000 ( 51%) | Loss:  1.720760\n",
            "train epoch: 4 | batch staus: 3200/5000 ( 63%) | Loss:  1.965590\n",
            "train epoch: 4 | batch staus: 3840/5000 ( 76%) | Loss:  1.839429\n",
            "train epoch: 4 | batch staus: 4480/5000 ( 89%) | Loss:  1.828427\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0614, Accuracy: 1552/8000(19%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 5 | batch staus: 0/5000 ( 0%) | Loss:  2.948933\n",
            "train epoch: 5 | batch staus: 640/5000 ( 13%) | Loss:  2.111667\n",
            "train epoch: 5 | batch staus: 1280/5000 ( 25%) | Loss:  2.314490\n",
            "train epoch: 5 | batch staus: 1920/5000 ( 38%) | Loss:  1.964667\n",
            "train epoch: 5 | batch staus: 2560/5000 ( 51%) | Loss:  1.857520\n",
            "train epoch: 5 | batch staus: 3200/5000 ( 63%) | Loss:  1.965981\n",
            "train epoch: 5 | batch staus: 3840/5000 ( 76%) | Loss:  1.857432\n",
            "train epoch: 5 | batch staus: 4480/5000 ( 89%) | Loss:  1.928068\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0312, Accuracy: 2085/8000(26%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 6 | batch staus: 0/5000 ( 0%) | Loss:  1.832222\n",
            "train epoch: 6 | batch staus: 640/5000 ( 13%) | Loss:  1.890592\n",
            "train epoch: 6 | batch staus: 1280/5000 ( 25%) | Loss:  1.619859\n",
            "train epoch: 6 | batch staus: 1920/5000 ( 38%) | Loss:  1.707371\n",
            "train epoch: 6 | batch staus: 2560/5000 ( 51%) | Loss:  1.605989\n",
            "train epoch: 6 | batch staus: 3200/5000 ( 63%) | Loss:  1.887966\n",
            "train epoch: 6 | batch staus: 3840/5000 ( 76%) | Loss:  1.676167\n",
            "train epoch: 6 | batch staus: 4480/5000 ( 89%) | Loss:  1.762562\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0326, Accuracy: 2408/8000(30%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 7 | batch staus: 0/5000 ( 0%) | Loss:  2.137860\n",
            "train epoch: 7 | batch staus: 640/5000 ( 13%) | Loss:  1.732152\n",
            "train epoch: 7 | batch staus: 1280/5000 ( 25%) | Loss:  1.909194\n",
            "train epoch: 7 | batch staus: 1920/5000 ( 38%) | Loss:  1.576883\n",
            "train epoch: 7 | batch staus: 2560/5000 ( 51%) | Loss:  1.732866\n",
            "train epoch: 7 | batch staus: 3200/5000 ( 63%) | Loss:  1.671412\n",
            "train epoch: 7 | batch staus: 3840/5000 ( 76%) | Loss:  1.611624\n",
            "train epoch: 7 | batch staus: 4480/5000 ( 89%) | Loss:  1.726112\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0286, Accuracy: 2311/8000(29%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 8 | batch staus: 0/5000 ( 0%) | Loss:  1.544107\n",
            "train epoch: 8 | batch staus: 640/5000 ( 13%) | Loss:  1.741370\n",
            "train epoch: 8 | batch staus: 1280/5000 ( 25%) | Loss:  1.583126\n",
            "train epoch: 8 | batch staus: 1920/5000 ( 38%) | Loss:  1.594024\n",
            "train epoch: 8 | batch staus: 2560/5000 ( 51%) | Loss:  1.584832\n",
            "train epoch: 8 | batch staus: 3200/5000 ( 63%) | Loss:  1.812343\n",
            "train epoch: 8 | batch staus: 3840/5000 ( 76%) | Loss:  1.680972\n",
            "train epoch: 8 | batch staus: 4480/5000 ( 89%) | Loss:  1.446429\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0385, Accuracy: 1969/8000(25%)\n",
            "Testing time: 1m 45s\n",
            "train epoch: 9 | batch staus: 0/5000 ( 0%) | Loss:  1.965502\n",
            "train epoch: 9 | batch staus: 640/5000 ( 13%) | Loss:  1.729743\n",
            "train epoch: 9 | batch staus: 1280/5000 ( 25%) | Loss:  1.590187\n",
            "train epoch: 9 | batch staus: 1920/5000 ( 38%) | Loss:  1.539933\n",
            "train epoch: 9 | batch staus: 2560/5000 ( 51%) | Loss:  1.407260\n",
            "train epoch: 9 | batch staus: 3200/5000 ( 63%) | Loss:  1.781002\n",
            "train epoch: 9 | batch staus: 3840/5000 ( 76%) | Loss:  1.639080\n",
            "train epoch: 9 | batch staus: 4480/5000 ( 89%) | Loss:  1.720285\n",
            "Training time: 1m 10s\n",
            "=======\n",
            " test set: average loss:  0.0321, Accuracy: 2209/8000(28%)\n",
            "Testing time: 1m 46s\n",
            "total time: 15m  50s \n",
            " model was trained on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhZeU89PoMlx",
        "outputId": "d093371a-6460-4d67-f152-48145fb6c07d"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "drive.mount('inceptionv4')\n",
        "batch_size=8\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "path2data = '/content/inceptionv4/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # bias=Fasle, because BN after conv includes bias.\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            BasicConv2d(3, 32, 3, stride=2, padding=0), # 149 x 149 x 32\n",
        "            BasicConv2d(32, 32, 3, stride=1, padding=0), # 147 x 147 x 32\n",
        "            BasicConv2d(32, 64, 3, stride=1, padding=1), # 147 x 147 x 64 \n",
        "        )\n",
        "\n",
        "        self.branch3x3_conv = BasicConv2d(64, 96, 3, padding=1) # 73x73x96\n",
        "\n",
        "        #  kernel_size=4: 피쳐맵 크기 73, kernel_size=3: 피쳐맵 크기 74\n",
        "        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1) # 73x73x64\n",
        "\n",
        "        self.branch7x7b = nn.Sequential(\n",
        "            BasicConv2d(160, 64, 1, stride=1, padding=0),\n",
        "            BasicConv2d(64, 96, 3, stride=1, padding=0)\n",
        "        ) # 71x71x96\n",
        "\n",
        "        self.branch7x7a = nn.Sequential(\n",
        "            BasicConv2d(160, 64, 1, stride=1, padding=0),\n",
        "            BasicConv2d(64, 64, (7,1), stride=1, padding=(3,0)),\n",
        "            BasicConv2d(64, 64, (1,7), stride=1, padding=(0,3)),\n",
        "            BasicConv2d(64, 96, 3, stride=1, padding=0)\n",
        "        ) # 71x71x96\n",
        "\n",
        "        self.branchpoolb = BasicConv2d(192, 192, 3, stride=1, padding=1) # 35x35x192\n",
        "\n",
        "        #  kernel_size=4: 피쳐맵 크기 73, kernel_size=3: 피쳐맵 크기 74\n",
        "        self.branchpoola = nn.MaxPool2d(3, 1, 1) # 35x35x192\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.cat([self.branch3x3_conv(x), self.branch3x3_pool(x)], dim=1)\n",
        "        x = torch.cat([self.branch7x7a(x), self.branch7x7b(x)], dim=1)\n",
        "        x = torch.cat([self.branchpoola(x), self.branchpoolb(x)], dim=1)\n",
        "        return x\n",
        "\n",
        "class Inception_Resnet_A(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 32, 1, stride=1, padding=0)\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, 1, stride=1, padding=0),\n",
        "            BasicConv2d(32, 32, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 32, 1, stride=1, padding=0),\n",
        "            BasicConv2d(32, 48, 3, stride=1, padding=1),\n",
        "            BasicConv2d(48, 64, 3, stride=1, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.reduction1x1 = nn.Conv2d(128, 384, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 384, 1, stride=1, padding=0)\n",
        "        self.bn = nn.BatchNorm2d(384)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch3x3(x), self.branch3x3stack(x)), dim=1)\n",
        "        x = self.reduction1x1(x)\n",
        "        x = self.bn(x_shortcut + x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Inception_Resnet_B(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 192, 1, stride=1, padding=0)\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 128, 1, stride=1, padding=0),\n",
        "            BasicConv2d(128, 160, (1,7), stride=1, padding=(0,3)),\n",
        "            BasicConv2d(160, 192, (7,1), stride=1, padding=(3,0))\n",
        "        )\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(384, 1152, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 1152, 1, stride=1, padding=0)\n",
        "        self.bn = nn.BatchNorm2d(1152)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch7x7(x)), dim=1)\n",
        "        x = self.reduction1x1(x) * 0.1\n",
        "        x = self.bn(x + x_shortcut)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Inception_Resnet_C(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 192, 1, stride=1, padding=0)\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 192, 1, stride=1, padding=0),\n",
        "            BasicConv2d(192, 224, (1,3), stride=1, padding=(0,1)),\n",
        "            BasicConv2d(224, 256, (3,1), stride=1, padding=(1,0))\n",
        "        )\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(448, 2144, 1, stride=1, padding=0)\n",
        "        self.shortcut = nn.Conv2d(in_channels, 2144, 1, stride=1, padding=0) # 2144\n",
        "        self.bn = nn.BatchNorm2d(2144)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shortcut = self.shortcut(x)\n",
        "        x = torch.cat((self.branch1x1(x), self.branch3x3(x)), dim=1)\n",
        "        x = self.reduction1x1(x) * 0.1\n",
        "        x = self.bn(x_shortcut + x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class ReductionA(nn.Module):\n",
        "    def __init__(self, in_channels, k, l, m, n):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(3, 2)\n",
        "        self.branch3x3 = BasicConv2d(in_channels, n, 3, stride=2, padding=0)\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, k, 1, stride=1, padding=0),\n",
        "            BasicConv2d(k, l, 3, stride=1, padding=1),\n",
        "            BasicConv2d(l, m, 3, stride=2, padding=0)\n",
        "        )\n",
        "\n",
        "        self.output_channels = in_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.branchpool(x), self.branch3x3(x), self.branch3x3stack(x)), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReductionB(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(3, 2)\n",
        "        self.branch3x3a = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 384, 3, stride=2, padding=0)\n",
        "        )\n",
        "        self.branch3x3b = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 288, 3, stride=2, padding=0)\n",
        "        )\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(in_channels, 256, 1, stride=1, padding=0),\n",
        "            BasicConv2d(256, 288, 3, stride=1, padding=1),\n",
        "            BasicConv2d(288, 320, 3, stride=2, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.branchpool(x), self.branch3x3a(x), self.branch3x3b(x), self.branch3x3stack(x)), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionResNetV2(nn.Module):\n",
        "    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        blocks.append(Stem())\n",
        "        for i in range(A):\n",
        "            blocks.append(Inception_Resnet_A(384))\n",
        "        blocks.append(ReductionA(384, k, l, m, n))\n",
        "        for i in range(B):\n",
        "            blocks.append(Inception_Resnet_B(1152))\n",
        "        blocks.append(ReductionB(1152))\n",
        "        for i in range(C):\n",
        "            blocks.append(Inception_Resnet_C(2144))\n",
        "\n",
        "        self.features = nn.Sequential(*blocks)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        # drop out\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.linear = nn.Linear(2144, num_classes)\n",
        "\n",
        "        # weights inittialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    # define weight initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model = InceptionResNetV2(10, 20, 10).to(device)\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 8):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at inceptionv4; to attempt to forcibly remount, call drive.mount(\"inceptionv4\", force_remount=True).\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train epoch: 1 | batch staus: 0/5000 ( 0%) | Loss:  2.286554\n",
            "train epoch: 1 | batch staus: 80/5000 ( 2%) | Loss:  2.454285\n",
            "train epoch: 1 | batch staus: 160/5000 ( 3%) | Loss:  2.403782\n",
            "train epoch: 1 | batch staus: 240/5000 ( 5%) | Loss:  2.721522\n",
            "train epoch: 1 | batch staus: 320/5000 ( 6%) | Loss:  2.111471\n",
            "train epoch: 1 | batch staus: 400/5000 ( 8%) | Loss:  2.125181\n",
            "train epoch: 1 | batch staus: 480/5000 ( 10%) | Loss:  2.312933\n",
            "train epoch: 1 | batch staus: 560/5000 ( 11%) | Loss:  2.222236\n",
            "train epoch: 1 | batch staus: 640/5000 ( 13%) | Loss:  2.398852\n",
            "train epoch: 1 | batch staus: 720/5000 ( 14%) | Loss:  2.939428\n",
            "train epoch: 1 | batch staus: 800/5000 ( 16%) | Loss:  2.128467\n",
            "train epoch: 1 | batch staus: 880/5000 ( 18%) | Loss:  2.396462\n",
            "train epoch: 1 | batch staus: 960/5000 ( 19%) | Loss:  2.383148\n",
            "train epoch: 1 | batch staus: 1040/5000 ( 21%) | Loss:  2.419031\n",
            "train epoch: 1 | batch staus: 1120/5000 ( 22%) | Loss:  2.346766\n",
            "train epoch: 1 | batch staus: 1200/5000 ( 24%) | Loss:  2.836213\n",
            "train epoch: 1 | batch staus: 1280/5000 ( 26%) | Loss:  2.610880\n",
            "train epoch: 1 | batch staus: 1360/5000 ( 27%) | Loss:  2.471619\n",
            "train epoch: 1 | batch staus: 1440/5000 ( 29%) | Loss:  2.157076\n",
            "train epoch: 1 | batch staus: 1520/5000 ( 30%) | Loss:  2.347551\n",
            "train epoch: 1 | batch staus: 1600/5000 ( 32%) | Loss:  2.136395\n",
            "train epoch: 1 | batch staus: 1680/5000 ( 34%) | Loss:  1.879409\n",
            "train epoch: 1 | batch staus: 1760/5000 ( 35%) | Loss:  2.297934\n",
            "train epoch: 1 | batch staus: 1840/5000 ( 37%) | Loss:  1.778263\n",
            "train epoch: 1 | batch staus: 1920/5000 ( 38%) | Loss:  2.607331\n",
            "train epoch: 1 | batch staus: 2000/5000 ( 40%) | Loss:  2.089479\n",
            "train epoch: 1 | batch staus: 2080/5000 ( 42%) | Loss:  2.122340\n",
            "train epoch: 1 | batch staus: 2160/5000 ( 43%) | Loss:  2.431854\n",
            "train epoch: 1 | batch staus: 2240/5000 ( 45%) | Loss:  2.120777\n",
            "train epoch: 1 | batch staus: 2320/5000 ( 46%) | Loss:  2.217481\n",
            "train epoch: 1 | batch staus: 2400/5000 ( 48%) | Loss:  1.900893\n",
            "train epoch: 1 | batch staus: 2480/5000 ( 50%) | Loss:  1.949920\n",
            "train epoch: 1 | batch staus: 2560/5000 ( 51%) | Loss:  2.204478\n",
            "train epoch: 1 | batch staus: 2640/5000 ( 53%) | Loss:  1.987307\n",
            "train epoch: 1 | batch staus: 2720/5000 ( 54%) | Loss:  2.092287\n",
            "train epoch: 1 | batch staus: 2800/5000 ( 56%) | Loss:  2.215400\n",
            "train epoch: 1 | batch staus: 2880/5000 ( 58%) | Loss:  1.921945\n",
            "train epoch: 1 | batch staus: 2960/5000 ( 59%) | Loss:  2.318783\n",
            "train epoch: 1 | batch staus: 3040/5000 ( 61%) | Loss:  2.330806\n",
            "train epoch: 1 | batch staus: 3120/5000 ( 62%) | Loss:  2.127395\n",
            "train epoch: 1 | batch staus: 3200/5000 ( 64%) | Loss:  2.665099\n",
            "train epoch: 1 | batch staus: 3280/5000 ( 66%) | Loss:  2.381979\n",
            "train epoch: 1 | batch staus: 3360/5000 ( 67%) | Loss:  2.501086\n",
            "train epoch: 1 | batch staus: 3440/5000 ( 69%) | Loss:  2.265075\n",
            "train epoch: 1 | batch staus: 3520/5000 ( 70%) | Loss:  2.261793\n",
            "train epoch: 1 | batch staus: 3600/5000 ( 72%) | Loss:  1.949123\n",
            "train epoch: 1 | batch staus: 3680/5000 ( 74%) | Loss:  2.152265\n",
            "train epoch: 1 | batch staus: 3760/5000 ( 75%) | Loss:  2.332105\n",
            "train epoch: 1 | batch staus: 3840/5000 ( 77%) | Loss:  1.989561\n",
            "train epoch: 1 | batch staus: 3920/5000 ( 78%) | Loss:  1.879241\n",
            "train epoch: 1 | batch staus: 4000/5000 ( 80%) | Loss:  2.377346\n",
            "train epoch: 1 | batch staus: 4080/5000 ( 82%) | Loss:  2.240484\n",
            "train epoch: 1 | batch staus: 4160/5000 ( 83%) | Loss:  2.337971\n",
            "train epoch: 1 | batch staus: 4240/5000 ( 85%) | Loss:  2.123034\n",
            "train epoch: 1 | batch staus: 4320/5000 ( 86%) | Loss:  1.906964\n",
            "train epoch: 1 | batch staus: 4400/5000 ( 88%) | Loss:  2.542350\n",
            "train epoch: 1 | batch staus: 4480/5000 ( 90%) | Loss:  2.214550\n",
            "train epoch: 1 | batch staus: 4560/5000 ( 91%) | Loss:  2.420188\n",
            "train epoch: 1 | batch staus: 4640/5000 ( 93%) | Loss:  1.875257\n",
            "train epoch: 1 | batch staus: 4720/5000 ( 94%) | Loss:  1.957378\n",
            "train epoch: 1 | batch staus: 4800/5000 ( 96%) | Loss:  2.057200\n",
            "train epoch: 1 | batch staus: 4880/5000 ( 98%) | Loss:  1.908359\n",
            "train epoch: 1 | batch staus: 4960/5000 ( 99%) | Loss:  1.888364\n",
            "Training time: 18m 47s\n",
            "=======\n",
            " test set: average loss:  0.2531, Accuracy: 1845/8000(23%)\n",
            "Testing time: 28m 50s\n",
            "train epoch: 2 | batch staus: 0/5000 ( 0%) | Loss:  1.987004\n",
            "train epoch: 2 | batch staus: 80/5000 ( 2%) | Loss:  2.044886\n",
            "train epoch: 2 | batch staus: 160/5000 ( 3%) | Loss:  2.588993\n",
            "train epoch: 2 | batch staus: 240/5000 ( 5%) | Loss:  2.488943\n",
            "train epoch: 2 | batch staus: 320/5000 ( 6%) | Loss:  2.036733\n",
            "train epoch: 2 | batch staus: 400/5000 ( 8%) | Loss:  1.967789\n",
            "train epoch: 2 | batch staus: 480/5000 ( 10%) | Loss:  1.632838\n",
            "train epoch: 2 | batch staus: 560/5000 ( 11%) | Loss:  1.756429\n",
            "train epoch: 2 | batch staus: 640/5000 ( 13%) | Loss:  2.260433\n",
            "train epoch: 2 | batch staus: 720/5000 ( 14%) | Loss:  2.402211\n",
            "train epoch: 2 | batch staus: 800/5000 ( 16%) | Loss:  2.117762\n",
            "train epoch: 2 | batch staus: 880/5000 ( 18%) | Loss:  1.933766\n",
            "train epoch: 2 | batch staus: 960/5000 ( 19%) | Loss:  1.937480\n",
            "train epoch: 2 | batch staus: 1040/5000 ( 21%) | Loss:  2.346551\n",
            "train epoch: 2 | batch staus: 1120/5000 ( 22%) | Loss:  1.734648\n",
            "train epoch: 2 | batch staus: 1200/5000 ( 24%) | Loss:  1.998309\n",
            "train epoch: 2 | batch staus: 1280/5000 ( 26%) | Loss:  1.964844\n",
            "train epoch: 2 | batch staus: 1360/5000 ( 27%) | Loss:  1.762372\n",
            "train epoch: 2 | batch staus: 1440/5000 ( 29%) | Loss:  1.357301\n",
            "train epoch: 2 | batch staus: 1520/5000 ( 30%) | Loss:  1.895813\n",
            "train epoch: 2 | batch staus: 1600/5000 ( 32%) | Loss:  1.701208\n",
            "train epoch: 2 | batch staus: 1680/5000 ( 34%) | Loss:  2.250893\n",
            "train epoch: 2 | batch staus: 1760/5000 ( 35%) | Loss:  2.356302\n",
            "train epoch: 2 | batch staus: 1840/5000 ( 37%) | Loss:  1.889745\n",
            "train epoch: 2 | batch staus: 1920/5000 ( 38%) | Loss:  2.204744\n",
            "train epoch: 2 | batch staus: 2000/5000 ( 40%) | Loss:  2.313011\n",
            "train epoch: 2 | batch staus: 2080/5000 ( 42%) | Loss:  2.038113\n",
            "train epoch: 2 | batch staus: 2160/5000 ( 43%) | Loss:  1.622159\n",
            "train epoch: 2 | batch staus: 2240/5000 ( 45%) | Loss:  1.997113\n",
            "train epoch: 2 | batch staus: 2320/5000 ( 46%) | Loss:  2.186660\n",
            "train epoch: 2 | batch staus: 2400/5000 ( 48%) | Loss:  1.690544\n",
            "train epoch: 2 | batch staus: 2480/5000 ( 50%) | Loss:  2.177993\n",
            "train epoch: 2 | batch staus: 2560/5000 ( 51%) | Loss:  1.878695\n",
            "train epoch: 2 | batch staus: 2640/5000 ( 53%) | Loss:  1.381931\n",
            "train epoch: 2 | batch staus: 2720/5000 ( 54%) | Loss:  2.085863\n",
            "train epoch: 2 | batch staus: 2800/5000 ( 56%) | Loss:  1.643271\n",
            "train epoch: 2 | batch staus: 2880/5000 ( 58%) | Loss:  2.073261\n",
            "train epoch: 2 | batch staus: 2960/5000 ( 59%) | Loss:  2.233564\n",
            "train epoch: 2 | batch staus: 3040/5000 ( 61%) | Loss:  1.549563\n",
            "train epoch: 2 | batch staus: 3120/5000 ( 62%) | Loss:  2.393182\n",
            "train epoch: 2 | batch staus: 3200/5000 ( 64%) | Loss:  2.640562\n",
            "train epoch: 2 | batch staus: 3280/5000 ( 66%) | Loss:  2.292618\n",
            "train epoch: 2 | batch staus: 3360/5000 ( 67%) | Loss:  2.046741\n",
            "train epoch: 2 | batch staus: 3440/5000 ( 69%) | Loss:  1.640032\n",
            "train epoch: 2 | batch staus: 3520/5000 ( 70%) | Loss:  1.354619\n",
            "train epoch: 2 | batch staus: 3600/5000 ( 72%) | Loss:  1.861346\n",
            "train epoch: 2 | batch staus: 3680/5000 ( 74%) | Loss:  1.528818\n",
            "train epoch: 2 | batch staus: 3760/5000 ( 75%) | Loss:  2.215700\n",
            "train epoch: 2 | batch staus: 3840/5000 ( 77%) | Loss:  1.541567\n",
            "train epoch: 2 | batch staus: 3920/5000 ( 78%) | Loss:  1.918497\n",
            "train epoch: 2 | batch staus: 4000/5000 ( 80%) | Loss:  1.694503\n",
            "train epoch: 2 | batch staus: 4080/5000 ( 82%) | Loss:  2.178989\n",
            "train epoch: 2 | batch staus: 4160/5000 ( 83%) | Loss:  2.306310\n",
            "train epoch: 2 | batch staus: 4240/5000 ( 85%) | Loss:  1.604392\n",
            "train epoch: 2 | batch staus: 4320/5000 ( 86%) | Loss:  1.495240\n",
            "train epoch: 2 | batch staus: 4400/5000 ( 88%) | Loss:  1.614831\n",
            "train epoch: 2 | batch staus: 4480/5000 ( 90%) | Loss:  1.903544\n",
            "train epoch: 2 | batch staus: 4560/5000 ( 91%) | Loss:  1.918704\n",
            "train epoch: 2 | batch staus: 4640/5000 ( 93%) | Loss:  1.324215\n",
            "train epoch: 2 | batch staus: 4720/5000 ( 94%) | Loss:  1.594023\n",
            "train epoch: 2 | batch staus: 4800/5000 ( 96%) | Loss:  1.574876\n",
            "train epoch: 2 | batch staus: 4880/5000 ( 98%) | Loss:  2.336654\n",
            "train epoch: 2 | batch staus: 4960/5000 ( 99%) | Loss:  1.409264\n",
            "Training time: 18m 40s\n",
            "=======\n",
            " test set: average loss:  0.2428, Accuracy: 2104/8000(26%)\n",
            "Testing time: 28m 46s\n",
            "train epoch: 3 | batch staus: 0/5000 ( 0%) | Loss:  1.616542\n",
            "train epoch: 3 | batch staus: 80/5000 ( 2%) | Loss:  2.733848\n",
            "train epoch: 3 | batch staus: 160/5000 ( 3%) | Loss:  1.805907\n",
            "train epoch: 3 | batch staus: 240/5000 ( 5%) | Loss:  1.787101\n",
            "train epoch: 3 | batch staus: 320/5000 ( 6%) | Loss:  2.110965\n",
            "train epoch: 3 | batch staus: 400/5000 ( 8%) | Loss:  2.086955\n",
            "train epoch: 3 | batch staus: 480/5000 ( 10%) | Loss:  1.730313\n",
            "train epoch: 3 | batch staus: 560/5000 ( 11%) | Loss:  1.923234\n",
            "train epoch: 3 | batch staus: 640/5000 ( 13%) | Loss:  2.319587\n",
            "train epoch: 3 | batch staus: 720/5000 ( 14%) | Loss:  1.731433\n",
            "train epoch: 3 | batch staus: 800/5000 ( 16%) | Loss:  1.960123\n",
            "train epoch: 3 | batch staus: 880/5000 ( 18%) | Loss:  1.844770\n",
            "train epoch: 3 | batch staus: 960/5000 ( 19%) | Loss:  1.535713\n",
            "train epoch: 3 | batch staus: 1040/5000 ( 21%) | Loss:  2.144906\n",
            "train epoch: 3 | batch staus: 1120/5000 ( 22%) | Loss:  1.866327\n",
            "train epoch: 3 | batch staus: 1200/5000 ( 24%) | Loss:  3.059991\n",
            "train epoch: 3 | batch staus: 1280/5000 ( 26%) | Loss:  2.170886\n",
            "train epoch: 3 | batch staus: 1360/5000 ( 27%) | Loss:  1.800765\n",
            "train epoch: 3 | batch staus: 1440/5000 ( 29%) | Loss:  2.349756\n",
            "train epoch: 3 | batch staus: 1520/5000 ( 30%) | Loss:  1.506789\n",
            "train epoch: 3 | batch staus: 1600/5000 ( 32%) | Loss:  1.961819\n",
            "train epoch: 3 | batch staus: 1680/5000 ( 34%) | Loss:  1.283326\n",
            "train epoch: 3 | batch staus: 1760/5000 ( 35%) | Loss:  1.938918\n",
            "train epoch: 3 | batch staus: 1840/5000 ( 37%) | Loss:  1.673773\n",
            "train epoch: 3 | batch staus: 1920/5000 ( 38%) | Loss:  1.999133\n",
            "train epoch: 3 | batch staus: 2000/5000 ( 40%) | Loss:  2.012846\n",
            "train epoch: 3 | batch staus: 2080/5000 ( 42%) | Loss:  1.605305\n",
            "train epoch: 3 | batch staus: 2160/5000 ( 43%) | Loss:  1.705097\n",
            "train epoch: 3 | batch staus: 2240/5000 ( 45%) | Loss:  1.808036\n",
            "train epoch: 3 | batch staus: 2320/5000 ( 46%) | Loss:  1.722443\n",
            "train epoch: 3 | batch staus: 2400/5000 ( 48%) | Loss:  1.545070\n",
            "train epoch: 3 | batch staus: 2480/5000 ( 50%) | Loss:  1.907345\n",
            "train epoch: 3 | batch staus: 2560/5000 ( 51%) | Loss:  1.847000\n",
            "train epoch: 3 | batch staus: 2640/5000 ( 53%) | Loss:  2.282560\n",
            "train epoch: 3 | batch staus: 2720/5000 ( 54%) | Loss:  1.593100\n",
            "train epoch: 3 | batch staus: 2800/5000 ( 56%) | Loss:  2.428063\n",
            "train epoch: 3 | batch staus: 2880/5000 ( 58%) | Loss:  2.910589\n",
            "train epoch: 3 | batch staus: 2960/5000 ( 59%) | Loss:  1.836304\n",
            "train epoch: 3 | batch staus: 3040/5000 ( 61%) | Loss:  2.129611\n",
            "train epoch: 3 | batch staus: 3120/5000 ( 62%) | Loss:  1.903663\n",
            "train epoch: 3 | batch staus: 3200/5000 ( 64%) | Loss:  1.877897\n",
            "train epoch: 3 | batch staus: 3280/5000 ( 66%) | Loss:  2.008333\n",
            "train epoch: 3 | batch staus: 3360/5000 ( 67%) | Loss:  1.638805\n",
            "train epoch: 3 | batch staus: 3440/5000 ( 69%) | Loss:  1.802997\n",
            "train epoch: 3 | batch staus: 3520/5000 ( 70%) | Loss:  2.099908\n",
            "train epoch: 3 | batch staus: 3600/5000 ( 72%) | Loss:  1.899511\n",
            "train epoch: 3 | batch staus: 3680/5000 ( 74%) | Loss:  2.114949\n",
            "train epoch: 3 | batch staus: 3760/5000 ( 75%) | Loss:  2.134543\n",
            "train epoch: 3 | batch staus: 3840/5000 ( 77%) | Loss:  2.239291\n",
            "train epoch: 3 | batch staus: 3920/5000 ( 78%) | Loss:  2.131559\n",
            "train epoch: 3 | batch staus: 4000/5000 ( 80%) | Loss:  2.147161\n",
            "train epoch: 3 | batch staus: 4080/5000 ( 82%) | Loss:  1.564146\n",
            "train epoch: 3 | batch staus: 4160/5000 ( 83%) | Loss:  2.382820\n",
            "train epoch: 3 | batch staus: 4240/5000 ( 85%) | Loss:  2.233076\n",
            "train epoch: 3 | batch staus: 4320/5000 ( 86%) | Loss:  2.029300\n",
            "train epoch: 3 | batch staus: 4400/5000 ( 88%) | Loss:  1.717136\n",
            "train epoch: 3 | batch staus: 4480/5000 ( 90%) | Loss:  2.150069\n",
            "train epoch: 3 | batch staus: 4560/5000 ( 91%) | Loss:  2.092704\n",
            "train epoch: 3 | batch staus: 4640/5000 ( 93%) | Loss:  2.640520\n",
            "train epoch: 3 | batch staus: 4720/5000 ( 94%) | Loss:  2.120395\n",
            "train epoch: 3 | batch staus: 4800/5000 ( 96%) | Loss:  1.760901\n",
            "train epoch: 3 | batch staus: 4880/5000 ( 98%) | Loss:  1.513993\n",
            "train epoch: 3 | batch staus: 4960/5000 ( 99%) | Loss:  1.674758\n",
            "Training time: 18m 41s\n",
            "=======\n",
            " test set: average loss:  0.2484, Accuracy: 2230/8000(28%)\n",
            "Testing time: 28m 42s\n",
            "train epoch: 4 | batch staus: 0/5000 ( 0%) | Loss:  1.923262\n",
            "train epoch: 4 | batch staus: 80/5000 ( 2%) | Loss:  1.671333\n",
            "train epoch: 4 | batch staus: 160/5000 ( 3%) | Loss:  2.839825\n",
            "train epoch: 4 | batch staus: 240/5000 ( 5%) | Loss:  2.041911\n",
            "train epoch: 4 | batch staus: 320/5000 ( 6%) | Loss:  2.009629\n",
            "train epoch: 4 | batch staus: 400/5000 ( 8%) | Loss:  1.488364\n",
            "train epoch: 4 | batch staus: 480/5000 ( 10%) | Loss:  1.681503\n",
            "train epoch: 4 | batch staus: 560/5000 ( 11%) | Loss:  1.429071\n",
            "train epoch: 4 | batch staus: 640/5000 ( 13%) | Loss:  1.631570\n",
            "train epoch: 4 | batch staus: 720/5000 ( 14%) | Loss:  1.718624\n",
            "train epoch: 4 | batch staus: 800/5000 ( 16%) | Loss:  1.383068\n",
            "train epoch: 4 | batch staus: 880/5000 ( 18%) | Loss:  1.619626\n",
            "train epoch: 4 | batch staus: 960/5000 ( 19%) | Loss:  1.729867\n",
            "train epoch: 4 | batch staus: 1040/5000 ( 21%) | Loss:  1.551022\n",
            "train epoch: 4 | batch staus: 1120/5000 ( 22%) | Loss:  1.412641\n",
            "train epoch: 4 | batch staus: 1200/5000 ( 24%) | Loss:  2.705562\n",
            "train epoch: 4 | batch staus: 1280/5000 ( 26%) | Loss:  2.173275\n",
            "train epoch: 4 | batch staus: 1360/5000 ( 27%) | Loss:  2.133322\n",
            "train epoch: 4 | batch staus: 1440/5000 ( 29%) | Loss:  1.697608\n",
            "train epoch: 4 | batch staus: 1520/5000 ( 30%) | Loss:  1.767250\n",
            "train epoch: 4 | batch staus: 1600/5000 ( 32%) | Loss:  1.832283\n",
            "train epoch: 4 | batch staus: 1680/5000 ( 34%) | Loss:  1.951516\n",
            "train epoch: 4 | batch staus: 1760/5000 ( 35%) | Loss:  2.267942\n",
            "train epoch: 4 | batch staus: 1840/5000 ( 37%) | Loss:  1.688314\n",
            "train epoch: 4 | batch staus: 1920/5000 ( 38%) | Loss:  2.165320\n",
            "train epoch: 4 | batch staus: 2000/5000 ( 40%) | Loss:  1.889240\n",
            "train epoch: 4 | batch staus: 2080/5000 ( 42%) | Loss:  1.906926\n",
            "train epoch: 4 | batch staus: 2160/5000 ( 43%) | Loss:  1.901402\n",
            "train epoch: 4 | batch staus: 2240/5000 ( 45%) | Loss:  1.973206\n",
            "train epoch: 4 | batch staus: 2320/5000 ( 46%) | Loss:  1.948731\n",
            "train epoch: 4 | batch staus: 2400/5000 ( 48%) | Loss:  1.992525\n",
            "train epoch: 4 | batch staus: 2480/5000 ( 50%) | Loss:  2.256308\n",
            "train epoch: 4 | batch staus: 2560/5000 ( 51%) | Loss:  3.145750\n",
            "train epoch: 4 | batch staus: 2640/5000 ( 53%) | Loss:  1.698629\n",
            "train epoch: 4 | batch staus: 2720/5000 ( 54%) | Loss:  2.117386\n",
            "train epoch: 4 | batch staus: 2800/5000 ( 56%) | Loss:  2.174793\n",
            "train epoch: 4 | batch staus: 2880/5000 ( 58%) | Loss:  1.548088\n",
            "train epoch: 4 | batch staus: 2960/5000 ( 59%) | Loss:  1.439390\n",
            "train epoch: 4 | batch staus: 3040/5000 ( 61%) | Loss:  1.307924\n",
            "train epoch: 4 | batch staus: 3120/5000 ( 62%) | Loss:  1.603722\n",
            "train epoch: 4 | batch staus: 3200/5000 ( 64%) | Loss:  2.662904\n",
            "train epoch: 4 | batch staus: 3280/5000 ( 66%) | Loss:  1.935071\n",
            "train epoch: 4 | batch staus: 3360/5000 ( 67%) | Loss:  2.140115\n",
            "train epoch: 4 | batch staus: 3440/5000 ( 69%) | Loss:  1.492024\n",
            "train epoch: 4 | batch staus: 3520/5000 ( 70%) | Loss:  1.997811\n",
            "train epoch: 4 | batch staus: 3600/5000 ( 72%) | Loss:  1.753048\n",
            "train epoch: 4 | batch staus: 3680/5000 ( 74%) | Loss:  2.033745\n",
            "train epoch: 4 | batch staus: 3760/5000 ( 75%) | Loss:  1.755311\n",
            "train epoch: 4 | batch staus: 3840/5000 ( 77%) | Loss:  2.360318\n",
            "train epoch: 4 | batch staus: 3920/5000 ( 78%) | Loss:  1.682603\n",
            "train epoch: 4 | batch staus: 4000/5000 ( 80%) | Loss:  1.816474\n",
            "train epoch: 4 | batch staus: 4080/5000 ( 82%) | Loss:  1.691751\n",
            "train epoch: 4 | batch staus: 4160/5000 ( 83%) | Loss:  1.486836\n",
            "train epoch: 4 | batch staus: 4240/5000 ( 85%) | Loss:  1.831824\n",
            "train epoch: 4 | batch staus: 4320/5000 ( 86%) | Loss:  2.324996\n",
            "train epoch: 4 | batch staus: 4400/5000 ( 88%) | Loss:  1.944950\n",
            "train epoch: 4 | batch staus: 4480/5000 ( 90%) | Loss:  1.533337\n",
            "train epoch: 4 | batch staus: 4560/5000 ( 91%) | Loss:  2.124348\n",
            "train epoch: 4 | batch staus: 4640/5000 ( 93%) | Loss:  1.745587\n",
            "train epoch: 4 | batch staus: 4720/5000 ( 94%) | Loss:  1.582947\n",
            "train epoch: 4 | batch staus: 4800/5000 ( 96%) | Loss:  2.120673\n",
            "train epoch: 4 | batch staus: 4880/5000 ( 98%) | Loss:  1.607105\n",
            "train epoch: 4 | batch staus: 4960/5000 ( 99%) | Loss:  1.489840\n",
            "Training time: 18m 41s\n",
            "=======\n",
            " test set: average loss:  0.2164, Accuracy: 2435/8000(30%)\n",
            "Testing time: 28m 37s\n",
            "train epoch: 5 | batch staus: 0/5000 ( 0%) | Loss:  2.065597\n",
            "train epoch: 5 | batch staus: 80/5000 ( 2%) | Loss:  1.425087\n",
            "train epoch: 5 | batch staus: 160/5000 ( 3%) | Loss:  1.617283\n",
            "train epoch: 5 | batch staus: 240/5000 ( 5%) | Loss:  1.540258\n",
            "train epoch: 5 | batch staus: 320/5000 ( 6%) | Loss:  1.802409\n",
            "train epoch: 5 | batch staus: 400/5000 ( 8%) | Loss:  1.284790\n",
            "train epoch: 5 | batch staus: 480/5000 ( 10%) | Loss:  1.842926\n",
            "train epoch: 5 | batch staus: 560/5000 ( 11%) | Loss:  1.424411\n",
            "train epoch: 5 | batch staus: 640/5000 ( 13%) | Loss:  1.640131\n",
            "train epoch: 5 | batch staus: 720/5000 ( 14%) | Loss:  1.494315\n",
            "train epoch: 5 | batch staus: 800/5000 ( 16%) | Loss:  2.013322\n",
            "train epoch: 5 | batch staus: 880/5000 ( 18%) | Loss:  2.393335\n",
            "train epoch: 5 | batch staus: 960/5000 ( 19%) | Loss:  1.421382\n",
            "train epoch: 5 | batch staus: 1040/5000 ( 21%) | Loss:  1.852751\n",
            "train epoch: 5 | batch staus: 1120/5000 ( 22%) | Loss:  1.878770\n",
            "train epoch: 5 | batch staus: 1200/5000 ( 24%) | Loss:  1.791062\n",
            "train epoch: 5 | batch staus: 1280/5000 ( 26%) | Loss:  2.227514\n",
            "train epoch: 5 | batch staus: 1360/5000 ( 27%) | Loss:  2.056170\n",
            "train epoch: 5 | batch staus: 1440/5000 ( 29%) | Loss:  2.032887\n",
            "train epoch: 5 | batch staus: 1520/5000 ( 30%) | Loss:  1.682608\n",
            "train epoch: 5 | batch staus: 1600/5000 ( 32%) | Loss:  1.481843\n",
            "train epoch: 5 | batch staus: 1680/5000 ( 34%) | Loss:  1.570796\n",
            "train epoch: 5 | batch staus: 1760/5000 ( 35%) | Loss:  1.659987\n",
            "train epoch: 5 | batch staus: 1840/5000 ( 37%) | Loss:  2.508953\n",
            "train epoch: 5 | batch staus: 1920/5000 ( 38%) | Loss:  1.589718\n",
            "train epoch: 5 | batch staus: 2000/5000 ( 40%) | Loss:  1.544532\n",
            "train epoch: 5 | batch staus: 2080/5000 ( 42%) | Loss:  1.985628\n",
            "train epoch: 5 | batch staus: 2160/5000 ( 43%) | Loss:  1.479097\n",
            "train epoch: 5 | batch staus: 2240/5000 ( 45%) | Loss:  1.471796\n",
            "train epoch: 5 | batch staus: 2320/5000 ( 46%) | Loss:  1.533812\n",
            "train epoch: 5 | batch staus: 2400/5000 ( 48%) | Loss:  1.985468\n",
            "train epoch: 5 | batch staus: 2480/5000 ( 50%) | Loss:  2.358585\n",
            "train epoch: 5 | batch staus: 2560/5000 ( 51%) | Loss:  1.974508\n",
            "train epoch: 5 | batch staus: 2640/5000 ( 53%) | Loss:  2.049961\n",
            "train epoch: 5 | batch staus: 2720/5000 ( 54%) | Loss:  1.536788\n",
            "train epoch: 5 | batch staus: 2800/5000 ( 56%) | Loss:  1.465009\n",
            "train epoch: 5 | batch staus: 2880/5000 ( 58%) | Loss:  1.542279\n",
            "train epoch: 5 | batch staus: 2960/5000 ( 59%) | Loss:  1.524661\n",
            "train epoch: 5 | batch staus: 3040/5000 ( 61%) | Loss:  1.620135\n",
            "train epoch: 5 | batch staus: 3120/5000 ( 62%) | Loss:  1.871324\n",
            "train epoch: 5 | batch staus: 3200/5000 ( 64%) | Loss:  1.776398\n",
            "train epoch: 5 | batch staus: 3280/5000 ( 66%) | Loss:  1.507791\n",
            "train epoch: 5 | batch staus: 3360/5000 ( 67%) | Loss:  1.664224\n",
            "train epoch: 5 | batch staus: 3440/5000 ( 69%) | Loss:  1.803475\n",
            "train epoch: 5 | batch staus: 3520/5000 ( 70%) | Loss:  1.688730\n",
            "train epoch: 5 | batch staus: 3600/5000 ( 72%) | Loss:  1.832522\n",
            "train epoch: 5 | batch staus: 3680/5000 ( 74%) | Loss:  1.728477\n",
            "train epoch: 5 | batch staus: 3760/5000 ( 75%) | Loss:  1.287711\n",
            "train epoch: 5 | batch staus: 3840/5000 ( 77%) | Loss:  1.597672\n",
            "train epoch: 5 | batch staus: 3920/5000 ( 78%) | Loss:  1.853089\n",
            "train epoch: 5 | batch staus: 4000/5000 ( 80%) | Loss:  2.435338\n",
            "train epoch: 5 | batch staus: 4080/5000 ( 82%) | Loss:  1.674164\n",
            "train epoch: 5 | batch staus: 4160/5000 ( 83%) | Loss:  1.617511\n",
            "train epoch: 5 | batch staus: 4240/5000 ( 85%) | Loss:  1.966888\n",
            "train epoch: 5 | batch staus: 4320/5000 ( 86%) | Loss:  1.738560\n",
            "train epoch: 5 | batch staus: 4400/5000 ( 88%) | Loss:  2.265927\n",
            "train epoch: 5 | batch staus: 4480/5000 ( 90%) | Loss:  1.724914\n",
            "train epoch: 5 | batch staus: 4560/5000 ( 91%) | Loss:  1.288028\n",
            "train epoch: 5 | batch staus: 4640/5000 ( 93%) | Loss:  1.430209\n",
            "train epoch: 5 | batch staus: 4720/5000 ( 94%) | Loss:  1.318025\n",
            "train epoch: 5 | batch staus: 4800/5000 ( 96%) | Loss:  1.356364\n",
            "train epoch: 5 | batch staus: 4880/5000 ( 98%) | Loss:  1.853937\n",
            "train epoch: 5 | batch staus: 4960/5000 ( 99%) | Loss:  1.621597\n",
            "Training time: 18m 40s\n",
            "=======\n",
            " test set: average loss:  0.2181, Accuracy: 2704/8000(34%)\n",
            "Testing time: 28m 39s\n",
            "train epoch: 6 | batch staus: 0/5000 ( 0%) | Loss:  2.185578\n",
            "train epoch: 6 | batch staus: 80/5000 ( 2%) | Loss:  1.538504\n",
            "train epoch: 6 | batch staus: 160/5000 ( 3%) | Loss:  1.309085\n",
            "train epoch: 6 | batch staus: 240/5000 ( 5%) | Loss:  2.139781\n",
            "train epoch: 6 | batch staus: 320/5000 ( 6%) | Loss:  2.021721\n",
            "train epoch: 6 | batch staus: 400/5000 ( 8%) | Loss:  1.923548\n",
            "train epoch: 6 | batch staus: 480/5000 ( 10%) | Loss:  2.024984\n",
            "train epoch: 6 | batch staus: 560/5000 ( 11%) | Loss:  1.647155\n",
            "train epoch: 6 | batch staus: 640/5000 ( 13%) | Loss:  1.986637\n",
            "train epoch: 6 | batch staus: 720/5000 ( 14%) | Loss:  2.229032\n",
            "train epoch: 6 | batch staus: 800/5000 ( 16%) | Loss:  1.724411\n",
            "train epoch: 6 | batch staus: 880/5000 ( 18%) | Loss:  1.936536\n",
            "train epoch: 6 | batch staus: 960/5000 ( 19%) | Loss:  1.761551\n",
            "train epoch: 6 | batch staus: 1040/5000 ( 21%) | Loss:  1.683738\n",
            "train epoch: 6 | batch staus: 1120/5000 ( 22%) | Loss:  2.105042\n",
            "train epoch: 6 | batch staus: 1200/5000 ( 24%) | Loss:  1.953144\n",
            "train epoch: 6 | batch staus: 1280/5000 ( 26%) | Loss:  2.230122\n",
            "train epoch: 6 | batch staus: 1360/5000 ( 27%) | Loss:  2.111020\n",
            "train epoch: 6 | batch staus: 1440/5000 ( 29%) | Loss:  1.705387\n",
            "train epoch: 6 | batch staus: 1520/5000 ( 30%) | Loss:  1.854350\n",
            "train epoch: 6 | batch staus: 1600/5000 ( 32%) | Loss:  1.399663\n",
            "train epoch: 6 | batch staus: 1680/5000 ( 34%) | Loss:  1.639007\n",
            "train epoch: 6 | batch staus: 1760/5000 ( 35%) | Loss:  2.557926\n",
            "train epoch: 6 | batch staus: 1840/5000 ( 37%) | Loss:  1.727422\n",
            "train epoch: 6 | batch staus: 1920/5000 ( 38%) | Loss:  1.296582\n",
            "train epoch: 6 | batch staus: 2000/5000 ( 40%) | Loss:  2.083599\n",
            "train epoch: 6 | batch staus: 2080/5000 ( 42%) | Loss:  1.743942\n",
            "train epoch: 6 | batch staus: 2160/5000 ( 43%) | Loss:  2.003428\n",
            "train epoch: 6 | batch staus: 2240/5000 ( 45%) | Loss:  2.058467\n",
            "train epoch: 6 | batch staus: 2320/5000 ( 46%) | Loss:  1.465867\n",
            "train epoch: 6 | batch staus: 2400/5000 ( 48%) | Loss:  1.466630\n",
            "train epoch: 6 | batch staus: 2480/5000 ( 50%) | Loss:  1.562588\n",
            "train epoch: 6 | batch staus: 2560/5000 ( 51%) | Loss:  1.891485\n",
            "train epoch: 6 | batch staus: 2640/5000 ( 53%) | Loss:  1.753572\n",
            "train epoch: 6 | batch staus: 2720/5000 ( 54%) | Loss:  1.679404\n",
            "train epoch: 6 | batch staus: 2800/5000 ( 56%) | Loss:  2.374356\n",
            "train epoch: 6 | batch staus: 2880/5000 ( 58%) | Loss:  1.605561\n",
            "train epoch: 6 | batch staus: 2960/5000 ( 59%) | Loss:  1.814840\n",
            "train epoch: 6 | batch staus: 3040/5000 ( 61%) | Loss:  1.850978\n",
            "train epoch: 6 | batch staus: 3120/5000 ( 62%) | Loss:  1.509920\n",
            "train epoch: 6 | batch staus: 3200/5000 ( 64%) | Loss:  1.473326\n",
            "train epoch: 6 | batch staus: 3280/5000 ( 66%) | Loss:  1.396923\n",
            "train epoch: 6 | batch staus: 3360/5000 ( 67%) | Loss:  1.488282\n",
            "train epoch: 6 | batch staus: 3440/5000 ( 69%) | Loss:  1.113861\n",
            "train epoch: 6 | batch staus: 3520/5000 ( 70%) | Loss:  1.711045\n",
            "train epoch: 6 | batch staus: 3600/5000 ( 72%) | Loss:  2.173520\n",
            "train epoch: 6 | batch staus: 3680/5000 ( 74%) | Loss:  1.617829\n",
            "train epoch: 6 | batch staus: 3760/5000 ( 75%) | Loss:  1.423002\n",
            "train epoch: 6 | batch staus: 3840/5000 ( 77%) | Loss:  2.324431\n",
            "train epoch: 6 | batch staus: 3920/5000 ( 78%) | Loss:  2.053175\n",
            "train epoch: 6 | batch staus: 4000/5000 ( 80%) | Loss:  1.855132\n",
            "train epoch: 6 | batch staus: 4080/5000 ( 82%) | Loss:  1.411307\n",
            "train epoch: 6 | batch staus: 4160/5000 ( 83%) | Loss:  1.667723\n",
            "train epoch: 6 | batch staus: 4240/5000 ( 85%) | Loss:  2.082428\n",
            "train epoch: 6 | batch staus: 4320/5000 ( 86%) | Loss:  1.854339\n",
            "train epoch: 6 | batch staus: 4400/5000 ( 88%) | Loss:  1.922561\n",
            "train epoch: 6 | batch staus: 4480/5000 ( 90%) | Loss:  1.699383\n",
            "train epoch: 6 | batch staus: 4560/5000 ( 91%) | Loss:  1.216784\n",
            "train epoch: 6 | batch staus: 4640/5000 ( 93%) | Loss:  1.550406\n",
            "train epoch: 6 | batch staus: 4720/5000 ( 94%) | Loss:  1.822885\n",
            "train epoch: 6 | batch staus: 4800/5000 ( 96%) | Loss:  1.626540\n",
            "train epoch: 6 | batch staus: 4880/5000 ( 98%) | Loss:  1.720018\n",
            "train epoch: 6 | batch staus: 4960/5000 ( 99%) | Loss:  1.587795\n",
            "Training time: 18m 41s\n",
            "=======\n",
            " test set: average loss:  0.2089, Accuracy: 2585/8000(32%)\n",
            "Testing time: 28m 38s\n",
            "train epoch: 7 | batch staus: 0/5000 ( 0%) | Loss:  1.457260\n",
            "train epoch: 7 | batch staus: 80/5000 ( 2%) | Loss:  1.704258\n",
            "train epoch: 7 | batch staus: 160/5000 ( 3%) | Loss:  1.613990\n",
            "train epoch: 7 | batch staus: 240/5000 ( 5%) | Loss:  1.418945\n",
            "train epoch: 7 | batch staus: 320/5000 ( 6%) | Loss:  1.598280\n",
            "train epoch: 7 | batch staus: 400/5000 ( 8%) | Loss:  1.333327\n",
            "train epoch: 7 | batch staus: 480/5000 ( 10%) | Loss:  1.552772\n",
            "train epoch: 7 | batch staus: 560/5000 ( 11%) | Loss:  1.875291\n",
            "train epoch: 7 | batch staus: 640/5000 ( 13%) | Loss:  1.793002\n",
            "train epoch: 7 | batch staus: 720/5000 ( 14%) | Loss:  1.724716\n",
            "train epoch: 7 | batch staus: 800/5000 ( 16%) | Loss:  1.750443\n",
            "train epoch: 7 | batch staus: 880/5000 ( 18%) | Loss:  1.531852\n",
            "train epoch: 7 | batch staus: 960/5000 ( 19%) | Loss:  1.945695\n",
            "train epoch: 7 | batch staus: 1040/5000 ( 21%) | Loss:  1.413507\n",
            "train epoch: 7 | batch staus: 1120/5000 ( 22%) | Loss:  1.643226\n",
            "train epoch: 7 | batch staus: 1200/5000 ( 24%) | Loss:  1.471386\n",
            "train epoch: 7 | batch staus: 1280/5000 ( 26%) | Loss:  1.509969\n",
            "train epoch: 7 | batch staus: 1360/5000 ( 27%) | Loss:  1.517588\n",
            "train epoch: 7 | batch staus: 1440/5000 ( 29%) | Loss:  1.545774\n",
            "train epoch: 7 | batch staus: 1520/5000 ( 30%) | Loss:  2.398564\n",
            "train epoch: 7 | batch staus: 1600/5000 ( 32%) | Loss:  1.129035\n",
            "train epoch: 7 | batch staus: 1680/5000 ( 34%) | Loss:  1.601457\n",
            "train epoch: 7 | batch staus: 1760/5000 ( 35%) | Loss:  1.709364\n",
            "train epoch: 7 | batch staus: 1840/5000 ( 37%) | Loss:  2.064823\n",
            "train epoch: 7 | batch staus: 1920/5000 ( 38%) | Loss:  2.406462\n",
            "train epoch: 7 | batch staus: 2000/5000 ( 40%) | Loss:  2.234234\n",
            "train epoch: 7 | batch staus: 2080/5000 ( 42%) | Loss:  1.827913\n",
            "train epoch: 7 | batch staus: 2160/5000 ( 43%) | Loss:  2.342149\n",
            "train epoch: 7 | batch staus: 2240/5000 ( 45%) | Loss:  1.714270\n",
            "train epoch: 7 | batch staus: 2320/5000 ( 46%) | Loss:  1.728699\n",
            "train epoch: 7 | batch staus: 2400/5000 ( 48%) | Loss:  1.581849\n",
            "train epoch: 7 | batch staus: 2480/5000 ( 50%) | Loss:  2.012257\n",
            "train epoch: 7 | batch staus: 2560/5000 ( 51%) | Loss:  1.452091\n",
            "train epoch: 7 | batch staus: 2640/5000 ( 53%) | Loss:  1.574478\n",
            "train epoch: 7 | batch staus: 2720/5000 ( 54%) | Loss:  1.441451\n",
            "train epoch: 7 | batch staus: 2800/5000 ( 56%) | Loss:  1.770433\n",
            "train epoch: 7 | batch staus: 2880/5000 ( 58%) | Loss:  1.785504\n",
            "train epoch: 7 | batch staus: 2960/5000 ( 59%) | Loss:  1.529507\n",
            "train epoch: 7 | batch staus: 3040/5000 ( 61%) | Loss:  2.102659\n",
            "train epoch: 7 | batch staus: 3120/5000 ( 62%) | Loss:  1.816702\n",
            "train epoch: 7 | batch staus: 3200/5000 ( 64%) | Loss:  1.954903\n",
            "train epoch: 7 | batch staus: 3280/5000 ( 66%) | Loss:  1.984198\n",
            "train epoch: 7 | batch staus: 3360/5000 ( 67%) | Loss:  1.726828\n",
            "train epoch: 7 | batch staus: 3440/5000 ( 69%) | Loss:  1.325256\n",
            "train epoch: 7 | batch staus: 3520/5000 ( 70%) | Loss:  1.540371\n",
            "train epoch: 7 | batch staus: 3600/5000 ( 72%) | Loss:  1.540914\n",
            "train epoch: 7 | batch staus: 3680/5000 ( 74%) | Loss:  1.872688\n",
            "train epoch: 7 | batch staus: 3760/5000 ( 75%) | Loss:  2.375906\n",
            "train epoch: 7 | batch staus: 3840/5000 ( 77%) | Loss:  1.874491\n",
            "train epoch: 7 | batch staus: 3920/5000 ( 78%) | Loss:  2.055230\n",
            "train epoch: 7 | batch staus: 4000/5000 ( 80%) | Loss:  1.699172\n",
            "train epoch: 7 | batch staus: 4080/5000 ( 82%) | Loss:  2.623418\n",
            "train epoch: 7 | batch staus: 4160/5000 ( 83%) | Loss:  1.686018\n",
            "train epoch: 7 | batch staus: 4240/5000 ( 85%) | Loss:  1.682853\n",
            "train epoch: 7 | batch staus: 4320/5000 ( 86%) | Loss:  2.086977\n",
            "train epoch: 7 | batch staus: 4400/5000 ( 88%) | Loss:  1.556684\n",
            "train epoch: 7 | batch staus: 4480/5000 ( 90%) | Loss:  2.180593\n",
            "train epoch: 7 | batch staus: 4560/5000 ( 91%) | Loss:  1.470923\n",
            "train epoch: 7 | batch staus: 4640/5000 ( 93%) | Loss:  1.712786\n",
            "train epoch: 7 | batch staus: 4720/5000 ( 94%) | Loss:  1.662411\n",
            "train epoch: 7 | batch staus: 4800/5000 ( 96%) | Loss:  1.615330\n",
            "train epoch: 7 | batch staus: 4880/5000 ( 98%) | Loss:  1.664990\n",
            "train epoch: 7 | batch staus: 4960/5000 ( 99%) | Loss:  1.592706\n",
            "Training time: 18m 40s\n",
            "=======\n",
            " test set: average loss:  0.1998, Accuracy: 2861/8000(36%)\n",
            "Testing time: 28m 33s\n",
            "total time: 200m  45s \n",
            " model was trained on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7miS1Urpd0",
        "outputId": "5bf9712b-ff46-46ae-9a28-0154717d86b8"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('densenet')\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "path2data = '/content/densenet/MyDrive/data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "train_dataset = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class BottleNeck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        inner_channels = 4 * growth_rate\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(inner_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n",
        "\n",
        "\n",
        "# Transition Block: reduce feature map size and number of channels\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "\n",
        "# DenseNet\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.growth_rate = growth_rate\n",
        "        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, inner_channels, 7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(3, 2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for i in range(len(nblocks)-1):\n",
        "            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n",
        "            inner_channels += growth_rate * nblocks[i]\n",
        "            out_channels = int(reduction * inner_channels)\n",
        "            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels \n",
        "        \n",
        "        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('relu', nn.ReLU())\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(inner_channels, num_classes)\n",
        "\n",
        "        # weight initialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.features(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def _make_dense_block(self, nblock, inner_channels):\n",
        "        dense_block = nn.Sequential()\n",
        "        for i in range(nblock):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n",
        "            inner_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def DenseNet_121():\n",
        "    return DenseNet([6, 12, 24, 6])\n",
        "model = DenseNet_121()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at densenet; to attempt to forcibly remount, call drive.mount(\"densenet\", force_remount=True).\n",
            "traing model on cuda\n",
            "============================================\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train epoch: 1 | batch staus: 0/5000 ( 0%) | Loss:  2.308819\n",
            "train epoch: 1 | batch staus: 640/5000 ( 13%) | Loss:  2.285026\n",
            "train epoch: 1 | batch staus: 1280/5000 ( 25%) | Loss:  2.239379\n",
            "train epoch: 1 | batch staus: 1920/5000 ( 38%) | Loss:  2.206953\n",
            "train epoch: 1 | batch staus: 2560/5000 ( 51%) | Loss:  2.212089\n",
            "train epoch: 1 | batch staus: 3200/5000 ( 63%) | Loss:  2.165587\n",
            "train epoch: 1 | batch staus: 3840/5000 ( 76%) | Loss:  2.131694\n",
            "train epoch: 1 | batch staus: 4480/5000 ( 89%) | Loss:  2.056072\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0324, Accuracy: 2179/8000(27%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 2 | batch staus: 0/5000 ( 0%) | Loss:  2.066060\n",
            "train epoch: 2 | batch staus: 640/5000 ( 13%) | Loss:  2.033450\n",
            "train epoch: 2 | batch staus: 1280/5000 ( 25%) | Loss:  1.940072\n",
            "train epoch: 2 | batch staus: 1920/5000 ( 38%) | Loss:  1.943460\n",
            "train epoch: 2 | batch staus: 2560/5000 ( 51%) | Loss:  1.921139\n",
            "train epoch: 2 | batch staus: 3200/5000 ( 63%) | Loss:  1.887898\n",
            "train epoch: 2 | batch staus: 3840/5000 ( 76%) | Loss:  1.870813\n",
            "train epoch: 2 | batch staus: 4480/5000 ( 89%) | Loss:  1.905525\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0325, Accuracy: 1826/8000(23%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 3 | batch staus: 0/5000 ( 0%) | Loss:  1.898758\n",
            "train epoch: 3 | batch staus: 640/5000 ( 13%) | Loss:  1.781021\n",
            "train epoch: 3 | batch staus: 1280/5000 ( 25%) | Loss:  1.844861\n",
            "train epoch: 3 | batch staus: 1920/5000 ( 38%) | Loss:  1.842050\n",
            "train epoch: 3 | batch staus: 2560/5000 ( 51%) | Loss:  1.866751\n",
            "train epoch: 3 | batch staus: 3200/5000 ( 63%) | Loss:  1.807223\n",
            "train epoch: 3 | batch staus: 3840/5000 ( 76%) | Loss:  1.786574\n",
            "train epoch: 3 | batch staus: 4480/5000 ( 89%) | Loss:  1.743847\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0295, Accuracy: 2381/8000(30%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 4 | batch staus: 0/5000 ( 0%) | Loss:  1.790169\n",
            "train epoch: 4 | batch staus: 640/5000 ( 13%) | Loss:  1.777617\n",
            "train epoch: 4 | batch staus: 1280/5000 ( 25%) | Loss:  1.700735\n",
            "train epoch: 4 | batch staus: 1920/5000 ( 38%) | Loss:  1.816743\n",
            "train epoch: 4 | batch staus: 2560/5000 ( 51%) | Loss:  1.857927\n",
            "train epoch: 4 | batch staus: 3200/5000 ( 63%) | Loss:  1.609918\n",
            "train epoch: 4 | batch staus: 3840/5000 ( 76%) | Loss:  1.789644\n",
            "train epoch: 4 | batch staus: 4480/5000 ( 89%) | Loss:  1.620402\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0389, Accuracy: 1771/8000(22%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 5 | batch staus: 0/5000 ( 0%) | Loss:  1.660998\n",
            "train epoch: 5 | batch staus: 640/5000 ( 13%) | Loss:  1.470306\n",
            "train epoch: 5 | batch staus: 1280/5000 ( 25%) | Loss:  1.792032\n",
            "train epoch: 5 | batch staus: 1920/5000 ( 38%) | Loss:  1.765656\n",
            "train epoch: 5 | batch staus: 2560/5000 ( 51%) | Loss:  1.782431\n",
            "train epoch: 5 | batch staus: 3200/5000 ( 63%) | Loss:  1.619244\n",
            "train epoch: 5 | batch staus: 3840/5000 ( 76%) | Loss:  1.518321\n",
            "train epoch: 5 | batch staus: 4480/5000 ( 89%) | Loss:  1.672554\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0289, Accuracy: 2212/8000(28%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 6 | batch staus: 0/5000 ( 0%) | Loss:  1.685555\n",
            "train epoch: 6 | batch staus: 640/5000 ( 13%) | Loss:  1.633986\n",
            "train epoch: 6 | batch staus: 1280/5000 ( 25%) | Loss:  1.589686\n",
            "train epoch: 6 | batch staus: 1920/5000 ( 38%) | Loss:  1.656419\n",
            "train epoch: 6 | batch staus: 2560/5000 ( 51%) | Loss:  1.672683\n",
            "train epoch: 6 | batch staus: 3200/5000 ( 63%) | Loss:  1.661108\n",
            "train epoch: 6 | batch staus: 3840/5000 ( 76%) | Loss:  1.688173\n",
            "train epoch: 6 | batch staus: 4480/5000 ( 89%) | Loss:  1.527275\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0274, Accuracy: 2546/8000(32%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 7 | batch staus: 0/5000 ( 0%) | Loss:  1.610536\n",
            "train epoch: 7 | batch staus: 640/5000 ( 13%) | Loss:  1.711881\n",
            "train epoch: 7 | batch staus: 1280/5000 ( 25%) | Loss:  1.526184\n",
            "train epoch: 7 | batch staus: 1920/5000 ( 38%) | Loss:  1.712016\n",
            "train epoch: 7 | batch staus: 2560/5000 ( 51%) | Loss:  1.783898\n",
            "train epoch: 7 | batch staus: 3200/5000 ( 63%) | Loss:  1.779588\n",
            "train epoch: 7 | batch staus: 3840/5000 ( 76%) | Loss:  1.631873\n",
            "train epoch: 7 | batch staus: 4480/5000 ( 89%) | Loss:  1.637943\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0271, Accuracy: 2703/8000(34%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 8 | batch staus: 0/5000 ( 0%) | Loss:  1.591097\n",
            "train epoch: 8 | batch staus: 640/5000 ( 13%) | Loss:  1.693871\n",
            "train epoch: 8 | batch staus: 1280/5000 ( 25%) | Loss:  1.514482\n",
            "train epoch: 8 | batch staus: 1920/5000 ( 38%) | Loss:  1.492260\n",
            "train epoch: 8 | batch staus: 2560/5000 ( 51%) | Loss:  1.473762\n",
            "train epoch: 8 | batch staus: 3200/5000 ( 63%) | Loss:  1.561167\n",
            "train epoch: 8 | batch staus: 3840/5000 ( 76%) | Loss:  1.431160\n",
            "train epoch: 8 | batch staus: 4480/5000 ( 89%) | Loss:  1.499656\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0283, Accuracy: 2404/8000(30%)\n",
            "Testing time: 0m 24s\n",
            "train epoch: 9 | batch staus: 0/5000 ( 0%) | Loss:  1.578632\n",
            "train epoch: 9 | batch staus: 640/5000 ( 13%) | Loss:  1.537735\n",
            "train epoch: 9 | batch staus: 1280/5000 ( 25%) | Loss:  1.589549\n",
            "train epoch: 9 | batch staus: 1920/5000 ( 38%) | Loss:  1.514706\n",
            "train epoch: 9 | batch staus: 2560/5000 ( 51%) | Loss:  1.493783\n",
            "train epoch: 9 | batch staus: 3200/5000 ( 63%) | Loss:  1.686725\n",
            "train epoch: 9 | batch staus: 3840/5000 ( 76%) | Loss:  1.470229\n",
            "train epoch: 9 | batch staus: 4480/5000 ( 89%) | Loss:  1.579228\n",
            "Training time: 0m 13s\n",
            "=======\n",
            " test set: average loss:  0.0290, Accuracy: 2518/8000(31%)\n",
            "Testing time: 0m 24s\n",
            "total time: 3m  34s \n",
            " model was trained on cuda!\n"
          ]
        }
      ]
    }
  ]
}