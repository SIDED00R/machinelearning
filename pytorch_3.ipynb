{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_3.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/SIDED00R/machinelearning/blob/main/pytorch_3.ipynb",
      "authorship_tag": "ABX9TyOWQALjQzEsGdjoPyKXOa9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/pytorch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olJ4ezNkZzt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sersflt9XK-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10542a7-157f-4b40-a0c1-2db033072f10"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(y_data)\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXj1fl-mknj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e1d7b6-dbf1-4ead-8231-7f95c1447f4a"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print(f'Epoch: {epoch+1}/100 | Loss: {loss.item(): .4}')\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 | Loss:  0.6622\n",
            "Epoch: 2/100 | Loss:  0.6612\n",
            "Epoch: 3/100 | Loss:  0.6601\n",
            "Epoch: 4/100 | Loss:  0.6592\n",
            "Epoch: 5/100 | Loss:  0.6582\n",
            "Epoch: 6/100 | Loss:  0.6574\n",
            "Epoch: 7/100 | Loss:  0.6565\n",
            "Epoch: 8/100 | Loss:  0.6557\n",
            "Epoch: 9/100 | Loss:  0.655\n",
            "Epoch: 10/100 | Loss:  0.6543\n",
            "Epoch: 11/100 | Loss:  0.6536\n",
            "Epoch: 12/100 | Loss:  0.6529\n",
            "Epoch: 13/100 | Loss:  0.6523\n",
            "Epoch: 14/100 | Loss:  0.6517\n",
            "Epoch: 15/100 | Loss:  0.6511\n",
            "Epoch: 16/100 | Loss:  0.6505\n",
            "Epoch: 17/100 | Loss:  0.65\n",
            "Epoch: 18/100 | Loss:  0.6495\n",
            "Epoch: 19/100 | Loss:  0.6489\n",
            "Epoch: 20/100 | Loss:  0.6484\n",
            "Epoch: 21/100 | Loss:  0.648\n",
            "Epoch: 22/100 | Loss:  0.6475\n",
            "Epoch: 23/100 | Loss:  0.6471\n",
            "Epoch: 24/100 | Loss:  0.6466\n",
            "Epoch: 25/100 | Loss:  0.6463\n",
            "Epoch: 26/100 | Loss:  0.6459\n",
            "Epoch: 27/100 | Loss:  0.6455\n",
            "Epoch: 28/100 | Loss:  0.6451\n",
            "Epoch: 29/100 | Loss:  0.6448\n",
            "Epoch: 30/100 | Loss:  0.6444\n",
            "Epoch: 31/100 | Loss:  0.6441\n",
            "Epoch: 32/100 | Loss:  0.6438\n",
            "Epoch: 33/100 | Loss:  0.6435\n",
            "Epoch: 34/100 | Loss:  0.6432\n",
            "Epoch: 35/100 | Loss:  0.6429\n",
            "Epoch: 36/100 | Loss:  0.6426\n",
            "Epoch: 37/100 | Loss:  0.6423\n",
            "Epoch: 38/100 | Loss:  0.6421\n",
            "Epoch: 39/100 | Loss:  0.6418\n",
            "Epoch: 40/100 | Loss:  0.6416\n",
            "Epoch: 41/100 | Loss:  0.6413\n",
            "Epoch: 42/100 | Loss:  0.6411\n",
            "Epoch: 43/100 | Loss:  0.6409\n",
            "Epoch: 44/100 | Loss:  0.6407\n",
            "Epoch: 45/100 | Loss:  0.6405\n",
            "Epoch: 46/100 | Loss:  0.6402\n",
            "Epoch: 47/100 | Loss:  0.64\n",
            "Epoch: 48/100 | Loss:  0.6398\n",
            "Epoch: 49/100 | Loss:  0.6396\n",
            "Epoch: 50/100 | Loss:  0.6395\n",
            "Epoch: 51/100 | Loss:  0.6393\n",
            "Epoch: 52/100 | Loss:  0.6391\n",
            "Epoch: 53/100 | Loss:  0.6389\n",
            "Epoch: 54/100 | Loss:  0.6387\n",
            "Epoch: 55/100 | Loss:  0.6385\n",
            "Epoch: 56/100 | Loss:  0.6384\n",
            "Epoch: 57/100 | Loss:  0.6382\n",
            "Epoch: 58/100 | Loss:  0.638\n",
            "Epoch: 59/100 | Loss:  0.6378\n",
            "Epoch: 60/100 | Loss:  0.6376\n",
            "Epoch: 61/100 | Loss:  0.6375\n",
            "Epoch: 62/100 | Loss:  0.6373\n",
            "Epoch: 63/100 | Loss:  0.6371\n",
            "Epoch: 64/100 | Loss:  0.6369\n",
            "Epoch: 65/100 | Loss:  0.6368\n",
            "Epoch: 66/100 | Loss:  0.6366\n",
            "Epoch: 67/100 | Loss:  0.6364\n",
            "Epoch: 68/100 | Loss:  0.6362\n",
            "Epoch: 69/100 | Loss:  0.6361\n",
            "Epoch: 70/100 | Loss:  0.6359\n",
            "Epoch: 71/100 | Loss:  0.6357\n",
            "Epoch: 72/100 | Loss:  0.6355\n",
            "Epoch: 73/100 | Loss:  0.6353\n",
            "Epoch: 74/100 | Loss:  0.6351\n",
            "Epoch: 75/100 | Loss:  0.635\n",
            "Epoch: 76/100 | Loss:  0.6348\n",
            "Epoch: 77/100 | Loss:  0.6346\n",
            "Epoch: 78/100 | Loss:  0.6344\n",
            "Epoch: 79/100 | Loss:  0.6342\n",
            "Epoch: 80/100 | Loss:  0.634\n",
            "Epoch: 81/100 | Loss:  0.6338\n",
            "Epoch: 82/100 | Loss:  0.6336\n",
            "Epoch: 83/100 | Loss:  0.6334\n",
            "Epoch: 84/100 | Loss:  0.6332\n",
            "Epoch: 85/100 | Loss:  0.633\n",
            "Epoch: 86/100 | Loss:  0.6327\n",
            "Epoch: 87/100 | Loss:  0.6325\n",
            "Epoch: 88/100 | Loss:  0.6323\n",
            "Epoch: 89/100 | Loss:  0.6321\n",
            "Epoch: 90/100 | Loss:  0.6319\n",
            "Epoch: 91/100 | Loss:  0.6316\n",
            "Epoch: 92/100 | Loss:  0.6314\n",
            "Epoch: 93/100 | Loss:  0.6312\n",
            "Epoch: 94/100 | Loss:  0.6309\n",
            "Epoch: 95/100 | Loss:  0.6307\n",
            "Epoch: 96/100 | Loss:  0.6305\n",
            "Epoch: 97/100 | Loss:  0.6302\n",
            "Epoch: 98/100 | Loss:  0.63\n",
            "Epoch: 99/100 | Loss:  0.6297\n",
            "Epoch: 100/100 | Loss:  0.6295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsusvxhRn85h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b44db50-8048-4b36-87a1-585c8a3347e5"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "    self.x_data=from_numpy(xy[:, 0:-1])\n",
        "    self.y_data=from_numpy(xy[:, [-1]])\n",
        "    self.len=xy.shape[0]\n",
        "\n",
        "    \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader,0):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    print(f'Epoch{epoch+1} | Batch: {i+1} Loss: {loss.item(): .4f}')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch1 | Batch: 1 Loss:  0.7388\n",
            "Epoch1 | Batch: 2 Loss:  0.7333\n",
            "Epoch1 | Batch: 3 Loss:  0.7037\n",
            "Epoch1 | Batch: 4 Loss:  0.7389\n",
            "Epoch1 | Batch: 5 Loss:  0.7346\n",
            "Epoch1 | Batch: 6 Loss:  0.7196\n",
            "Epoch1 | Batch: 7 Loss:  0.6950\n",
            "Epoch1 | Batch: 8 Loss:  0.7075\n",
            "Epoch1 | Batch: 9 Loss:  0.7248\n",
            "Epoch1 | Batch: 10 Loss:  0.6984\n",
            "Epoch1 | Batch: 11 Loss:  0.7000\n",
            "Epoch1 | Batch: 12 Loss:  0.7024\n",
            "Epoch1 | Batch: 13 Loss:  0.6928\n",
            "Epoch1 | Batch: 14 Loss:  0.6904\n",
            "Epoch1 | Batch: 15 Loss:  0.6859\n",
            "Epoch1 | Batch: 16 Loss:  0.6919\n",
            "Epoch1 | Batch: 17 Loss:  0.6863\n",
            "Epoch1 | Batch: 18 Loss:  0.6833\n",
            "Epoch1 | Batch: 19 Loss:  0.6691\n",
            "Epoch1 | Batch: 20 Loss:  0.6877\n",
            "Epoch1 | Batch: 21 Loss:  0.6762\n",
            "Epoch1 | Batch: 22 Loss:  0.6857\n",
            "Epoch1 | Batch: 23 Loss:  0.6757\n",
            "Epoch1 | Batch: 24 Loss:  0.6807\n",
            "Epoch2 | Batch: 1 Loss:  0.6717\n",
            "Epoch2 | Batch: 2 Loss:  0.6608\n",
            "Epoch2 | Batch: 3 Loss:  0.6697\n",
            "Epoch2 | Batch: 4 Loss:  0.6436\n",
            "Epoch2 | Batch: 5 Loss:  0.6594\n",
            "Epoch2 | Batch: 6 Loss:  0.6556\n",
            "Epoch2 | Batch: 7 Loss:  0.6696\n",
            "Epoch2 | Batch: 8 Loss:  0.6697\n",
            "Epoch2 | Batch: 9 Loss:  0.6774\n",
            "Epoch2 | Batch: 10 Loss:  0.6852\n",
            "Epoch2 | Batch: 11 Loss:  0.6450\n",
            "Epoch2 | Batch: 12 Loss:  0.6429\n",
            "Epoch2 | Batch: 13 Loss:  0.7205\n",
            "Epoch2 | Batch: 14 Loss:  0.6418\n",
            "Epoch2 | Batch: 15 Loss:  0.6846\n",
            "Epoch2 | Batch: 16 Loss:  0.6496\n",
            "Epoch2 | Batch: 17 Loss:  0.6570\n",
            "Epoch2 | Batch: 18 Loss:  0.6554\n",
            "Epoch2 | Batch: 19 Loss:  0.6558\n",
            "Epoch2 | Batch: 20 Loss:  0.6453\n",
            "Epoch2 | Batch: 21 Loss:  0.6642\n",
            "Epoch2 | Batch: 22 Loss:  0.6550\n",
            "Epoch2 | Batch: 23 Loss:  0.6649\n",
            "Epoch2 | Batch: 24 Loss:  0.6378\n",
            "Epoch3 | Batch: 1 Loss:  0.6525\n",
            "Epoch3 | Batch: 2 Loss:  0.6632\n",
            "Epoch3 | Batch: 3 Loss:  0.6291\n",
            "Epoch3 | Batch: 4 Loss:  0.6979\n",
            "Epoch3 | Batch: 5 Loss:  0.6276\n",
            "Epoch3 | Batch: 6 Loss:  0.6385\n",
            "Epoch3 | Batch: 7 Loss:  0.6495\n",
            "Epoch3 | Batch: 8 Loss:  0.5992\n",
            "Epoch3 | Batch: 9 Loss:  0.6742\n",
            "Epoch3 | Batch: 10 Loss:  0.6609\n",
            "Epoch3 | Batch: 11 Loss:  0.6098\n",
            "Epoch3 | Batch: 12 Loss:  0.6894\n",
            "Epoch3 | Batch: 13 Loss:  0.6741\n",
            "Epoch3 | Batch: 14 Loss:  0.6889\n",
            "Epoch3 | Batch: 15 Loss:  0.7014\n",
            "Epoch3 | Batch: 16 Loss:  0.6226\n",
            "Epoch3 | Batch: 17 Loss:  0.6488\n",
            "Epoch3 | Batch: 18 Loss:  0.6487\n",
            "Epoch3 | Batch: 19 Loss:  0.6333\n",
            "Epoch3 | Batch: 20 Loss:  0.6339\n",
            "Epoch3 | Batch: 21 Loss:  0.5777\n",
            "Epoch3 | Batch: 22 Loss:  0.5717\n",
            "Epoch3 | Batch: 23 Loss:  0.7395\n",
            "Epoch3 | Batch: 24 Loss:  0.6882\n",
            "Epoch4 | Batch: 1 Loss:  0.6172\n",
            "Epoch4 | Batch: 2 Loss:  0.5852\n",
            "Epoch4 | Batch: 3 Loss:  0.5983\n",
            "Epoch4 | Batch: 4 Loss:  0.6766\n",
            "Epoch4 | Batch: 5 Loss:  0.7252\n",
            "Epoch4 | Batch: 6 Loss:  0.5665\n",
            "Epoch4 | Batch: 7 Loss:  0.6610\n",
            "Epoch4 | Batch: 8 Loss:  0.6122\n",
            "Epoch4 | Batch: 9 Loss:  0.7097\n",
            "Epoch4 | Batch: 10 Loss:  0.6604\n",
            "Epoch4 | Batch: 11 Loss:  0.6436\n",
            "Epoch4 | Batch: 12 Loss:  0.6290\n",
            "Epoch4 | Batch: 13 Loss:  0.6767\n",
            "Epoch4 | Batch: 14 Loss:  0.6125\n",
            "Epoch4 | Batch: 15 Loss:  0.6444\n",
            "Epoch4 | Batch: 16 Loss:  0.6443\n",
            "Epoch4 | Batch: 17 Loss:  0.6267\n",
            "Epoch4 | Batch: 18 Loss:  0.6609\n",
            "Epoch4 | Batch: 19 Loss:  0.6944\n",
            "Epoch4 | Batch: 20 Loss:  0.6277\n",
            "Epoch4 | Batch: 21 Loss:  0.6775\n",
            "Epoch4 | Batch: 22 Loss:  0.6781\n",
            "Epoch4 | Batch: 23 Loss:  0.6440\n",
            "Epoch4 | Batch: 24 Loss:  0.6473\n",
            "Epoch5 | Batch: 1 Loss:  0.7120\n",
            "Epoch5 | Batch: 2 Loss:  0.6613\n",
            "Epoch5 | Batch: 3 Loss:  0.6110\n",
            "Epoch5 | Batch: 4 Loss:  0.5932\n",
            "Epoch5 | Batch: 5 Loss:  0.6607\n",
            "Epoch5 | Batch: 6 Loss:  0.6283\n",
            "Epoch5 | Batch: 7 Loss:  0.6425\n",
            "Epoch5 | Batch: 8 Loss:  0.5748\n",
            "Epoch5 | Batch: 9 Loss:  0.7141\n",
            "Epoch5 | Batch: 10 Loss:  0.6960\n",
            "Epoch5 | Batch: 11 Loss:  0.5582\n",
            "Epoch5 | Batch: 12 Loss:  0.6601\n",
            "Epoch5 | Batch: 13 Loss:  0.7132\n",
            "Epoch5 | Batch: 14 Loss:  0.6948\n",
            "Epoch5 | Batch: 15 Loss:  0.6441\n",
            "Epoch5 | Batch: 16 Loss:  0.6625\n",
            "Epoch5 | Batch: 17 Loss:  0.6443\n",
            "Epoch5 | Batch: 18 Loss:  0.5737\n",
            "Epoch5 | Batch: 19 Loss:  0.6071\n",
            "Epoch5 | Batch: 20 Loss:  0.6244\n",
            "Epoch5 | Batch: 21 Loss:  0.6613\n",
            "Epoch5 | Batch: 22 Loss:  0.6428\n",
            "Epoch5 | Batch: 23 Loss:  0.6779\n",
            "Epoch5 | Batch: 24 Loss:  0.6218\n",
            "Epoch6 | Batch: 1 Loss:  0.5899\n",
            "Epoch6 | Batch: 2 Loss:  0.7367\n",
            "Epoch6 | Batch: 3 Loss:  0.6239\n",
            "Epoch6 | Batch: 4 Loss:  0.5890\n",
            "Epoch6 | Batch: 5 Loss:  0.6436\n",
            "Epoch6 | Batch: 6 Loss:  0.6064\n",
            "Epoch6 | Batch: 7 Loss:  0.6620\n",
            "Epoch6 | Batch: 8 Loss:  0.6974\n",
            "Epoch6 | Batch: 9 Loss:  0.6599\n",
            "Epoch6 | Batch: 10 Loss:  0.6063\n",
            "Epoch6 | Batch: 11 Loss:  0.6982\n",
            "Epoch6 | Batch: 12 Loss:  0.6975\n",
            "Epoch6 | Batch: 13 Loss:  0.6772\n",
            "Epoch6 | Batch: 14 Loss:  0.5891\n",
            "Epoch6 | Batch: 15 Loss:  0.6260\n",
            "Epoch6 | Batch: 16 Loss:  0.5676\n",
            "Epoch6 | Batch: 17 Loss:  0.6240\n",
            "Epoch6 | Batch: 18 Loss:  0.7370\n",
            "Epoch6 | Batch: 19 Loss:  0.6042\n",
            "Epoch6 | Batch: 20 Loss:  0.6619\n",
            "Epoch6 | Batch: 21 Loss:  0.6418\n",
            "Epoch6 | Batch: 22 Loss:  0.6990\n",
            "Epoch6 | Batch: 23 Loss:  0.6423\n",
            "Epoch6 | Batch: 24 Loss:  0.5669\n",
            "Epoch7 | Batch: 1 Loss:  0.6243\n",
            "Epoch7 | Batch: 2 Loss:  0.6230\n",
            "Epoch7 | Batch: 3 Loss:  0.6043\n",
            "Epoch7 | Batch: 4 Loss:  0.6603\n",
            "Epoch7 | Batch: 5 Loss:  0.6614\n",
            "Epoch7 | Batch: 6 Loss:  0.7761\n",
            "Epoch7 | Batch: 7 Loss:  0.5860\n",
            "Epoch7 | Batch: 8 Loss:  0.6046\n",
            "Epoch7 | Batch: 9 Loss:  0.6804\n",
            "Epoch7 | Batch: 10 Loss:  0.6425\n",
            "Epoch7 | Batch: 11 Loss:  0.6237\n",
            "Epoch7 | Batch: 12 Loss:  0.6026\n",
            "Epoch7 | Batch: 13 Loss:  0.6231\n",
            "Epoch7 | Batch: 14 Loss:  0.6211\n",
            "Epoch7 | Batch: 15 Loss:  0.5471\n",
            "Epoch7 | Batch: 16 Loss:  0.5814\n",
            "Epoch7 | Batch: 17 Loss:  0.7230\n",
            "Epoch7 | Batch: 18 Loss:  0.7216\n",
            "Epoch7 | Batch: 19 Loss:  0.7016\n",
            "Epoch7 | Batch: 20 Loss:  0.6611\n",
            "Epoch7 | Batch: 21 Loss:  0.6223\n",
            "Epoch7 | Batch: 22 Loss:  0.6416\n",
            "Epoch7 | Batch: 23 Loss:  0.6402\n",
            "Epoch7 | Batch: 24 Loss:  0.6976\n",
            "Epoch8 | Batch: 1 Loss:  0.7520\n",
            "Epoch8 | Batch: 2 Loss:  0.6435\n",
            "Epoch8 | Batch: 3 Loss:  0.6941\n",
            "Epoch8 | Batch: 4 Loss:  0.6253\n",
            "Epoch8 | Batch: 5 Loss:  0.6609\n",
            "Epoch8 | Batch: 6 Loss:  0.6580\n",
            "Epoch8 | Batch: 7 Loss:  0.6983\n",
            "Epoch8 | Batch: 8 Loss:  0.5543\n",
            "Epoch8 | Batch: 9 Loss:  0.5493\n",
            "Epoch8 | Batch: 10 Loss:  0.7562\n",
            "Epoch8 | Batch: 11 Loss:  0.7137\n",
            "Epoch8 | Batch: 12 Loss:  0.5700\n",
            "Epoch8 | Batch: 13 Loss:  0.6263\n",
            "Epoch8 | Batch: 14 Loss:  0.6233\n",
            "Epoch8 | Batch: 15 Loss:  0.6405\n",
            "Epoch8 | Batch: 16 Loss:  0.6599\n",
            "Epoch8 | Batch: 17 Loss:  0.6789\n",
            "Epoch8 | Batch: 18 Loss:  0.5702\n",
            "Epoch8 | Batch: 19 Loss:  0.6774\n",
            "Epoch8 | Batch: 20 Loss:  0.5834\n",
            "Epoch8 | Batch: 21 Loss:  0.6787\n",
            "Epoch8 | Batch: 22 Loss:  0.6603\n",
            "Epoch8 | Batch: 23 Loss:  0.5643\n",
            "Epoch8 | Batch: 24 Loss:  0.5906\n",
            "Epoch9 | Batch: 1 Loss:  0.6397\n",
            "Epoch9 | Batch: 2 Loss:  0.6024\n",
            "Epoch9 | Batch: 3 Loss:  0.5825\n",
            "Epoch9 | Batch: 4 Loss:  0.6407\n",
            "Epoch9 | Batch: 5 Loss:  0.6608\n",
            "Epoch9 | Batch: 6 Loss:  0.6397\n",
            "Epoch9 | Batch: 7 Loss:  0.6408\n",
            "Epoch9 | Batch: 8 Loss:  0.6615\n",
            "Epoch9 | Batch: 9 Loss:  0.7195\n",
            "Epoch9 | Batch: 10 Loss:  0.6393\n",
            "Epoch9 | Batch: 11 Loss:  0.5829\n",
            "Epoch9 | Batch: 12 Loss:  0.6995\n",
            "Epoch9 | Batch: 13 Loss:  0.7394\n",
            "Epoch9 | Batch: 14 Loss:  0.7133\n",
            "Epoch9 | Batch: 15 Loss:  0.6400\n",
            "Epoch9 | Batch: 16 Loss:  0.6031\n",
            "Epoch9 | Batch: 17 Loss:  0.6561\n",
            "Epoch9 | Batch: 18 Loss:  0.5851\n",
            "Epoch9 | Batch: 19 Loss:  0.6397\n",
            "Epoch9 | Batch: 20 Loss:  0.6228\n",
            "Epoch9 | Batch: 21 Loss:  0.7004\n",
            "Epoch9 | Batch: 22 Loss:  0.6415\n",
            "Epoch9 | Batch: 23 Loss:  0.6028\n",
            "Epoch9 | Batch: 24 Loss:  0.5385\n",
            "Epoch10 | Batch: 1 Loss:  0.5830\n",
            "Epoch10 | Batch: 2 Loss:  0.6395\n",
            "Epoch10 | Batch: 3 Loss:  0.6799\n",
            "Epoch10 | Batch: 4 Loss:  0.6395\n",
            "Epoch10 | Batch: 5 Loss:  0.6987\n",
            "Epoch10 | Batch: 6 Loss:  0.5611\n",
            "Epoch10 | Batch: 7 Loss:  0.5398\n",
            "Epoch10 | Batch: 8 Loss:  0.5805\n",
            "Epoch10 | Batch: 9 Loss:  0.6211\n",
            "Epoch10 | Batch: 10 Loss:  0.6824\n",
            "Epoch10 | Batch: 11 Loss:  0.5780\n",
            "Epoch10 | Batch: 12 Loss:  0.5763\n",
            "Epoch10 | Batch: 13 Loss:  0.6371\n",
            "Epoch10 | Batch: 14 Loss:  0.8075\n",
            "Epoch10 | Batch: 15 Loss:  0.6001\n",
            "Epoch10 | Batch: 16 Loss:  0.7174\n",
            "Epoch10 | Batch: 17 Loss:  0.7184\n",
            "Epoch10 | Batch: 18 Loss:  0.6607\n",
            "Epoch10 | Batch: 19 Loss:  0.5641\n",
            "Epoch10 | Batch: 20 Loss:  0.6560\n",
            "Epoch10 | Batch: 21 Loss:  0.7781\n",
            "Epoch10 | Batch: 22 Loss:  0.6748\n",
            "Epoch10 | Batch: 23 Loss:  0.6221\n",
            "Epoch10 | Batch: 24 Loss:  0.5662\n",
            "Epoch11 | Batch: 1 Loss:  0.5262\n",
            "Epoch11 | Batch: 2 Loss:  0.5987\n",
            "Epoch11 | Batch: 3 Loss:  0.6991\n",
            "Epoch11 | Batch: 4 Loss:  0.6651\n",
            "Epoch11 | Batch: 5 Loss:  0.6152\n",
            "Epoch11 | Batch: 6 Loss:  0.6387\n",
            "Epoch11 | Batch: 7 Loss:  0.5799\n",
            "Epoch11 | Batch: 8 Loss:  0.6594\n",
            "Epoch11 | Batch: 9 Loss:  0.6726\n",
            "Epoch11 | Batch: 10 Loss:  0.6948\n",
            "Epoch11 | Batch: 11 Loss:  0.6572\n",
            "Epoch11 | Batch: 12 Loss:  0.6929\n",
            "Epoch11 | Batch: 13 Loss:  0.5070\n",
            "Epoch11 | Batch: 14 Loss:  0.5781\n",
            "Epoch11 | Batch: 15 Loss:  0.7046\n",
            "Epoch11 | Batch: 16 Loss:  0.6555\n",
            "Epoch11 | Batch: 17 Loss:  0.7168\n",
            "Epoch11 | Batch: 18 Loss:  0.6352\n",
            "Epoch11 | Batch: 19 Loss:  0.7134\n",
            "Epoch11 | Batch: 20 Loss:  0.6392\n",
            "Epoch11 | Batch: 21 Loss:  0.6767\n",
            "Epoch11 | Batch: 22 Loss:  0.6893\n",
            "Epoch11 | Batch: 23 Loss:  0.5655\n",
            "Epoch11 | Batch: 24 Loss:  0.5654\n",
            "Epoch12 | Batch: 1 Loss:  0.5450\n",
            "Epoch12 | Batch: 2 Loss:  0.8333\n",
            "Epoch12 | Batch: 3 Loss:  0.6388\n",
            "Epoch12 | Batch: 4 Loss:  0.6568\n",
            "Epoch12 | Batch: 5 Loss:  0.6760\n",
            "Epoch12 | Batch: 6 Loss:  0.6357\n",
            "Epoch12 | Batch: 7 Loss:  0.6948\n",
            "Epoch12 | Batch: 8 Loss:  0.6042\n",
            "Epoch12 | Batch: 9 Loss:  0.6020\n",
            "Epoch12 | Batch: 10 Loss:  0.6579\n",
            "Epoch12 | Batch: 11 Loss:  0.6236\n",
            "Epoch12 | Batch: 12 Loss:  0.5935\n",
            "Epoch12 | Batch: 13 Loss:  0.6475\n",
            "Epoch12 | Batch: 14 Loss:  0.5980\n",
            "Epoch12 | Batch: 15 Loss:  0.6354\n",
            "Epoch12 | Batch: 16 Loss:  0.6125\n",
            "Epoch12 | Batch: 17 Loss:  0.6376\n",
            "Epoch12 | Batch: 18 Loss:  0.6374\n",
            "Epoch12 | Batch: 19 Loss:  0.5624\n",
            "Epoch12 | Batch: 20 Loss:  0.7670\n",
            "Epoch12 | Batch: 21 Loss:  0.6184\n",
            "Epoch12 | Batch: 22 Loss:  0.6126\n",
            "Epoch12 | Batch: 23 Loss:  0.6124\n",
            "Epoch12 | Batch: 24 Loss:  0.6106\n",
            "Epoch13 | Batch: 1 Loss:  0.7224\n",
            "Epoch13 | Batch: 2 Loss:  0.5932\n",
            "Epoch13 | Batch: 3 Loss:  0.5990\n",
            "Epoch13 | Batch: 4 Loss:  0.6392\n",
            "Epoch13 | Batch: 5 Loss:  0.6400\n",
            "Epoch13 | Batch: 6 Loss:  0.6308\n",
            "Epoch13 | Batch: 7 Loss:  0.6558\n",
            "Epoch13 | Batch: 8 Loss:  0.6853\n",
            "Epoch13 | Batch: 9 Loss:  0.6120\n",
            "Epoch13 | Batch: 10 Loss:  0.6902\n",
            "Epoch13 | Batch: 11 Loss:  0.6859\n",
            "Epoch13 | Batch: 12 Loss:  0.6152\n",
            "Epoch13 | Batch: 13 Loss:  0.5820\n",
            "Epoch13 | Batch: 14 Loss:  0.6122\n",
            "Epoch13 | Batch: 15 Loss:  0.6857\n",
            "Epoch13 | Batch: 16 Loss:  0.6075\n",
            "Epoch13 | Batch: 17 Loss:  0.6735\n",
            "Epoch13 | Batch: 18 Loss:  0.5016\n",
            "Epoch13 | Batch: 19 Loss:  0.5790\n",
            "Epoch13 | Batch: 20 Loss:  0.5941\n",
            "Epoch13 | Batch: 21 Loss:  0.6985\n",
            "Epoch13 | Batch: 22 Loss:  0.5406\n",
            "Epoch13 | Batch: 23 Loss:  0.6938\n",
            "Epoch13 | Batch: 24 Loss:  0.7476\n",
            "Epoch14 | Batch: 1 Loss:  0.6117\n",
            "Epoch14 | Batch: 2 Loss:  0.5960\n",
            "Epoch14 | Batch: 3 Loss:  0.5805\n",
            "Epoch14 | Batch: 4 Loss:  0.5909\n",
            "Epoch14 | Batch: 5 Loss:  0.7302\n",
            "Epoch14 | Batch: 6 Loss:  0.6442\n",
            "Epoch14 | Batch: 7 Loss:  0.6314\n",
            "Epoch14 | Batch: 8 Loss:  0.5981\n",
            "Epoch14 | Batch: 9 Loss:  0.6347\n",
            "Epoch14 | Batch: 10 Loss:  0.5933\n",
            "Epoch14 | Batch: 11 Loss:  0.6777\n",
            "Epoch14 | Batch: 12 Loss:  0.7216\n",
            "Epoch14 | Batch: 13 Loss:  0.6126\n",
            "Epoch14 | Batch: 14 Loss:  0.6865\n",
            "Epoch14 | Batch: 15 Loss:  0.6433\n",
            "Epoch14 | Batch: 16 Loss:  0.5779\n",
            "Epoch14 | Batch: 17 Loss:  0.6047\n",
            "Epoch14 | Batch: 18 Loss:  0.6587\n",
            "Epoch14 | Batch: 19 Loss:  0.6168\n",
            "Epoch14 | Batch: 20 Loss:  0.6014\n",
            "Epoch14 | Batch: 21 Loss:  0.6123\n",
            "Epoch14 | Batch: 22 Loss:  0.6693\n",
            "Epoch14 | Batch: 23 Loss:  0.5937\n",
            "Epoch14 | Batch: 24 Loss:  0.6779\n",
            "Epoch15 | Batch: 1 Loss:  0.6284\n",
            "Epoch15 | Batch: 2 Loss:  0.6524\n",
            "Epoch15 | Batch: 3 Loss:  0.6768\n",
            "Epoch15 | Batch: 4 Loss:  0.6400\n",
            "Epoch15 | Batch: 5 Loss:  0.5411\n",
            "Epoch15 | Batch: 6 Loss:  0.6142\n",
            "Epoch15 | Batch: 7 Loss:  0.6773\n",
            "Epoch15 | Batch: 8 Loss:  0.6079\n",
            "Epoch15 | Batch: 9 Loss:  0.5983\n",
            "Epoch15 | Batch: 10 Loss:  0.5327\n",
            "Epoch15 | Batch: 11 Loss:  0.5917\n",
            "Epoch15 | Batch: 12 Loss:  0.6376\n",
            "Epoch15 | Batch: 13 Loss:  0.6899\n",
            "Epoch15 | Batch: 14 Loss:  0.6256\n",
            "Epoch15 | Batch: 15 Loss:  0.5867\n",
            "Epoch15 | Batch: 16 Loss:  0.6839\n",
            "Epoch15 | Batch: 17 Loss:  0.6259\n",
            "Epoch15 | Batch: 18 Loss:  0.6140\n",
            "Epoch15 | Batch: 19 Loss:  0.5867\n",
            "Epoch15 | Batch: 20 Loss:  0.7196\n",
            "Epoch15 | Batch: 21 Loss:  0.6154\n",
            "Epoch15 | Batch: 22 Loss:  0.5651\n",
            "Epoch15 | Batch: 23 Loss:  0.6369\n",
            "Epoch15 | Batch: 24 Loss:  0.6507\n",
            "Epoch16 | Batch: 1 Loss:  0.6454\n",
            "Epoch16 | Batch: 2 Loss:  0.6081\n",
            "Epoch16 | Batch: 3 Loss:  0.6341\n",
            "Epoch16 | Batch: 4 Loss:  0.6869\n",
            "Epoch16 | Batch: 5 Loss:  0.6057\n",
            "Epoch16 | Batch: 6 Loss:  0.6130\n",
            "Epoch16 | Batch: 7 Loss:  0.6091\n",
            "Epoch16 | Batch: 8 Loss:  0.5576\n",
            "Epoch16 | Batch: 9 Loss:  0.6310\n",
            "Epoch16 | Batch: 10 Loss:  0.6519\n",
            "Epoch16 | Batch: 11 Loss:  0.6624\n",
            "Epoch16 | Batch: 12 Loss:  0.6211\n",
            "Epoch16 | Batch: 13 Loss:  0.5280\n",
            "Epoch16 | Batch: 14 Loss:  0.6561\n",
            "Epoch16 | Batch: 15 Loss:  0.5800\n",
            "Epoch16 | Batch: 16 Loss:  0.6088\n",
            "Epoch16 | Batch: 17 Loss:  0.6733\n",
            "Epoch16 | Batch: 18 Loss:  0.5329\n",
            "Epoch16 | Batch: 19 Loss:  0.5707\n",
            "Epoch16 | Batch: 20 Loss:  0.5527\n",
            "Epoch16 | Batch: 21 Loss:  0.5427\n",
            "Epoch16 | Batch: 22 Loss:  0.6950\n",
            "Epoch16 | Batch: 23 Loss:  0.6198\n",
            "Epoch16 | Batch: 24 Loss:  0.6636\n",
            "Epoch17 | Batch: 1 Loss:  0.6088\n",
            "Epoch17 | Batch: 2 Loss:  0.5988\n",
            "Epoch17 | Batch: 3 Loss:  0.5497\n",
            "Epoch17 | Batch: 4 Loss:  0.6196\n",
            "Epoch17 | Batch: 5 Loss:  0.5544\n",
            "Epoch17 | Batch: 6 Loss:  0.6582\n",
            "Epoch17 | Batch: 7 Loss:  0.5611\n",
            "Epoch17 | Batch: 8 Loss:  0.6488\n",
            "Epoch17 | Batch: 9 Loss:  0.7111\n",
            "Epoch17 | Batch: 10 Loss:  0.5683\n",
            "Epoch17 | Batch: 11 Loss:  0.6445\n",
            "Epoch17 | Batch: 12 Loss:  0.6416\n",
            "Epoch17 | Batch: 13 Loss:  0.6330\n",
            "Epoch17 | Batch: 14 Loss:  0.6074\n",
            "Epoch17 | Batch: 15 Loss:  0.5312\n",
            "Epoch17 | Batch: 16 Loss:  0.5496\n",
            "Epoch17 | Batch: 17 Loss:  0.5840\n",
            "Epoch17 | Batch: 18 Loss:  0.6043\n",
            "Epoch17 | Batch: 19 Loss:  0.5436\n",
            "Epoch17 | Batch: 20 Loss:  0.5595\n",
            "Epoch17 | Batch: 21 Loss:  0.6071\n",
            "Epoch17 | Batch: 22 Loss:  0.6330\n",
            "Epoch17 | Batch: 23 Loss:  0.5695\n",
            "Epoch17 | Batch: 24 Loss:  0.6312\n",
            "Epoch18 | Batch: 1 Loss:  0.5718\n",
            "Epoch18 | Batch: 2 Loss:  0.6478\n",
            "Epoch18 | Batch: 3 Loss:  0.6820\n",
            "Epoch18 | Batch: 4 Loss:  0.6046\n",
            "Epoch18 | Batch: 5 Loss:  0.6019\n",
            "Epoch18 | Batch: 6 Loss:  0.5618\n",
            "Epoch18 | Batch: 7 Loss:  0.5574\n",
            "Epoch18 | Batch: 8 Loss:  0.6357\n",
            "Epoch18 | Batch: 9 Loss:  0.5528\n",
            "Epoch18 | Batch: 10 Loss:  0.5172\n",
            "Epoch18 | Batch: 11 Loss:  0.6259\n",
            "Epoch18 | Batch: 12 Loss:  0.5635\n",
            "Epoch18 | Batch: 13 Loss:  0.5668\n",
            "Epoch18 | Batch: 14 Loss:  0.5950\n",
            "Epoch18 | Batch: 15 Loss:  0.6451\n",
            "Epoch18 | Batch: 16 Loss:  0.6443\n",
            "Epoch18 | Batch: 17 Loss:  0.5737\n",
            "Epoch18 | Batch: 18 Loss:  0.5778\n",
            "Epoch18 | Batch: 19 Loss:  0.5636\n",
            "Epoch18 | Batch: 20 Loss:  0.5133\n",
            "Epoch18 | Batch: 21 Loss:  0.5987\n",
            "Epoch18 | Batch: 22 Loss:  0.5358\n",
            "Epoch18 | Batch: 23 Loss:  0.5288\n",
            "Epoch18 | Batch: 24 Loss:  0.5136\n",
            "Epoch19 | Batch: 1 Loss:  0.5649\n",
            "Epoch19 | Batch: 2 Loss:  0.5989\n",
            "Epoch19 | Batch: 3 Loss:  0.5363\n",
            "Epoch19 | Batch: 4 Loss:  0.7085\n",
            "Epoch19 | Batch: 5 Loss:  0.5533\n",
            "Epoch19 | Batch: 6 Loss:  0.5284\n",
            "Epoch19 | Batch: 7 Loss:  0.5272\n",
            "Epoch19 | Batch: 8 Loss:  0.4849\n",
            "Epoch19 | Batch: 9 Loss:  0.5913\n",
            "Epoch19 | Batch: 10 Loss:  0.6596\n",
            "Epoch19 | Batch: 11 Loss:  0.5448\n",
            "Epoch19 | Batch: 12 Loss:  0.5775\n",
            "Epoch19 | Batch: 13 Loss:  0.4933\n",
            "Epoch19 | Batch: 14 Loss:  0.5341\n",
            "Epoch19 | Batch: 15 Loss:  0.6153\n",
            "Epoch19 | Batch: 16 Loss:  0.5611\n",
            "Epoch19 | Batch: 17 Loss:  0.5978\n",
            "Epoch19 | Batch: 18 Loss:  0.5030\n",
            "Epoch19 | Batch: 19 Loss:  0.6291\n",
            "Epoch19 | Batch: 20 Loss:  0.5088\n",
            "Epoch19 | Batch: 21 Loss:  0.5212\n",
            "Epoch19 | Batch: 22 Loss:  0.5396\n",
            "Epoch19 | Batch: 23 Loss:  0.6242\n",
            "Epoch19 | Batch: 24 Loss:  0.5176\n",
            "Epoch20 | Batch: 1 Loss:  0.5235\n",
            "Epoch20 | Batch: 2 Loss:  0.5557\n",
            "Epoch20 | Batch: 3 Loss:  0.5721\n",
            "Epoch20 | Batch: 4 Loss:  0.4635\n",
            "Epoch20 | Batch: 5 Loss:  0.4513\n",
            "Epoch20 | Batch: 6 Loss:  0.5097\n",
            "Epoch20 | Batch: 7 Loss:  0.5940\n",
            "Epoch20 | Batch: 8 Loss:  0.5330\n",
            "Epoch20 | Batch: 9 Loss:  0.4986\n",
            "Epoch20 | Batch: 10 Loss:  0.6577\n",
            "Epoch20 | Batch: 11 Loss:  0.5409\n",
            "Epoch20 | Batch: 12 Loss:  0.5690\n",
            "Epoch20 | Batch: 13 Loss:  0.4836\n",
            "Epoch20 | Batch: 14 Loss:  0.6278\n",
            "Epoch20 | Batch: 15 Loss:  0.4502\n",
            "Epoch20 | Batch: 16 Loss:  0.6614\n",
            "Epoch20 | Batch: 17 Loss:  0.6080\n",
            "Epoch20 | Batch: 18 Loss:  0.5768\n",
            "Epoch20 | Batch: 19 Loss:  0.5626\n",
            "Epoch20 | Batch: 20 Loss:  0.5813\n",
            "Epoch20 | Batch: 21 Loss:  0.5161\n",
            "Epoch20 | Batch: 22 Loss:  0.5453\n",
            "Epoch20 | Batch: 23 Loss:  0.4322\n",
            "Epoch20 | Batch: 24 Loss:  0.5241\n",
            "Epoch21 | Batch: 1 Loss:  0.5365\n",
            "Epoch21 | Batch: 2 Loss:  0.4989\n",
            "Epoch21 | Batch: 3 Loss:  0.4700\n",
            "Epoch21 | Batch: 4 Loss:  0.5137\n",
            "Epoch21 | Batch: 5 Loss:  0.5107\n",
            "Epoch21 | Batch: 6 Loss:  0.4806\n",
            "Epoch21 | Batch: 7 Loss:  0.5421\n",
            "Epoch21 | Batch: 8 Loss:  0.5472\n",
            "Epoch21 | Batch: 9 Loss:  0.6707\n",
            "Epoch21 | Batch: 10 Loss:  0.5231\n",
            "Epoch21 | Batch: 11 Loss:  0.5537\n",
            "Epoch21 | Batch: 12 Loss:  0.5700\n",
            "Epoch21 | Batch: 13 Loss:  0.4612\n",
            "Epoch21 | Batch: 14 Loss:  0.4736\n",
            "Epoch21 | Batch: 15 Loss:  0.4625\n",
            "Epoch21 | Batch: 16 Loss:  0.6225\n",
            "Epoch21 | Batch: 17 Loss:  0.5600\n",
            "Epoch21 | Batch: 18 Loss:  0.6703\n",
            "Epoch21 | Batch: 19 Loss:  0.4551\n",
            "Epoch21 | Batch: 20 Loss:  0.4997\n",
            "Epoch21 | Batch: 21 Loss:  0.4938\n",
            "Epoch21 | Batch: 22 Loss:  0.5169\n",
            "Epoch21 | Batch: 23 Loss:  0.4412\n",
            "Epoch21 | Batch: 24 Loss:  0.5507\n",
            "Epoch22 | Batch: 1 Loss:  0.5129\n",
            "Epoch22 | Batch: 2 Loss:  0.4702\n",
            "Epoch22 | Batch: 3 Loss:  0.4956\n",
            "Epoch22 | Batch: 4 Loss:  0.5715\n",
            "Epoch22 | Batch: 5 Loss:  0.5083\n",
            "Epoch22 | Batch: 6 Loss:  0.6181\n",
            "Epoch22 | Batch: 7 Loss:  0.6283\n",
            "Epoch22 | Batch: 8 Loss:  0.4510\n",
            "Epoch22 | Batch: 9 Loss:  0.4948\n",
            "Epoch22 | Batch: 10 Loss:  0.4157\n",
            "Epoch22 | Batch: 11 Loss:  0.4937\n",
            "Epoch22 | Batch: 12 Loss:  0.4410\n",
            "Epoch22 | Batch: 13 Loss:  0.5254\n",
            "Epoch22 | Batch: 14 Loss:  0.5867\n",
            "Epoch22 | Batch: 15 Loss:  0.4873\n",
            "Epoch22 | Batch: 16 Loss:  0.5251\n",
            "Epoch22 | Batch: 17 Loss:  0.4942\n",
            "Epoch22 | Batch: 18 Loss:  0.5555\n",
            "Epoch22 | Batch: 19 Loss:  0.4764\n",
            "Epoch22 | Batch: 20 Loss:  0.5408\n",
            "Epoch22 | Batch: 21 Loss:  0.5014\n",
            "Epoch22 | Batch: 22 Loss:  0.3939\n",
            "Epoch22 | Batch: 23 Loss:  0.5220\n",
            "Epoch22 | Batch: 24 Loss:  0.5175\n",
            "Epoch23 | Batch: 1 Loss:  0.4657\n",
            "Epoch23 | Batch: 2 Loss:  0.4710\n",
            "Epoch23 | Batch: 3 Loss:  0.4901\n",
            "Epoch23 | Batch: 4 Loss:  0.3412\n",
            "Epoch23 | Batch: 5 Loss:  0.4797\n",
            "Epoch23 | Batch: 6 Loss:  0.6030\n",
            "Epoch23 | Batch: 7 Loss:  0.5372\n",
            "Epoch23 | Batch: 8 Loss:  0.4737\n",
            "Epoch23 | Batch: 9 Loss:  0.5613\n",
            "Epoch23 | Batch: 10 Loss:  0.4534\n",
            "Epoch23 | Batch: 11 Loss:  0.4661\n",
            "Epoch23 | Batch: 12 Loss:  0.5027\n",
            "Epoch23 | Batch: 13 Loss:  0.4643\n",
            "Epoch23 | Batch: 14 Loss:  0.4415\n",
            "Epoch23 | Batch: 15 Loss:  0.4405\n",
            "Epoch23 | Batch: 16 Loss:  0.6046\n",
            "Epoch23 | Batch: 17 Loss:  0.5007\n",
            "Epoch23 | Batch: 18 Loss:  0.5083\n",
            "Epoch23 | Batch: 19 Loss:  0.5308\n",
            "Epoch23 | Batch: 20 Loss:  0.4984\n",
            "Epoch23 | Batch: 21 Loss:  0.5902\n",
            "Epoch23 | Batch: 22 Loss:  0.4734\n",
            "Epoch23 | Batch: 23 Loss:  0.5627\n",
            "Epoch23 | Batch: 24 Loss:  0.4782\n",
            "Epoch24 | Batch: 1 Loss:  0.6029\n",
            "Epoch24 | Batch: 2 Loss:  0.4820\n",
            "Epoch24 | Batch: 3 Loss:  0.3726\n",
            "Epoch24 | Batch: 4 Loss:  0.3538\n",
            "Epoch24 | Batch: 5 Loss:  0.5363\n",
            "Epoch24 | Batch: 6 Loss:  0.5044\n",
            "Epoch24 | Batch: 7 Loss:  0.4476\n",
            "Epoch24 | Batch: 8 Loss:  0.5454\n",
            "Epoch24 | Batch: 9 Loss:  0.5247\n",
            "Epoch24 | Batch: 10 Loss:  0.5150\n",
            "Epoch24 | Batch: 11 Loss:  0.4776\n",
            "Epoch24 | Batch: 12 Loss:  0.6009\n",
            "Epoch24 | Batch: 13 Loss:  0.4522\n",
            "Epoch24 | Batch: 14 Loss:  0.4336\n",
            "Epoch24 | Batch: 15 Loss:  0.4603\n",
            "Epoch24 | Batch: 16 Loss:  0.5804\n",
            "Epoch24 | Batch: 17 Loss:  0.5001\n",
            "Epoch24 | Batch: 18 Loss:  0.4842\n",
            "Epoch24 | Batch: 19 Loss:  0.4954\n",
            "Epoch24 | Batch: 20 Loss:  0.4688\n",
            "Epoch24 | Batch: 21 Loss:  0.5042\n",
            "Epoch24 | Batch: 22 Loss:  0.3736\n",
            "Epoch24 | Batch: 23 Loss:  0.5389\n",
            "Epoch24 | Batch: 24 Loss:  0.4971\n",
            "Epoch25 | Batch: 1 Loss:  0.6207\n",
            "Epoch25 | Batch: 2 Loss:  0.5240\n",
            "Epoch25 | Batch: 3 Loss:  0.4113\n",
            "Epoch25 | Batch: 4 Loss:  0.4051\n",
            "Epoch25 | Batch: 5 Loss:  0.4029\n",
            "Epoch25 | Batch: 6 Loss:  0.4012\n",
            "Epoch25 | Batch: 7 Loss:  0.4829\n",
            "Epoch25 | Batch: 8 Loss:  0.5389\n",
            "Epoch25 | Batch: 9 Loss:  0.5407\n",
            "Epoch25 | Batch: 10 Loss:  0.5598\n",
            "Epoch25 | Batch: 11 Loss:  0.4742\n",
            "Epoch25 | Batch: 12 Loss:  0.4294\n",
            "Epoch25 | Batch: 13 Loss:  0.3375\n",
            "Epoch25 | Batch: 14 Loss:  0.5577\n",
            "Epoch25 | Batch: 15 Loss:  0.3484\n",
            "Epoch25 | Batch: 16 Loss:  0.6039\n",
            "Epoch25 | Batch: 17 Loss:  0.6722\n",
            "Epoch25 | Batch: 18 Loss:  0.4545\n",
            "Epoch25 | Batch: 19 Loss:  0.3697\n",
            "Epoch25 | Batch: 20 Loss:  0.6040\n",
            "Epoch25 | Batch: 21 Loss:  0.5905\n",
            "Epoch25 | Batch: 22 Loss:  0.4828\n",
            "Epoch25 | Batch: 23 Loss:  0.4471\n",
            "Epoch25 | Batch: 24 Loss:  0.4690\n",
            "Epoch26 | Batch: 1 Loss:  0.4949\n",
            "Epoch26 | Batch: 2 Loss:  0.4226\n",
            "Epoch26 | Batch: 3 Loss:  0.6195\n",
            "Epoch26 | Batch: 4 Loss:  0.4319\n",
            "Epoch26 | Batch: 5 Loss:  0.5680\n",
            "Epoch26 | Batch: 6 Loss:  0.5470\n",
            "Epoch26 | Batch: 7 Loss:  0.4095\n",
            "Epoch26 | Batch: 8 Loss:  0.3452\n",
            "Epoch26 | Batch: 9 Loss:  0.3446\n",
            "Epoch26 | Batch: 10 Loss:  0.6670\n",
            "Epoch26 | Batch: 11 Loss:  0.5121\n",
            "Epoch26 | Batch: 12 Loss:  0.4794\n",
            "Epoch26 | Batch: 13 Loss:  0.5959\n",
            "Epoch26 | Batch: 14 Loss:  0.4883\n",
            "Epoch26 | Batch: 15 Loss:  0.4891\n",
            "Epoch26 | Batch: 16 Loss:  0.3864\n",
            "Epoch26 | Batch: 17 Loss:  0.3760\n",
            "Epoch26 | Batch: 18 Loss:  0.4724\n",
            "Epoch26 | Batch: 19 Loss:  0.4925\n",
            "Epoch26 | Batch: 20 Loss:  0.4406\n",
            "Epoch26 | Batch: 21 Loss:  0.4420\n",
            "Epoch26 | Batch: 22 Loss:  0.4166\n",
            "Epoch26 | Batch: 23 Loss:  0.5922\n",
            "Epoch26 | Batch: 24 Loss:  0.6395\n",
            "Epoch27 | Batch: 1 Loss:  0.4476\n",
            "Epoch27 | Batch: 2 Loss:  0.5637\n",
            "Epoch27 | Batch: 3 Loss:  0.5555\n",
            "Epoch27 | Batch: 4 Loss:  0.3720\n",
            "Epoch27 | Batch: 5 Loss:  0.4683\n",
            "Epoch27 | Batch: 6 Loss:  0.3304\n",
            "Epoch27 | Batch: 7 Loss:  0.5304\n",
            "Epoch27 | Batch: 8 Loss:  0.5630\n",
            "Epoch27 | Batch: 9 Loss:  0.5202\n",
            "Epoch27 | Batch: 10 Loss:  0.5236\n",
            "Epoch27 | Batch: 11 Loss:  0.5712\n",
            "Epoch27 | Batch: 12 Loss:  0.5536\n",
            "Epoch27 | Batch: 13 Loss:  0.4426\n",
            "Epoch27 | Batch: 14 Loss:  0.4348\n",
            "Epoch27 | Batch: 15 Loss:  0.4752\n",
            "Epoch27 | Batch: 16 Loss:  0.4629\n",
            "Epoch27 | Batch: 17 Loss:  0.5842\n",
            "Epoch27 | Batch: 18 Loss:  0.4242\n",
            "Epoch27 | Batch: 19 Loss:  0.4406\n",
            "Epoch27 | Batch: 20 Loss:  0.4239\n",
            "Epoch27 | Batch: 21 Loss:  0.4972\n",
            "Epoch27 | Batch: 22 Loss:  0.4741\n",
            "Epoch27 | Batch: 23 Loss:  0.4300\n",
            "Epoch27 | Batch: 24 Loss:  0.4546\n",
            "Epoch28 | Batch: 1 Loss:  0.4397\n",
            "Epoch28 | Batch: 2 Loss:  0.5254\n",
            "Epoch28 | Batch: 3 Loss:  0.5751\n",
            "Epoch28 | Batch: 4 Loss:  0.5578\n",
            "Epoch28 | Batch: 5 Loss:  0.3599\n",
            "Epoch28 | Batch: 6 Loss:  0.5028\n",
            "Epoch28 | Batch: 7 Loss:  0.6054\n",
            "Epoch28 | Batch: 8 Loss:  0.3775\n",
            "Epoch28 | Batch: 9 Loss:  0.4843\n",
            "Epoch28 | Batch: 10 Loss:  0.4798\n",
            "Epoch28 | Batch: 11 Loss:  0.6084\n",
            "Epoch28 | Batch: 12 Loss:  0.5007\n",
            "Epoch28 | Batch: 13 Loss:  0.3409\n",
            "Epoch28 | Batch: 14 Loss:  0.6641\n",
            "Epoch28 | Batch: 15 Loss:  0.2861\n",
            "Epoch28 | Batch: 16 Loss:  0.4829\n",
            "Epoch28 | Batch: 17 Loss:  0.5417\n",
            "Epoch28 | Batch: 18 Loss:  0.3581\n",
            "Epoch28 | Batch: 19 Loss:  0.5794\n",
            "Epoch28 | Batch: 20 Loss:  0.4719\n",
            "Epoch28 | Batch: 21 Loss:  0.3894\n",
            "Epoch28 | Batch: 22 Loss:  0.5130\n",
            "Epoch28 | Batch: 23 Loss:  0.4861\n",
            "Epoch28 | Batch: 24 Loss:  0.3419\n",
            "Epoch29 | Batch: 1 Loss:  0.3551\n",
            "Epoch29 | Batch: 2 Loss:  0.5044\n",
            "Epoch29 | Batch: 3 Loss:  0.4472\n",
            "Epoch29 | Batch: 4 Loss:  0.5209\n",
            "Epoch29 | Batch: 5 Loss:  0.3625\n",
            "Epoch29 | Batch: 6 Loss:  0.5020\n",
            "Epoch29 | Batch: 7 Loss:  0.2326\n",
            "Epoch29 | Batch: 8 Loss:  0.4258\n",
            "Epoch29 | Batch: 9 Loss:  0.5819\n",
            "Epoch29 | Batch: 10 Loss:  0.4227\n",
            "Epoch29 | Batch: 11 Loss:  0.5387\n",
            "Epoch29 | Batch: 12 Loss:  0.3973\n",
            "Epoch29 | Batch: 13 Loss:  0.4955\n",
            "Epoch29 | Batch: 14 Loss:  0.5694\n",
            "Epoch29 | Batch: 15 Loss:  0.5016\n",
            "Epoch29 | Batch: 16 Loss:  0.3573\n",
            "Epoch29 | Batch: 17 Loss:  0.5143\n",
            "Epoch29 | Batch: 18 Loss:  0.4307\n",
            "Epoch29 | Batch: 19 Loss:  0.4594\n",
            "Epoch29 | Batch: 20 Loss:  0.7377\n",
            "Epoch29 | Batch: 21 Loss:  0.6534\n",
            "Epoch29 | Batch: 22 Loss:  0.4590\n",
            "Epoch29 | Batch: 23 Loss:  0.5114\n",
            "Epoch29 | Batch: 24 Loss:  0.4461\n",
            "Epoch30 | Batch: 1 Loss:  0.4002\n",
            "Epoch30 | Batch: 2 Loss:  0.5659\n",
            "Epoch30 | Batch: 3 Loss:  0.4933\n",
            "Epoch30 | Batch: 4 Loss:  0.4641\n",
            "Epoch30 | Batch: 5 Loss:  0.4815\n",
            "Epoch30 | Batch: 6 Loss:  0.4976\n",
            "Epoch30 | Batch: 7 Loss:  0.4892\n",
            "Epoch30 | Batch: 8 Loss:  0.3894\n",
            "Epoch30 | Batch: 9 Loss:  0.5676\n",
            "Epoch30 | Batch: 10 Loss:  0.4706\n",
            "Epoch30 | Batch: 11 Loss:  0.4519\n",
            "Epoch30 | Batch: 12 Loss:  0.4879\n",
            "Epoch30 | Batch: 13 Loss:  0.4877\n",
            "Epoch30 | Batch: 14 Loss:  0.4644\n",
            "Epoch30 | Batch: 15 Loss:  0.3891\n",
            "Epoch30 | Batch: 16 Loss:  0.6339\n",
            "Epoch30 | Batch: 17 Loss:  0.5328\n",
            "Epoch30 | Batch: 18 Loss:  0.4887\n",
            "Epoch30 | Batch: 19 Loss:  0.4421\n",
            "Epoch30 | Batch: 20 Loss:  0.4018\n",
            "Epoch30 | Batch: 21 Loss:  0.4335\n",
            "Epoch30 | Batch: 22 Loss:  0.3401\n",
            "Epoch30 | Batch: 23 Loss:  0.5957\n",
            "Epoch30 | Batch: 24 Loss:  0.5699\n",
            "Epoch31 | Batch: 1 Loss:  0.3132\n",
            "Epoch31 | Batch: 2 Loss:  0.2665\n",
            "Epoch31 | Batch: 3 Loss:  0.3620\n",
            "Epoch31 | Batch: 4 Loss:  0.5581\n",
            "Epoch31 | Batch: 5 Loss:  0.4060\n",
            "Epoch31 | Batch: 6 Loss:  0.5457\n",
            "Epoch31 | Batch: 7 Loss:  0.4928\n",
            "Epoch31 | Batch: 8 Loss:  0.4275\n",
            "Epoch31 | Batch: 9 Loss:  0.5375\n",
            "Epoch31 | Batch: 10 Loss:  0.5383\n",
            "Epoch31 | Batch: 11 Loss:  0.5525\n",
            "Epoch31 | Batch: 12 Loss:  0.4358\n",
            "Epoch31 | Batch: 13 Loss:  0.6119\n",
            "Epoch31 | Batch: 14 Loss:  0.5425\n",
            "Epoch31 | Batch: 15 Loss:  0.6370\n",
            "Epoch31 | Batch: 16 Loss:  0.3566\n",
            "Epoch31 | Batch: 17 Loss:  0.4466\n",
            "Epoch31 | Batch: 18 Loss:  0.4651\n",
            "Epoch31 | Batch: 19 Loss:  0.4630\n",
            "Epoch31 | Batch: 20 Loss:  0.5907\n",
            "Epoch31 | Batch: 21 Loss:  0.4620\n",
            "Epoch31 | Batch: 22 Loss:  0.4156\n",
            "Epoch31 | Batch: 23 Loss:  0.4764\n",
            "Epoch31 | Batch: 24 Loss:  0.5637\n",
            "Epoch32 | Batch: 1 Loss:  0.4593\n",
            "Epoch32 | Batch: 2 Loss:  0.4860\n",
            "Epoch32 | Batch: 3 Loss:  0.5006\n",
            "Epoch32 | Batch: 4 Loss:  0.4243\n",
            "Epoch32 | Batch: 5 Loss:  0.4890\n",
            "Epoch32 | Batch: 6 Loss:  0.4871\n",
            "Epoch32 | Batch: 7 Loss:  0.5933\n",
            "Epoch32 | Batch: 8 Loss:  0.5215\n",
            "Epoch32 | Batch: 9 Loss:  0.6021\n",
            "Epoch32 | Batch: 10 Loss:  0.6336\n",
            "Epoch32 | Batch: 11 Loss:  0.5934\n",
            "Epoch32 | Batch: 12 Loss:  0.3140\n",
            "Epoch32 | Batch: 13 Loss:  0.5557\n",
            "Epoch32 | Batch: 14 Loss:  0.4073\n",
            "Epoch32 | Batch: 15 Loss:  0.4074\n",
            "Epoch32 | Batch: 16 Loss:  0.2734\n",
            "Epoch32 | Batch: 17 Loss:  0.4682\n",
            "Epoch32 | Batch: 18 Loss:  0.3611\n",
            "Epoch32 | Batch: 19 Loss:  0.6061\n",
            "Epoch32 | Batch: 20 Loss:  0.4069\n",
            "Epoch32 | Batch: 21 Loss:  0.4689\n",
            "Epoch32 | Batch: 22 Loss:  0.5234\n",
            "Epoch32 | Batch: 23 Loss:  0.4615\n",
            "Epoch32 | Batch: 24 Loss:  0.3084\n",
            "Epoch33 | Batch: 1 Loss:  0.3793\n",
            "Epoch33 | Batch: 2 Loss:  0.3738\n",
            "Epoch33 | Batch: 3 Loss:  0.4277\n",
            "Epoch33 | Batch: 4 Loss:  0.3041\n",
            "Epoch33 | Batch: 5 Loss:  0.3856\n",
            "Epoch33 | Batch: 6 Loss:  0.5302\n",
            "Epoch33 | Batch: 7 Loss:  0.5144\n",
            "Epoch33 | Batch: 8 Loss:  0.3099\n",
            "Epoch33 | Batch: 9 Loss:  0.3834\n",
            "Epoch33 | Batch: 10 Loss:  0.4325\n",
            "Epoch33 | Batch: 11 Loss:  0.5652\n",
            "Epoch33 | Batch: 12 Loss:  0.5496\n",
            "Epoch33 | Batch: 13 Loss:  0.5426\n",
            "Epoch33 | Batch: 14 Loss:  0.5712\n",
            "Epoch33 | Batch: 15 Loss:  0.6595\n",
            "Epoch33 | Batch: 16 Loss:  0.4913\n",
            "Epoch33 | Batch: 17 Loss:  0.4908\n",
            "Epoch33 | Batch: 18 Loss:  0.3736\n",
            "Epoch33 | Batch: 19 Loss:  0.3481\n",
            "Epoch33 | Batch: 20 Loss:  0.5845\n",
            "Epoch33 | Batch: 21 Loss:  0.5135\n",
            "Epoch33 | Batch: 22 Loss:  0.5586\n",
            "Epoch33 | Batch: 23 Loss:  0.5569\n",
            "Epoch33 | Batch: 24 Loss:  0.5637\n",
            "Epoch34 | Batch: 1 Loss:  0.5238\n",
            "Epoch34 | Batch: 2 Loss:  0.5421\n",
            "Epoch34 | Batch: 3 Loss:  0.3441\n",
            "Epoch34 | Batch: 4 Loss:  0.5605\n",
            "Epoch34 | Batch: 5 Loss:  0.3096\n",
            "Epoch34 | Batch: 6 Loss:  0.4430\n",
            "Epoch34 | Batch: 7 Loss:  0.4755\n",
            "Epoch34 | Batch: 8 Loss:  0.4500\n",
            "Epoch34 | Batch: 9 Loss:  0.6067\n",
            "Epoch34 | Batch: 10 Loss:  0.6014\n",
            "Epoch34 | Batch: 11 Loss:  0.4065\n",
            "Epoch34 | Batch: 12 Loss:  0.4097\n",
            "Epoch34 | Batch: 13 Loss:  0.3977\n",
            "Epoch34 | Batch: 14 Loss:  0.5625\n",
            "Epoch34 | Batch: 15 Loss:  0.5541\n",
            "Epoch34 | Batch: 16 Loss:  0.4521\n",
            "Epoch34 | Batch: 17 Loss:  0.5028\n",
            "Epoch34 | Batch: 18 Loss:  0.4387\n",
            "Epoch34 | Batch: 19 Loss:  0.3762\n",
            "Epoch34 | Batch: 20 Loss:  0.4422\n",
            "Epoch34 | Batch: 21 Loss:  0.4477\n",
            "Epoch34 | Batch: 22 Loss:  0.4794\n",
            "Epoch34 | Batch: 23 Loss:  0.4701\n",
            "Epoch34 | Batch: 24 Loss:  0.5627\n",
            "Epoch35 | Batch: 1 Loss:  0.6797\n",
            "Epoch35 | Batch: 2 Loss:  0.7636\n",
            "Epoch35 | Batch: 3 Loss:  0.5002\n",
            "Epoch35 | Batch: 4 Loss:  0.5832\n",
            "Epoch35 | Batch: 5 Loss:  0.4360\n",
            "Epoch35 | Batch: 6 Loss:  0.3578\n",
            "Epoch35 | Batch: 7 Loss:  0.5167\n",
            "Epoch35 | Batch: 8 Loss:  0.4300\n",
            "Epoch35 | Batch: 9 Loss:  0.4574\n",
            "Epoch35 | Batch: 10 Loss:  0.4644\n",
            "Epoch35 | Batch: 11 Loss:  0.5275\n",
            "Epoch35 | Batch: 12 Loss:  0.4167\n",
            "Epoch35 | Batch: 13 Loss:  0.5810\n",
            "Epoch35 | Batch: 14 Loss:  0.4566\n",
            "Epoch35 | Batch: 15 Loss:  0.2284\n",
            "Epoch35 | Batch: 16 Loss:  0.3511\n",
            "Epoch35 | Batch: 17 Loss:  0.5028\n",
            "Epoch35 | Batch: 18 Loss:  0.5756\n",
            "Epoch35 | Batch: 19 Loss:  0.4204\n",
            "Epoch35 | Batch: 20 Loss:  0.4409\n",
            "Epoch35 | Batch: 21 Loss:  0.3549\n",
            "Epoch35 | Batch: 22 Loss:  0.4579\n",
            "Epoch35 | Batch: 23 Loss:  0.4053\n",
            "Epoch35 | Batch: 24 Loss:  0.5816\n",
            "Epoch36 | Batch: 1 Loss:  0.3779\n",
            "Epoch36 | Batch: 2 Loss:  0.5230\n",
            "Epoch36 | Batch: 3 Loss:  0.4379\n",
            "Epoch36 | Batch: 4 Loss:  0.4612\n",
            "Epoch36 | Batch: 5 Loss:  0.4704\n",
            "Epoch36 | Batch: 6 Loss:  0.4413\n",
            "Epoch36 | Batch: 7 Loss:  0.5533\n",
            "Epoch36 | Batch: 8 Loss:  0.5241\n",
            "Epoch36 | Batch: 9 Loss:  0.3887\n",
            "Epoch36 | Batch: 10 Loss:  0.5589\n",
            "Epoch36 | Batch: 11 Loss:  0.5399\n",
            "Epoch36 | Batch: 12 Loss:  0.4204\n",
            "Epoch36 | Batch: 13 Loss:  0.5607\n",
            "Epoch36 | Batch: 14 Loss:  0.3926\n",
            "Epoch36 | Batch: 15 Loss:  0.4042\n",
            "Epoch36 | Batch: 16 Loss:  0.3924\n",
            "Epoch36 | Batch: 17 Loss:  0.7275\n",
            "Epoch36 | Batch: 18 Loss:  0.4806\n",
            "Epoch36 | Batch: 19 Loss:  0.4360\n",
            "Epoch36 | Batch: 20 Loss:  0.3231\n",
            "Epoch36 | Batch: 21 Loss:  0.3725\n",
            "Epoch36 | Batch: 22 Loss:  0.6171\n",
            "Epoch36 | Batch: 23 Loss:  0.4417\n",
            "Epoch36 | Batch: 24 Loss:  0.4621\n",
            "Epoch37 | Batch: 1 Loss:  0.4802\n",
            "Epoch37 | Batch: 2 Loss:  0.4275\n",
            "Epoch37 | Batch: 3 Loss:  0.5286\n",
            "Epoch37 | Batch: 4 Loss:  0.4176\n",
            "Epoch37 | Batch: 5 Loss:  0.6810\n",
            "Epoch37 | Batch: 6 Loss:  0.4388\n",
            "Epoch37 | Batch: 7 Loss:  0.4831\n",
            "Epoch37 | Batch: 8 Loss:  0.5177\n",
            "Epoch37 | Batch: 9 Loss:  0.3974\n",
            "Epoch37 | Batch: 10 Loss:  0.5905\n",
            "Epoch37 | Batch: 11 Loss:  0.4515\n",
            "Epoch37 | Batch: 12 Loss:  0.3919\n",
            "Epoch37 | Batch: 13 Loss:  0.4746\n",
            "Epoch37 | Batch: 14 Loss:  0.3645\n",
            "Epoch37 | Batch: 15 Loss:  0.5659\n",
            "Epoch37 | Batch: 16 Loss:  0.5344\n",
            "Epoch37 | Batch: 17 Loss:  0.3446\n",
            "Epoch37 | Batch: 18 Loss:  0.5513\n",
            "Epoch37 | Batch: 19 Loss:  0.3441\n",
            "Epoch37 | Batch: 20 Loss:  0.4984\n",
            "Epoch37 | Batch: 21 Loss:  0.5413\n",
            "Epoch37 | Batch: 22 Loss:  0.4722\n",
            "Epoch37 | Batch: 23 Loss:  0.4712\n",
            "Epoch37 | Batch: 24 Loss:  0.4276\n",
            "Epoch38 | Batch: 1 Loss:  0.2819\n",
            "Epoch38 | Batch: 2 Loss:  0.5640\n",
            "Epoch38 | Batch: 3 Loss:  0.5275\n",
            "Epoch38 | Batch: 4 Loss:  0.4618\n",
            "Epoch38 | Batch: 5 Loss:  0.5820\n",
            "Epoch38 | Batch: 6 Loss:  0.4518\n",
            "Epoch38 | Batch: 7 Loss:  0.5790\n",
            "Epoch38 | Batch: 8 Loss:  0.3914\n",
            "Epoch38 | Batch: 9 Loss:  0.4846\n",
            "Epoch38 | Batch: 10 Loss:  0.4804\n",
            "Epoch38 | Batch: 11 Loss:  0.4853\n",
            "Epoch38 | Batch: 12 Loss:  0.4805\n",
            "Epoch38 | Batch: 13 Loss:  0.4150\n",
            "Epoch38 | Batch: 14 Loss:  0.5104\n",
            "Epoch38 | Batch: 15 Loss:  0.5484\n",
            "Epoch38 | Batch: 16 Loss:  0.3795\n",
            "Epoch38 | Batch: 17 Loss:  0.4443\n",
            "Epoch38 | Batch: 18 Loss:  0.4656\n",
            "Epoch38 | Batch: 19 Loss:  0.4110\n",
            "Epoch38 | Batch: 20 Loss:  0.4791\n",
            "Epoch38 | Batch: 21 Loss:  0.4450\n",
            "Epoch38 | Batch: 22 Loss:  0.3877\n",
            "Epoch38 | Batch: 23 Loss:  0.6488\n",
            "Epoch38 | Batch: 24 Loss:  0.4155\n",
            "Epoch39 | Batch: 1 Loss:  0.6583\n",
            "Epoch39 | Batch: 2 Loss:  0.5592\n",
            "Epoch39 | Batch: 3 Loss:  0.5271\n",
            "Epoch39 | Batch: 4 Loss:  0.5504\n",
            "Epoch39 | Batch: 5 Loss:  0.3074\n",
            "Epoch39 | Batch: 6 Loss:  0.4859\n",
            "Epoch39 | Batch: 7 Loss:  0.3587\n",
            "Epoch39 | Batch: 8 Loss:  0.4508\n",
            "Epoch39 | Batch: 9 Loss:  0.5117\n",
            "Epoch39 | Batch: 10 Loss:  0.4680\n",
            "Epoch39 | Batch: 11 Loss:  0.4951\n",
            "Epoch39 | Batch: 12 Loss:  0.4601\n",
            "Epoch39 | Batch: 13 Loss:  0.4987\n",
            "Epoch39 | Batch: 14 Loss:  0.4633\n",
            "Epoch39 | Batch: 15 Loss:  0.4280\n",
            "Epoch39 | Batch: 16 Loss:  0.5178\n",
            "Epoch39 | Batch: 17 Loss:  0.3677\n",
            "Epoch39 | Batch: 18 Loss:  0.5055\n",
            "Epoch39 | Batch: 19 Loss:  0.6022\n",
            "Epoch39 | Batch: 20 Loss:  0.4601\n",
            "Epoch39 | Batch: 21 Loss:  0.3292\n",
            "Epoch39 | Batch: 22 Loss:  0.4774\n",
            "Epoch39 | Batch: 23 Loss:  0.3421\n",
            "Epoch39 | Batch: 24 Loss:  0.5465\n",
            "Epoch40 | Batch: 1 Loss:  0.5547\n",
            "Epoch40 | Batch: 2 Loss:  0.4588\n",
            "Epoch40 | Batch: 3 Loss:  0.4504\n",
            "Epoch40 | Batch: 4 Loss:  0.4625\n",
            "Epoch40 | Batch: 5 Loss:  0.6123\n",
            "Epoch40 | Batch: 6 Loss:  0.4556\n",
            "Epoch40 | Batch: 7 Loss:  0.5109\n",
            "Epoch40 | Batch: 8 Loss:  0.3693\n",
            "Epoch40 | Batch: 9 Loss:  0.4524\n",
            "Epoch40 | Batch: 10 Loss:  0.5730\n",
            "Epoch40 | Batch: 11 Loss:  0.4566\n",
            "Epoch40 | Batch: 12 Loss:  0.2861\n",
            "Epoch40 | Batch: 13 Loss:  0.5415\n",
            "Epoch40 | Batch: 14 Loss:  0.5191\n",
            "Epoch40 | Batch: 15 Loss:  0.4333\n",
            "Epoch40 | Batch: 16 Loss:  0.3464\n",
            "Epoch40 | Batch: 17 Loss:  0.6333\n",
            "Epoch40 | Batch: 18 Loss:  0.4395\n",
            "Epoch40 | Batch: 19 Loss:  0.3796\n",
            "Epoch40 | Batch: 20 Loss:  0.5190\n",
            "Epoch40 | Batch: 21 Loss:  0.4132\n",
            "Epoch40 | Batch: 22 Loss:  0.4181\n",
            "Epoch40 | Batch: 23 Loss:  0.5151\n",
            "Epoch40 | Batch: 24 Loss:  0.4987\n",
            "Epoch41 | Batch: 1 Loss:  0.3830\n",
            "Epoch41 | Batch: 2 Loss:  0.5572\n",
            "Epoch41 | Batch: 3 Loss:  0.4142\n",
            "Epoch41 | Batch: 4 Loss:  0.3067\n",
            "Epoch41 | Batch: 5 Loss:  0.5340\n",
            "Epoch41 | Batch: 6 Loss:  0.4508\n",
            "Epoch41 | Batch: 7 Loss:  0.4079\n",
            "Epoch41 | Batch: 8 Loss:  0.3923\n",
            "Epoch41 | Batch: 9 Loss:  0.3221\n",
            "Epoch41 | Batch: 10 Loss:  0.3509\n",
            "Epoch41 | Batch: 11 Loss:  0.3223\n",
            "Epoch41 | Batch: 12 Loss:  0.5622\n",
            "Epoch41 | Batch: 13 Loss:  0.4658\n",
            "Epoch41 | Batch: 14 Loss:  0.6208\n",
            "Epoch41 | Batch: 15 Loss:  0.3970\n",
            "Epoch41 | Batch: 16 Loss:  0.4233\n",
            "Epoch41 | Batch: 17 Loss:  0.5905\n",
            "Epoch41 | Batch: 18 Loss:  0.4632\n",
            "Epoch41 | Batch: 19 Loss:  0.7157\n",
            "Epoch41 | Batch: 20 Loss:  0.4918\n",
            "Epoch41 | Batch: 21 Loss:  0.5954\n",
            "Epoch41 | Batch: 22 Loss:  0.5742\n",
            "Epoch41 | Batch: 23 Loss:  0.5119\n",
            "Epoch41 | Batch: 24 Loss:  0.4331\n",
            "Epoch42 | Batch: 1 Loss:  0.6946\n",
            "Epoch42 | Batch: 2 Loss:  0.4822\n",
            "Epoch42 | Batch: 3 Loss:  0.4700\n",
            "Epoch42 | Batch: 4 Loss:  0.5477\n",
            "Epoch42 | Batch: 5 Loss:  0.3711\n",
            "Epoch42 | Batch: 6 Loss:  0.5059\n",
            "Epoch42 | Batch: 7 Loss:  0.3326\n",
            "Epoch42 | Batch: 8 Loss:  0.4224\n",
            "Epoch42 | Batch: 9 Loss:  0.4093\n",
            "Epoch42 | Batch: 10 Loss:  0.5117\n",
            "Epoch42 | Batch: 11 Loss:  0.3957\n",
            "Epoch42 | Batch: 12 Loss:  0.3144\n",
            "Epoch42 | Batch: 13 Loss:  0.4818\n",
            "Epoch42 | Batch: 14 Loss:  0.3654\n",
            "Epoch42 | Batch: 15 Loss:  0.4518\n",
            "Epoch42 | Batch: 16 Loss:  0.5245\n",
            "Epoch42 | Batch: 17 Loss:  0.4280\n",
            "Epoch42 | Batch: 18 Loss:  0.5493\n",
            "Epoch42 | Batch: 19 Loss:  0.5418\n",
            "Epoch42 | Batch: 20 Loss:  0.4137\n",
            "Epoch42 | Batch: 21 Loss:  0.5309\n",
            "Epoch42 | Batch: 22 Loss:  0.5860\n",
            "Epoch42 | Batch: 23 Loss:  0.4345\n",
            "Epoch42 | Batch: 24 Loss:  0.5522\n",
            "Epoch43 | Batch: 1 Loss:  0.5997\n",
            "Epoch43 | Batch: 2 Loss:  0.6316\n",
            "Epoch43 | Batch: 3 Loss:  0.4903\n",
            "Epoch43 | Batch: 4 Loss:  0.3004\n",
            "Epoch43 | Batch: 5 Loss:  0.6027\n",
            "Epoch43 | Batch: 6 Loss:  0.4892\n",
            "Epoch43 | Batch: 7 Loss:  0.4263\n",
            "Epoch43 | Batch: 8 Loss:  0.6237\n",
            "Epoch43 | Batch: 9 Loss:  0.4872\n",
            "Epoch43 | Batch: 10 Loss:  0.3601\n",
            "Epoch43 | Batch: 11 Loss:  0.5991\n",
            "Epoch43 | Batch: 12 Loss:  0.3865\n",
            "Epoch43 | Batch: 13 Loss:  0.4908\n",
            "Epoch43 | Batch: 14 Loss:  0.3949\n",
            "Epoch43 | Batch: 15 Loss:  0.4817\n",
            "Epoch43 | Batch: 16 Loss:  0.5396\n",
            "Epoch43 | Batch: 17 Loss:  0.4786\n",
            "Epoch43 | Batch: 18 Loss:  0.4607\n",
            "Epoch43 | Batch: 19 Loss:  0.3563\n",
            "Epoch43 | Batch: 20 Loss:  0.4595\n",
            "Epoch43 | Batch: 21 Loss:  0.4086\n",
            "Epoch43 | Batch: 22 Loss:  0.4881\n",
            "Epoch43 | Batch: 23 Loss:  0.3943\n",
            "Epoch43 | Batch: 24 Loss:  0.3569\n",
            "Epoch44 | Batch: 1 Loss:  0.5725\n",
            "Epoch44 | Batch: 2 Loss:  0.6377\n",
            "Epoch44 | Batch: 3 Loss:  0.4817\n",
            "Epoch44 | Batch: 4 Loss:  0.2717\n",
            "Epoch44 | Batch: 5 Loss:  0.3412\n",
            "Epoch44 | Batch: 6 Loss:  0.6079\n",
            "Epoch44 | Batch: 7 Loss:  0.5460\n",
            "Epoch44 | Batch: 8 Loss:  0.5217\n",
            "Epoch44 | Batch: 9 Loss:  0.5168\n",
            "Epoch44 | Batch: 10 Loss:  0.3392\n",
            "Epoch44 | Batch: 11 Loss:  0.5154\n",
            "Epoch44 | Batch: 12 Loss:  0.5678\n",
            "Epoch44 | Batch: 13 Loss:  0.5198\n",
            "Epoch44 | Batch: 14 Loss:  0.3433\n",
            "Epoch44 | Batch: 15 Loss:  0.3137\n",
            "Epoch44 | Batch: 16 Loss:  0.5965\n",
            "Epoch44 | Batch: 17 Loss:  0.5492\n",
            "Epoch44 | Batch: 18 Loss:  0.3833\n",
            "Epoch44 | Batch: 19 Loss:  0.4017\n",
            "Epoch44 | Batch: 20 Loss:  0.5348\n",
            "Epoch44 | Batch: 21 Loss:  0.4673\n",
            "Epoch44 | Batch: 22 Loss:  0.4232\n",
            "Epoch44 | Batch: 23 Loss:  0.5004\n",
            "Epoch44 | Batch: 24 Loss:  0.2853\n",
            "Epoch45 | Batch: 1 Loss:  0.4436\n",
            "Epoch45 | Batch: 2 Loss:  0.4298\n",
            "Epoch45 | Batch: 3 Loss:  0.4260\n",
            "Epoch45 | Batch: 4 Loss:  0.4008\n",
            "Epoch45 | Batch: 5 Loss:  0.3780\n",
            "Epoch45 | Batch: 6 Loss:  0.5018\n",
            "Epoch45 | Batch: 7 Loss:  0.4075\n",
            "Epoch45 | Batch: 8 Loss:  0.3930\n",
            "Epoch45 | Batch: 9 Loss:  0.6657\n",
            "Epoch45 | Batch: 10 Loss:  0.5002\n",
            "Epoch45 | Batch: 11 Loss:  0.5852\n",
            "Epoch45 | Batch: 12 Loss:  0.5125\n",
            "Epoch45 | Batch: 13 Loss:  0.4121\n",
            "Epoch45 | Batch: 14 Loss:  0.4930\n",
            "Epoch45 | Batch: 15 Loss:  0.3825\n",
            "Epoch45 | Batch: 16 Loss:  0.4675\n",
            "Epoch45 | Batch: 17 Loss:  0.6005\n",
            "Epoch45 | Batch: 18 Loss:  0.4020\n",
            "Epoch45 | Batch: 19 Loss:  0.4763\n",
            "Epoch45 | Batch: 20 Loss:  0.4066\n",
            "Epoch45 | Batch: 21 Loss:  0.4738\n",
            "Epoch45 | Batch: 22 Loss:  0.6160\n",
            "Epoch45 | Batch: 23 Loss:  0.4765\n",
            "Epoch45 | Batch: 24 Loss:  0.4668\n",
            "Epoch46 | Batch: 1 Loss:  0.3871\n",
            "Epoch46 | Batch: 2 Loss:  0.4951\n",
            "Epoch46 | Batch: 3 Loss:  0.3905\n",
            "Epoch46 | Batch: 4 Loss:  0.4209\n",
            "Epoch46 | Batch: 5 Loss:  0.3638\n",
            "Epoch46 | Batch: 6 Loss:  0.6102\n",
            "Epoch46 | Batch: 7 Loss:  0.5496\n",
            "Epoch46 | Batch: 8 Loss:  0.4427\n",
            "Epoch46 | Batch: 9 Loss:  0.6354\n",
            "Epoch46 | Batch: 10 Loss:  0.4936\n",
            "Epoch46 | Batch: 11 Loss:  0.4011\n",
            "Epoch46 | Batch: 12 Loss:  0.5223\n",
            "Epoch46 | Batch: 13 Loss:  0.3493\n",
            "Epoch46 | Batch: 14 Loss:  0.5154\n",
            "Epoch46 | Batch: 15 Loss:  0.5081\n",
            "Epoch46 | Batch: 16 Loss:  0.4591\n",
            "Epoch46 | Batch: 17 Loss:  0.4674\n",
            "Epoch46 | Batch: 18 Loss:  0.5203\n",
            "Epoch46 | Batch: 19 Loss:  0.4024\n",
            "Epoch46 | Batch: 20 Loss:  0.4989\n",
            "Epoch46 | Batch: 21 Loss:  0.4445\n",
            "Epoch46 | Batch: 22 Loss:  0.3906\n",
            "Epoch46 | Batch: 23 Loss:  0.5725\n",
            "Epoch46 | Batch: 24 Loss:  0.4290\n",
            "Epoch47 | Batch: 1 Loss:  0.4617\n",
            "Epoch47 | Batch: 2 Loss:  0.6120\n",
            "Epoch47 | Batch: 3 Loss:  0.5286\n",
            "Epoch47 | Batch: 4 Loss:  0.4438\n",
            "Epoch47 | Batch: 5 Loss:  0.3743\n",
            "Epoch47 | Batch: 6 Loss:  0.4075\n",
            "Epoch47 | Batch: 7 Loss:  0.5443\n",
            "Epoch47 | Batch: 8 Loss:  0.4068\n",
            "Epoch47 | Batch: 9 Loss:  0.3914\n",
            "Epoch47 | Batch: 10 Loss:  0.4181\n",
            "Epoch47 | Batch: 11 Loss:  0.3902\n",
            "Epoch47 | Batch: 12 Loss:  0.3866\n",
            "Epoch47 | Batch: 13 Loss:  0.3783\n",
            "Epoch47 | Batch: 14 Loss:  0.5688\n",
            "Epoch47 | Batch: 15 Loss:  0.5534\n",
            "Epoch47 | Batch: 16 Loss:  0.5603\n",
            "Epoch47 | Batch: 17 Loss:  0.3925\n",
            "Epoch47 | Batch: 18 Loss:  0.5796\n",
            "Epoch47 | Batch: 19 Loss:  0.3355\n",
            "Epoch47 | Batch: 20 Loss:  0.4659\n",
            "Epoch47 | Batch: 21 Loss:  0.5651\n",
            "Epoch47 | Batch: 22 Loss:  0.4351\n",
            "Epoch47 | Batch: 23 Loss:  0.5288\n",
            "Epoch47 | Batch: 24 Loss:  0.5668\n",
            "Epoch48 | Batch: 1 Loss:  0.5310\n",
            "Epoch48 | Batch: 2 Loss:  0.4971\n",
            "Epoch48 | Batch: 3 Loss:  0.4974\n",
            "Epoch48 | Batch: 4 Loss:  0.5669\n",
            "Epoch48 | Batch: 5 Loss:  0.2909\n",
            "Epoch48 | Batch: 6 Loss:  0.6136\n",
            "Epoch48 | Batch: 7 Loss:  0.4049\n",
            "Epoch48 | Batch: 8 Loss:  0.2453\n",
            "Epoch48 | Batch: 9 Loss:  0.5558\n",
            "Epoch48 | Batch: 10 Loss:  0.5109\n",
            "Epoch48 | Batch: 11 Loss:  0.4780\n",
            "Epoch48 | Batch: 12 Loss:  0.5327\n",
            "Epoch48 | Batch: 13 Loss:  0.4619\n",
            "Epoch48 | Batch: 14 Loss:  0.4259\n",
            "Epoch48 | Batch: 15 Loss:  0.4138\n",
            "Epoch48 | Batch: 16 Loss:  0.2885\n",
            "Epoch48 | Batch: 17 Loss:  0.4009\n",
            "Epoch48 | Batch: 18 Loss:  0.7079\n",
            "Epoch48 | Batch: 19 Loss:  0.4934\n",
            "Epoch48 | Batch: 20 Loss:  0.4920\n",
            "Epoch48 | Batch: 21 Loss:  0.6004\n",
            "Epoch48 | Batch: 22 Loss:  0.4730\n",
            "Epoch48 | Batch: 23 Loss:  0.4489\n",
            "Epoch48 | Batch: 24 Loss:  0.3503\n",
            "Epoch49 | Batch: 1 Loss:  0.4168\n",
            "Epoch49 | Batch: 2 Loss:  0.4037\n",
            "Epoch49 | Batch: 3 Loss:  0.3576\n",
            "Epoch49 | Batch: 4 Loss:  0.4225\n",
            "Epoch49 | Batch: 5 Loss:  0.3751\n",
            "Epoch49 | Batch: 6 Loss:  0.6217\n",
            "Epoch49 | Batch: 7 Loss:  0.5665\n",
            "Epoch49 | Batch: 8 Loss:  0.5599\n",
            "Epoch49 | Batch: 9 Loss:  0.4121\n",
            "Epoch49 | Batch: 10 Loss:  0.3981\n",
            "Epoch49 | Batch: 11 Loss:  0.4731\n",
            "Epoch49 | Batch: 12 Loss:  0.3788\n",
            "Epoch49 | Batch: 13 Loss:  0.5245\n",
            "Epoch49 | Batch: 14 Loss:  0.4873\n",
            "Epoch49 | Batch: 15 Loss:  0.3667\n",
            "Epoch49 | Batch: 16 Loss:  0.4504\n",
            "Epoch49 | Batch: 17 Loss:  0.4350\n",
            "Epoch49 | Batch: 18 Loss:  0.5204\n",
            "Epoch49 | Batch: 19 Loss:  0.5234\n",
            "Epoch49 | Batch: 20 Loss:  0.4888\n",
            "Epoch49 | Batch: 21 Loss:  0.5883\n",
            "Epoch49 | Batch: 22 Loss:  0.4167\n",
            "Epoch49 | Batch: 23 Loss:  0.5930\n",
            "Epoch49 | Batch: 24 Loss:  0.4550\n",
            "Epoch50 | Batch: 1 Loss:  0.4759\n",
            "Epoch50 | Batch: 2 Loss:  0.4136\n",
            "Epoch50 | Batch: 3 Loss:  0.4901\n",
            "Epoch50 | Batch: 4 Loss:  0.4340\n",
            "Epoch50 | Batch: 5 Loss:  0.4078\n",
            "Epoch50 | Batch: 6 Loss:  0.4522\n",
            "Epoch50 | Batch: 7 Loss:  0.4703\n",
            "Epoch50 | Batch: 8 Loss:  0.3034\n",
            "Epoch50 | Batch: 9 Loss:  0.4121\n",
            "Epoch50 | Batch: 10 Loss:  0.6247\n",
            "Epoch50 | Batch: 11 Loss:  0.3171\n",
            "Epoch50 | Batch: 12 Loss:  0.2830\n",
            "Epoch50 | Batch: 13 Loss:  0.5274\n",
            "Epoch50 | Batch: 14 Loss:  0.5237\n",
            "Epoch50 | Batch: 15 Loss:  0.5013\n",
            "Epoch50 | Batch: 16 Loss:  0.4483\n",
            "Epoch50 | Batch: 17 Loss:  0.5301\n",
            "Epoch50 | Batch: 18 Loss:  0.5896\n",
            "Epoch50 | Batch: 19 Loss:  0.5651\n",
            "Epoch50 | Batch: 20 Loss:  0.5085\n",
            "Epoch50 | Batch: 21 Loss:  0.5483\n",
            "Epoch50 | Batch: 22 Loss:  0.4616\n",
            "Epoch50 | Batch: 23 Loss:  0.5074\n",
            "Epoch50 | Batch: 24 Loss:  0.3765\n",
            "Epoch51 | Batch: 1 Loss:  0.5169\n",
            "Epoch51 | Batch: 2 Loss:  0.3091\n",
            "Epoch51 | Batch: 3 Loss:  0.4291\n",
            "Epoch51 | Batch: 4 Loss:  0.4016\n",
            "Epoch51 | Batch: 5 Loss:  0.4578\n",
            "Epoch51 | Batch: 6 Loss:  0.4848\n",
            "Epoch51 | Batch: 7 Loss:  0.4850\n",
            "Epoch51 | Batch: 8 Loss:  0.3967\n",
            "Epoch51 | Batch: 9 Loss:  0.4721\n",
            "Epoch51 | Batch: 10 Loss:  0.5626\n",
            "Epoch51 | Batch: 11 Loss:  0.4780\n",
            "Epoch51 | Batch: 12 Loss:  0.3853\n",
            "Epoch51 | Batch: 13 Loss:  0.4675\n",
            "Epoch51 | Batch: 14 Loss:  0.6689\n",
            "Epoch51 | Batch: 15 Loss:  0.3972\n",
            "Epoch51 | Batch: 16 Loss:  0.5036\n",
            "Epoch51 | Batch: 17 Loss:  0.4544\n",
            "Epoch51 | Batch: 18 Loss:  0.4549\n",
            "Epoch51 | Batch: 19 Loss:  0.4836\n",
            "Epoch51 | Batch: 20 Loss:  0.4238\n",
            "Epoch51 | Batch: 21 Loss:  0.4126\n",
            "Epoch51 | Batch: 22 Loss:  0.6421\n",
            "Epoch51 | Batch: 23 Loss:  0.4603\n",
            "Epoch51 | Batch: 24 Loss:  0.4861\n",
            "Epoch52 | Batch: 1 Loss:  0.4323\n",
            "Epoch52 | Batch: 2 Loss:  0.5251\n",
            "Epoch52 | Batch: 3 Loss:  0.5485\n",
            "Epoch52 | Batch: 4 Loss:  0.5417\n",
            "Epoch52 | Batch: 5 Loss:  0.4273\n",
            "Epoch52 | Batch: 6 Loss:  0.3378\n",
            "Epoch52 | Batch: 7 Loss:  0.4798\n",
            "Epoch52 | Batch: 8 Loss:  0.4864\n",
            "Epoch52 | Batch: 9 Loss:  0.3096\n",
            "Epoch52 | Batch: 10 Loss:  0.5105\n",
            "Epoch52 | Batch: 11 Loss:  0.3557\n",
            "Epoch52 | Batch: 12 Loss:  0.3760\n",
            "Epoch52 | Batch: 13 Loss:  0.4857\n",
            "Epoch52 | Batch: 14 Loss:  0.5915\n",
            "Epoch52 | Batch: 15 Loss:  0.4974\n",
            "Epoch52 | Batch: 16 Loss:  0.4631\n",
            "Epoch52 | Batch: 17 Loss:  0.6466\n",
            "Epoch52 | Batch: 18 Loss:  0.3342\n",
            "Epoch52 | Batch: 19 Loss:  0.5776\n",
            "Epoch52 | Batch: 20 Loss:  0.4898\n",
            "Epoch52 | Batch: 21 Loss:  0.4558\n",
            "Epoch52 | Batch: 22 Loss:  0.4159\n",
            "Epoch52 | Batch: 23 Loss:  0.5277\n",
            "Epoch52 | Batch: 24 Loss:  0.4107\n",
            "Epoch53 | Batch: 1 Loss:  0.3306\n",
            "Epoch53 | Batch: 2 Loss:  0.3445\n",
            "Epoch53 | Batch: 3 Loss:  0.4897\n",
            "Epoch53 | Batch: 4 Loss:  0.5484\n",
            "Epoch53 | Batch: 5 Loss:  0.4997\n",
            "Epoch53 | Batch: 6 Loss:  0.4455\n",
            "Epoch53 | Batch: 7 Loss:  0.4515\n",
            "Epoch53 | Batch: 8 Loss:  0.5688\n",
            "Epoch53 | Batch: 9 Loss:  0.5556\n",
            "Epoch53 | Batch: 10 Loss:  0.3643\n",
            "Epoch53 | Batch: 11 Loss:  0.3676\n",
            "Epoch53 | Batch: 12 Loss:  0.2919\n",
            "Epoch53 | Batch: 13 Loss:  0.3885\n",
            "Epoch53 | Batch: 14 Loss:  0.5125\n",
            "Epoch53 | Batch: 15 Loss:  0.5597\n",
            "Epoch53 | Batch: 16 Loss:  0.6471\n",
            "Epoch53 | Batch: 17 Loss:  0.4802\n",
            "Epoch53 | Batch: 18 Loss:  0.5019\n",
            "Epoch53 | Batch: 19 Loss:  0.4481\n",
            "Epoch53 | Batch: 20 Loss:  0.3508\n",
            "Epoch53 | Batch: 21 Loss:  0.5084\n",
            "Epoch53 | Batch: 22 Loss:  0.5777\n",
            "Epoch53 | Batch: 23 Loss:  0.3866\n",
            "Epoch53 | Batch: 24 Loss:  0.7112\n",
            "Epoch54 | Batch: 1 Loss:  0.4071\n",
            "Epoch54 | Batch: 2 Loss:  0.5902\n",
            "Epoch54 | Batch: 3 Loss:  0.4734\n",
            "Epoch54 | Batch: 4 Loss:  0.4990\n",
            "Epoch54 | Batch: 5 Loss:  0.5679\n",
            "Epoch54 | Batch: 6 Loss:  0.5435\n",
            "Epoch54 | Batch: 7 Loss:  0.4542\n",
            "Epoch54 | Batch: 8 Loss:  0.3770\n",
            "Epoch54 | Batch: 9 Loss:  0.3454\n",
            "Epoch54 | Batch: 10 Loss:  0.4945\n",
            "Epoch54 | Batch: 11 Loss:  0.5284\n",
            "Epoch54 | Batch: 12 Loss:  0.4570\n",
            "Epoch54 | Batch: 13 Loss:  0.4817\n",
            "Epoch54 | Batch: 14 Loss:  0.6575\n",
            "Epoch54 | Batch: 15 Loss:  0.4215\n",
            "Epoch54 | Batch: 16 Loss:  0.5671\n",
            "Epoch54 | Batch: 17 Loss:  0.3638\n",
            "Epoch54 | Batch: 18 Loss:  0.3630\n",
            "Epoch54 | Batch: 19 Loss:  0.5309\n",
            "Epoch54 | Batch: 20 Loss:  0.3703\n",
            "Epoch54 | Batch: 21 Loss:  0.4005\n",
            "Epoch54 | Batch: 22 Loss:  0.4915\n",
            "Epoch54 | Batch: 23 Loss:  0.4234\n",
            "Epoch54 | Batch: 24 Loss:  0.3627\n",
            "Epoch55 | Batch: 1 Loss:  0.5864\n",
            "Epoch55 | Batch: 2 Loss:  0.4503\n",
            "Epoch55 | Batch: 3 Loss:  0.5935\n",
            "Epoch55 | Batch: 4 Loss:  0.3279\n",
            "Epoch55 | Batch: 5 Loss:  0.5603\n",
            "Epoch55 | Batch: 6 Loss:  0.4821\n",
            "Epoch55 | Batch: 7 Loss:  0.4756\n",
            "Epoch55 | Batch: 8 Loss:  0.4992\n",
            "Epoch55 | Batch: 9 Loss:  0.5187\n",
            "Epoch55 | Batch: 10 Loss:  0.4971\n",
            "Epoch55 | Batch: 11 Loss:  0.4209\n",
            "Epoch55 | Batch: 12 Loss:  0.4588\n",
            "Epoch55 | Batch: 13 Loss:  0.5206\n",
            "Epoch55 | Batch: 14 Loss:  0.3827\n",
            "Epoch55 | Batch: 15 Loss:  0.3931\n",
            "Epoch55 | Batch: 16 Loss:  0.4239\n",
            "Epoch55 | Batch: 17 Loss:  0.4377\n",
            "Epoch55 | Batch: 18 Loss:  0.3484\n",
            "Epoch55 | Batch: 19 Loss:  0.4594\n",
            "Epoch55 | Batch: 20 Loss:  0.4795\n",
            "Epoch55 | Batch: 21 Loss:  0.5228\n",
            "Epoch55 | Batch: 22 Loss:  0.4942\n",
            "Epoch55 | Batch: 23 Loss:  0.4883\n",
            "Epoch55 | Batch: 24 Loss:  0.3728\n",
            "Epoch56 | Batch: 1 Loss:  0.4680\n",
            "Epoch56 | Batch: 2 Loss:  0.3459\n",
            "Epoch56 | Batch: 3 Loss:  0.3941\n",
            "Epoch56 | Batch: 4 Loss:  0.4964\n",
            "Epoch56 | Batch: 5 Loss:  0.4578\n",
            "Epoch56 | Batch: 6 Loss:  0.5989\n",
            "Epoch56 | Batch: 7 Loss:  0.4538\n",
            "Epoch56 | Batch: 8 Loss:  0.6672\n",
            "Epoch56 | Batch: 9 Loss:  0.4855\n",
            "Epoch56 | Batch: 10 Loss:  0.3010\n",
            "Epoch56 | Batch: 11 Loss:  0.5404\n",
            "Epoch56 | Batch: 12 Loss:  0.4650\n",
            "Epoch56 | Batch: 13 Loss:  0.4396\n",
            "Epoch56 | Batch: 14 Loss:  0.4499\n",
            "Epoch56 | Batch: 15 Loss:  0.4601\n",
            "Epoch56 | Batch: 16 Loss:  0.3920\n",
            "Epoch56 | Batch: 17 Loss:  0.6221\n",
            "Epoch56 | Batch: 18 Loss:  0.4041\n",
            "Epoch56 | Batch: 19 Loss:  0.4576\n",
            "Epoch56 | Batch: 20 Loss:  0.5387\n",
            "Epoch56 | Batch: 21 Loss:  0.3712\n",
            "Epoch56 | Batch: 22 Loss:  0.3195\n",
            "Epoch56 | Batch: 23 Loss:  0.5057\n",
            "Epoch56 | Batch: 24 Loss:  0.5661\n",
            "Epoch57 | Batch: 1 Loss:  0.3244\n",
            "Epoch57 | Batch: 2 Loss:  0.5479\n",
            "Epoch57 | Batch: 3 Loss:  0.3431\n",
            "Epoch57 | Batch: 4 Loss:  0.5444\n",
            "Epoch57 | Batch: 5 Loss:  0.4319\n",
            "Epoch57 | Batch: 6 Loss:  0.4894\n",
            "Epoch57 | Batch: 7 Loss:  0.4440\n",
            "Epoch57 | Batch: 8 Loss:  0.5279\n",
            "Epoch57 | Batch: 9 Loss:  0.4182\n",
            "Epoch57 | Batch: 10 Loss:  0.4791\n",
            "Epoch57 | Batch: 11 Loss:  0.4787\n",
            "Epoch57 | Batch: 12 Loss:  0.6696\n",
            "Epoch57 | Batch: 13 Loss:  0.5236\n",
            "Epoch57 | Batch: 14 Loss:  0.2768\n",
            "Epoch57 | Batch: 15 Loss:  0.3184\n",
            "Epoch57 | Batch: 16 Loss:  0.5211\n",
            "Epoch57 | Batch: 17 Loss:  0.6213\n",
            "Epoch57 | Batch: 18 Loss:  0.4299\n",
            "Epoch57 | Batch: 19 Loss:  0.3314\n",
            "Epoch57 | Batch: 20 Loss:  0.4588\n",
            "Epoch57 | Batch: 21 Loss:  0.4373\n",
            "Epoch57 | Batch: 22 Loss:  0.5496\n",
            "Epoch57 | Batch: 23 Loss:  0.5483\n",
            "Epoch57 | Batch: 24 Loss:  0.5229\n",
            "Epoch58 | Batch: 1 Loss:  0.5193\n",
            "Epoch58 | Batch: 2 Loss:  0.5386\n",
            "Epoch58 | Batch: 3 Loss:  0.4242\n",
            "Epoch58 | Batch: 4 Loss:  0.6870\n",
            "Epoch58 | Batch: 5 Loss:  0.5983\n",
            "Epoch58 | Batch: 6 Loss:  0.4359\n",
            "Epoch58 | Batch: 7 Loss:  0.4487\n",
            "Epoch58 | Batch: 8 Loss:  0.3559\n",
            "Epoch58 | Batch: 9 Loss:  0.3275\n",
            "Epoch58 | Batch: 10 Loss:  0.3721\n",
            "Epoch58 | Batch: 11 Loss:  0.5058\n",
            "Epoch58 | Batch: 12 Loss:  0.5221\n",
            "Epoch58 | Batch: 13 Loss:  0.4743\n",
            "Epoch58 | Batch: 14 Loss:  0.3694\n",
            "Epoch58 | Batch: 15 Loss:  0.5946\n",
            "Epoch58 | Batch: 16 Loss:  0.5273\n",
            "Epoch58 | Batch: 17 Loss:  0.5869\n",
            "Epoch58 | Batch: 18 Loss:  0.2040\n",
            "Epoch58 | Batch: 19 Loss:  0.4870\n",
            "Epoch58 | Batch: 20 Loss:  0.4757\n",
            "Epoch58 | Batch: 21 Loss:  0.4319\n",
            "Epoch58 | Batch: 22 Loss:  0.3819\n",
            "Epoch58 | Batch: 23 Loss:  0.4819\n",
            "Epoch58 | Batch: 24 Loss:  0.4043\n",
            "Epoch59 | Batch: 1 Loss:  0.5893\n",
            "Epoch59 | Batch: 2 Loss:  0.3427\n",
            "Epoch59 | Batch: 3 Loss:  0.3631\n",
            "Epoch59 | Batch: 4 Loss:  0.4879\n",
            "Epoch59 | Batch: 5 Loss:  0.6464\n",
            "Epoch59 | Batch: 6 Loss:  0.2989\n",
            "Epoch59 | Batch: 7 Loss:  0.4920\n",
            "Epoch59 | Batch: 8 Loss:  0.4129\n",
            "Epoch59 | Batch: 9 Loss:  0.2948\n",
            "Epoch59 | Batch: 10 Loss:  0.3719\n",
            "Epoch59 | Batch: 11 Loss:  0.6167\n",
            "Epoch59 | Batch: 12 Loss:  0.4040\n",
            "Epoch59 | Batch: 13 Loss:  0.5265\n",
            "Epoch59 | Batch: 14 Loss:  0.5534\n",
            "Epoch59 | Batch: 15 Loss:  0.5666\n",
            "Epoch59 | Batch: 16 Loss:  0.4313\n",
            "Epoch59 | Batch: 17 Loss:  0.4051\n",
            "Epoch59 | Batch: 18 Loss:  0.4598\n",
            "Epoch59 | Batch: 19 Loss:  0.6149\n",
            "Epoch59 | Batch: 20 Loss:  0.5201\n",
            "Epoch59 | Batch: 21 Loss:  0.2925\n",
            "Epoch59 | Batch: 22 Loss:  0.4598\n",
            "Epoch59 | Batch: 23 Loss:  0.8120\n",
            "Epoch59 | Batch: 24 Loss:  0.3793\n",
            "Epoch60 | Batch: 1 Loss:  0.3950\n",
            "Epoch60 | Batch: 2 Loss:  0.3773\n",
            "Epoch60 | Batch: 3 Loss:  0.5898\n",
            "Epoch60 | Batch: 4 Loss:  0.5476\n",
            "Epoch60 | Batch: 5 Loss:  0.3983\n",
            "Epoch60 | Batch: 6 Loss:  0.2657\n",
            "Epoch60 | Batch: 7 Loss:  0.5219\n",
            "Epoch60 | Batch: 8 Loss:  0.4565\n",
            "Epoch60 | Batch: 9 Loss:  0.4604\n",
            "Epoch60 | Batch: 10 Loss:  0.5998\n",
            "Epoch60 | Batch: 11 Loss:  0.5495\n",
            "Epoch60 | Batch: 12 Loss:  0.4566\n",
            "Epoch60 | Batch: 13 Loss:  0.4849\n",
            "Epoch60 | Batch: 14 Loss:  0.3371\n",
            "Epoch60 | Batch: 15 Loss:  0.5264\n",
            "Epoch60 | Batch: 16 Loss:  0.5420\n",
            "Epoch60 | Batch: 17 Loss:  0.5817\n",
            "Epoch60 | Batch: 18 Loss:  0.4248\n",
            "Epoch60 | Batch: 19 Loss:  0.4195\n",
            "Epoch60 | Batch: 20 Loss:  0.4748\n",
            "Epoch60 | Batch: 21 Loss:  0.5134\n",
            "Epoch60 | Batch: 22 Loss:  0.4167\n",
            "Epoch60 | Batch: 23 Loss:  0.4685\n",
            "Epoch60 | Batch: 24 Loss:  0.4234\n",
            "Epoch61 | Batch: 1 Loss:  0.5079\n",
            "Epoch61 | Batch: 2 Loss:  0.5471\n",
            "Epoch61 | Batch: 3 Loss:  0.4910\n",
            "Epoch61 | Batch: 4 Loss:  0.5182\n",
            "Epoch61 | Batch: 5 Loss:  0.4209\n",
            "Epoch61 | Batch: 6 Loss:  0.5481\n",
            "Epoch61 | Batch: 7 Loss:  0.6535\n",
            "Epoch61 | Batch: 8 Loss:  0.4660\n",
            "Epoch61 | Batch: 9 Loss:  0.3983\n",
            "Epoch61 | Batch: 10 Loss:  0.5175\n",
            "Epoch61 | Batch: 11 Loss:  0.5289\n",
            "Epoch61 | Batch: 12 Loss:  0.3913\n",
            "Epoch61 | Batch: 13 Loss:  0.2572\n",
            "Epoch61 | Batch: 14 Loss:  0.4829\n",
            "Epoch61 | Batch: 15 Loss:  0.4133\n",
            "Epoch61 | Batch: 16 Loss:  0.4882\n",
            "Epoch61 | Batch: 17 Loss:  0.5269\n",
            "Epoch61 | Batch: 18 Loss:  0.4099\n",
            "Epoch61 | Batch: 19 Loss:  0.4611\n",
            "Epoch61 | Batch: 20 Loss:  0.5280\n",
            "Epoch61 | Batch: 21 Loss:  0.3380\n",
            "Epoch61 | Batch: 22 Loss:  0.5068\n",
            "Epoch61 | Batch: 23 Loss:  0.4981\n",
            "Epoch61 | Batch: 24 Loss:  0.3672\n",
            "Epoch62 | Batch: 1 Loss:  0.3150\n",
            "Epoch62 | Batch: 2 Loss:  0.6009\n",
            "Epoch62 | Batch: 3 Loss:  0.3789\n",
            "Epoch62 | Batch: 4 Loss:  0.3511\n",
            "Epoch62 | Batch: 5 Loss:  0.4815\n",
            "Epoch62 | Batch: 6 Loss:  0.4914\n",
            "Epoch62 | Batch: 7 Loss:  0.6026\n",
            "Epoch62 | Batch: 8 Loss:  0.4335\n",
            "Epoch62 | Batch: 9 Loss:  0.3719\n",
            "Epoch62 | Batch: 10 Loss:  0.4125\n",
            "Epoch62 | Batch: 11 Loss:  0.4259\n",
            "Epoch62 | Batch: 12 Loss:  0.3827\n",
            "Epoch62 | Batch: 13 Loss:  0.4702\n",
            "Epoch62 | Batch: 14 Loss:  0.5603\n",
            "Epoch62 | Batch: 15 Loss:  0.3821\n",
            "Epoch62 | Batch: 16 Loss:  0.5104\n",
            "Epoch62 | Batch: 17 Loss:  0.5047\n",
            "Epoch62 | Batch: 18 Loss:  0.4461\n",
            "Epoch62 | Batch: 19 Loss:  0.5168\n",
            "Epoch62 | Batch: 20 Loss:  0.5897\n",
            "Epoch62 | Batch: 21 Loss:  0.4408\n",
            "Epoch62 | Batch: 22 Loss:  0.4327\n",
            "Epoch62 | Batch: 23 Loss:  0.5465\n",
            "Epoch62 | Batch: 24 Loss:  0.5585\n",
            "Epoch63 | Batch: 1 Loss:  0.4041\n",
            "Epoch63 | Batch: 2 Loss:  0.3455\n",
            "Epoch63 | Batch: 3 Loss:  0.5082\n",
            "Epoch63 | Batch: 4 Loss:  0.4264\n",
            "Epoch63 | Batch: 5 Loss:  0.4736\n",
            "Epoch63 | Batch: 6 Loss:  0.4707\n",
            "Epoch63 | Batch: 7 Loss:  0.4978\n",
            "Epoch63 | Batch: 8 Loss:  0.5088\n",
            "Epoch63 | Batch: 9 Loss:  0.5367\n",
            "Epoch63 | Batch: 10 Loss:  0.4502\n",
            "Epoch63 | Batch: 11 Loss:  0.4998\n",
            "Epoch63 | Batch: 12 Loss:  0.2707\n",
            "Epoch63 | Batch: 13 Loss:  0.5531\n",
            "Epoch63 | Batch: 14 Loss:  0.4141\n",
            "Epoch63 | Batch: 15 Loss:  0.5122\n",
            "Epoch63 | Batch: 16 Loss:  0.4152\n",
            "Epoch63 | Batch: 17 Loss:  0.4664\n",
            "Epoch63 | Batch: 18 Loss:  0.3574\n",
            "Epoch63 | Batch: 19 Loss:  0.2496\n",
            "Epoch63 | Batch: 20 Loss:  0.5294\n",
            "Epoch63 | Batch: 21 Loss:  0.6178\n",
            "Epoch63 | Batch: 22 Loss:  0.6617\n",
            "Epoch63 | Batch: 23 Loss:  0.5552\n",
            "Epoch63 | Batch: 24 Loss:  0.5698\n",
            "Epoch64 | Batch: 1 Loss:  0.4984\n",
            "Epoch64 | Batch: 2 Loss:  0.5445\n",
            "Epoch64 | Batch: 3 Loss:  0.4693\n",
            "Epoch64 | Batch: 4 Loss:  0.3826\n",
            "Epoch64 | Batch: 5 Loss:  0.6853\n",
            "Epoch64 | Batch: 6 Loss:  0.4470\n",
            "Epoch64 | Batch: 7 Loss:  0.3903\n",
            "Epoch64 | Batch: 8 Loss:  0.3680\n",
            "Epoch64 | Batch: 9 Loss:  0.4042\n",
            "Epoch64 | Batch: 10 Loss:  0.6419\n",
            "Epoch64 | Batch: 11 Loss:  0.4225\n",
            "Epoch64 | Batch: 12 Loss:  0.3889\n",
            "Epoch64 | Batch: 13 Loss:  0.5379\n",
            "Epoch64 | Batch: 14 Loss:  0.3046\n",
            "Epoch64 | Batch: 15 Loss:  0.4782\n",
            "Epoch64 | Batch: 16 Loss:  0.5633\n",
            "Epoch64 | Batch: 17 Loss:  0.4812\n",
            "Epoch64 | Batch: 18 Loss:  0.5018\n",
            "Epoch64 | Batch: 19 Loss:  0.5358\n",
            "Epoch64 | Batch: 20 Loss:  0.4440\n",
            "Epoch64 | Batch: 21 Loss:  0.4538\n",
            "Epoch64 | Batch: 22 Loss:  0.3970\n",
            "Epoch64 | Batch: 23 Loss:  0.3892\n",
            "Epoch64 | Batch: 24 Loss:  0.5308\n",
            "Epoch65 | Batch: 1 Loss:  0.5851\n",
            "Epoch65 | Batch: 2 Loss:  0.4587\n",
            "Epoch65 | Batch: 3 Loss:  0.4512\n",
            "Epoch65 | Batch: 4 Loss:  0.5798\n",
            "Epoch65 | Batch: 5 Loss:  0.3713\n",
            "Epoch65 | Batch: 6 Loss:  0.4274\n",
            "Epoch65 | Batch: 7 Loss:  0.5529\n",
            "Epoch65 | Batch: 8 Loss:  0.4602\n",
            "Epoch65 | Batch: 9 Loss:  0.5057\n",
            "Epoch65 | Batch: 10 Loss:  0.4955\n",
            "Epoch65 | Batch: 11 Loss:  0.3393\n",
            "Epoch65 | Batch: 12 Loss:  0.4693\n",
            "Epoch65 | Batch: 13 Loss:  0.5150\n",
            "Epoch65 | Batch: 14 Loss:  0.3553\n",
            "Epoch65 | Batch: 15 Loss:  0.5868\n",
            "Epoch65 | Batch: 16 Loss:  0.5893\n",
            "Epoch65 | Batch: 17 Loss:  0.3775\n",
            "Epoch65 | Batch: 18 Loss:  0.2623\n",
            "Epoch65 | Batch: 19 Loss:  0.5295\n",
            "Epoch65 | Batch: 20 Loss:  0.3945\n",
            "Epoch65 | Batch: 21 Loss:  0.4216\n",
            "Epoch65 | Batch: 22 Loss:  0.4928\n",
            "Epoch65 | Batch: 23 Loss:  0.4507\n",
            "Epoch65 | Batch: 24 Loss:  0.5685\n",
            "Epoch66 | Batch: 1 Loss:  0.4625\n",
            "Epoch66 | Batch: 2 Loss:  0.5463\n",
            "Epoch66 | Batch: 3 Loss:  0.5278\n",
            "Epoch66 | Batch: 4 Loss:  0.3463\n",
            "Epoch66 | Batch: 5 Loss:  0.3934\n",
            "Epoch66 | Batch: 6 Loss:  0.4564\n",
            "Epoch66 | Batch: 7 Loss:  0.4198\n",
            "Epoch66 | Batch: 8 Loss:  0.4195\n",
            "Epoch66 | Batch: 9 Loss:  0.4199\n",
            "Epoch66 | Batch: 10 Loss:  0.5191\n",
            "Epoch66 | Batch: 11 Loss:  0.3986\n",
            "Epoch66 | Batch: 12 Loss:  0.3533\n",
            "Epoch66 | Batch: 13 Loss:  0.3651\n",
            "Epoch66 | Batch: 14 Loss:  0.5847\n",
            "Epoch66 | Batch: 15 Loss:  0.6496\n",
            "Epoch66 | Batch: 16 Loss:  0.6093\n",
            "Epoch66 | Batch: 17 Loss:  0.6275\n",
            "Epoch66 | Batch: 18 Loss:  0.5080\n",
            "Epoch66 | Batch: 19 Loss:  0.5365\n",
            "Epoch66 | Batch: 20 Loss:  0.5689\n",
            "Epoch66 | Batch: 21 Loss:  0.4608\n",
            "Epoch66 | Batch: 22 Loss:  0.1901\n",
            "Epoch66 | Batch: 23 Loss:  0.3217\n",
            "Epoch66 | Batch: 24 Loss:  0.5282\n",
            "Epoch67 | Batch: 1 Loss:  0.4005\n",
            "Epoch67 | Batch: 2 Loss:  0.5719\n",
            "Epoch67 | Batch: 3 Loss:  0.5265\n",
            "Epoch67 | Batch: 4 Loss:  0.4989\n",
            "Epoch67 | Batch: 5 Loss:  0.3846\n",
            "Epoch67 | Batch: 6 Loss:  0.4521\n",
            "Epoch67 | Batch: 7 Loss:  0.6354\n",
            "Epoch67 | Batch: 8 Loss:  0.3900\n",
            "Epoch67 | Batch: 9 Loss:  0.4097\n",
            "Epoch67 | Batch: 10 Loss:  0.4986\n",
            "Epoch67 | Batch: 11 Loss:  0.3729\n",
            "Epoch67 | Batch: 12 Loss:  0.5890\n",
            "Epoch67 | Batch: 13 Loss:  0.4196\n",
            "Epoch67 | Batch: 14 Loss:  0.5198\n",
            "Epoch67 | Batch: 15 Loss:  0.4071\n",
            "Epoch67 | Batch: 16 Loss:  0.4463\n",
            "Epoch67 | Batch: 17 Loss:  0.4562\n",
            "Epoch67 | Batch: 18 Loss:  0.3712\n",
            "Epoch67 | Batch: 19 Loss:  0.3962\n",
            "Epoch67 | Batch: 20 Loss:  0.3048\n",
            "Epoch67 | Batch: 21 Loss:  0.5628\n",
            "Epoch67 | Batch: 22 Loss:  0.6619\n",
            "Epoch67 | Batch: 23 Loss:  0.4976\n",
            "Epoch67 | Batch: 24 Loss:  0.4865\n",
            "Epoch68 | Batch: 1 Loss:  0.3663\n",
            "Epoch68 | Batch: 2 Loss:  0.7135\n",
            "Epoch68 | Batch: 3 Loss:  0.3576\n",
            "Epoch68 | Batch: 4 Loss:  0.5153\n",
            "Epoch68 | Batch: 5 Loss:  0.4284\n",
            "Epoch68 | Batch: 6 Loss:  0.5846\n",
            "Epoch68 | Batch: 7 Loss:  0.3761\n",
            "Epoch68 | Batch: 8 Loss:  0.3827\n",
            "Epoch68 | Batch: 9 Loss:  0.4879\n",
            "Epoch68 | Batch: 10 Loss:  0.3850\n",
            "Epoch68 | Batch: 11 Loss:  0.3784\n",
            "Epoch68 | Batch: 12 Loss:  0.4361\n",
            "Epoch68 | Batch: 13 Loss:  0.5005\n",
            "Epoch68 | Batch: 14 Loss:  0.5961\n",
            "Epoch68 | Batch: 15 Loss:  0.4033\n",
            "Epoch68 | Batch: 16 Loss:  0.6722\n",
            "Epoch68 | Batch: 17 Loss:  0.3941\n",
            "Epoch68 | Batch: 18 Loss:  0.3899\n",
            "Epoch68 | Batch: 19 Loss:  0.4279\n",
            "Epoch68 | Batch: 20 Loss:  0.4796\n",
            "Epoch68 | Batch: 21 Loss:  0.4752\n",
            "Epoch68 | Batch: 22 Loss:  0.4863\n",
            "Epoch68 | Batch: 23 Loss:  0.3898\n",
            "Epoch68 | Batch: 24 Loss:  0.5973\n",
            "Epoch69 | Batch: 1 Loss:  0.6688\n",
            "Epoch69 | Batch: 2 Loss:  0.7488\n",
            "Epoch69 | Batch: 3 Loss:  0.4271\n",
            "Epoch69 | Batch: 4 Loss:  0.4337\n",
            "Epoch69 | Batch: 5 Loss:  0.6298\n",
            "Epoch69 | Batch: 6 Loss:  0.4710\n",
            "Epoch69 | Batch: 7 Loss:  0.4406\n",
            "Epoch69 | Batch: 8 Loss:  0.3643\n",
            "Epoch69 | Batch: 9 Loss:  0.4498\n",
            "Epoch69 | Batch: 10 Loss:  0.4235\n",
            "Epoch69 | Batch: 11 Loss:  0.3989\n",
            "Epoch69 | Batch: 12 Loss:  0.4262\n",
            "Epoch69 | Batch: 13 Loss:  0.4813\n",
            "Epoch69 | Batch: 14 Loss:  0.2733\n",
            "Epoch69 | Batch: 15 Loss:  0.5457\n",
            "Epoch69 | Batch: 16 Loss:  0.3824\n",
            "Epoch69 | Batch: 17 Loss:  0.3732\n",
            "Epoch69 | Batch: 18 Loss:  0.4408\n",
            "Epoch69 | Batch: 19 Loss:  0.5325\n",
            "Epoch69 | Batch: 20 Loss:  0.4970\n",
            "Epoch69 | Batch: 21 Loss:  0.3907\n",
            "Epoch69 | Batch: 22 Loss:  0.4473\n",
            "Epoch69 | Batch: 23 Loss:  0.5125\n",
            "Epoch69 | Batch: 24 Loss:  0.3939\n",
            "Epoch70 | Batch: 1 Loss:  0.3268\n",
            "Epoch70 | Batch: 2 Loss:  0.4625\n",
            "Epoch70 | Batch: 3 Loss:  0.3656\n",
            "Epoch70 | Batch: 4 Loss:  0.4323\n",
            "Epoch70 | Batch: 5 Loss:  0.5026\n",
            "Epoch70 | Batch: 6 Loss:  0.4280\n",
            "Epoch70 | Batch: 7 Loss:  0.4602\n",
            "Epoch70 | Batch: 8 Loss:  0.3637\n",
            "Epoch70 | Batch: 9 Loss:  0.4917\n",
            "Epoch70 | Batch: 10 Loss:  0.5927\n",
            "Epoch70 | Batch: 11 Loss:  0.4967\n",
            "Epoch70 | Batch: 12 Loss:  0.6443\n",
            "Epoch70 | Batch: 13 Loss:  0.4674\n",
            "Epoch70 | Batch: 14 Loss:  0.4603\n",
            "Epoch70 | Batch: 15 Loss:  0.5776\n",
            "Epoch70 | Batch: 16 Loss:  0.3924\n",
            "Epoch70 | Batch: 17 Loss:  0.3684\n",
            "Epoch70 | Batch: 18 Loss:  0.4465\n",
            "Epoch70 | Batch: 19 Loss:  0.4494\n",
            "Epoch70 | Batch: 20 Loss:  0.3737\n",
            "Epoch70 | Batch: 21 Loss:  0.4375\n",
            "Epoch70 | Batch: 22 Loss:  0.5172\n",
            "Epoch70 | Batch: 23 Loss:  0.6644\n",
            "Epoch70 | Batch: 24 Loss:  0.5016\n",
            "Epoch71 | Batch: 1 Loss:  0.5086\n",
            "Epoch71 | Batch: 2 Loss:  0.4346\n",
            "Epoch71 | Batch: 3 Loss:  0.5933\n",
            "Epoch71 | Batch: 4 Loss:  0.3867\n",
            "Epoch71 | Batch: 5 Loss:  0.4158\n",
            "Epoch71 | Batch: 6 Loss:  0.3950\n",
            "Epoch71 | Batch: 7 Loss:  0.4643\n",
            "Epoch71 | Batch: 8 Loss:  0.5156\n",
            "Epoch71 | Batch: 9 Loss:  0.5759\n",
            "Epoch71 | Batch: 10 Loss:  0.4015\n",
            "Epoch71 | Batch: 11 Loss:  0.5688\n",
            "Epoch71 | Batch: 12 Loss:  0.4157\n",
            "Epoch71 | Batch: 13 Loss:  0.4098\n",
            "Epoch71 | Batch: 14 Loss:  0.4455\n",
            "Epoch71 | Batch: 15 Loss:  0.4413\n",
            "Epoch71 | Batch: 16 Loss:  0.4605\n",
            "Epoch71 | Batch: 17 Loss:  0.3814\n",
            "Epoch71 | Batch: 18 Loss:  0.5463\n",
            "Epoch71 | Batch: 19 Loss:  0.4145\n",
            "Epoch71 | Batch: 20 Loss:  0.3851\n",
            "Epoch71 | Batch: 21 Loss:  0.5998\n",
            "Epoch71 | Batch: 22 Loss:  0.5314\n",
            "Epoch71 | Batch: 23 Loss:  0.4469\n",
            "Epoch71 | Batch: 24 Loss:  0.4116\n",
            "Epoch72 | Batch: 1 Loss:  0.4627\n",
            "Epoch72 | Batch: 2 Loss:  0.5094\n",
            "Epoch72 | Batch: 3 Loss:  0.4589\n",
            "Epoch72 | Batch: 4 Loss:  0.4815\n",
            "Epoch72 | Batch: 5 Loss:  0.4498\n",
            "Epoch72 | Batch: 6 Loss:  0.4897\n",
            "Epoch72 | Batch: 7 Loss:  0.3623\n",
            "Epoch72 | Batch: 8 Loss:  0.4388\n",
            "Epoch72 | Batch: 9 Loss:  0.3650\n",
            "Epoch72 | Batch: 10 Loss:  0.4197\n",
            "Epoch72 | Batch: 11 Loss:  0.3758\n",
            "Epoch72 | Batch: 12 Loss:  0.6059\n",
            "Epoch72 | Batch: 13 Loss:  0.4327\n",
            "Epoch72 | Batch: 14 Loss:  0.3842\n",
            "Epoch72 | Batch: 15 Loss:  0.4008\n",
            "Epoch72 | Batch: 16 Loss:  0.7346\n",
            "Epoch72 | Batch: 17 Loss:  0.3747\n",
            "Epoch72 | Batch: 18 Loss:  0.4830\n",
            "Epoch72 | Batch: 19 Loss:  0.4191\n",
            "Epoch72 | Batch: 20 Loss:  0.4509\n",
            "Epoch72 | Batch: 21 Loss:  0.4745\n",
            "Epoch72 | Batch: 22 Loss:  0.5671\n",
            "Epoch72 | Batch: 23 Loss:  0.6775\n",
            "Epoch72 | Batch: 24 Loss:  0.4196\n",
            "Epoch73 | Batch: 1 Loss:  0.4345\n",
            "Epoch73 | Batch: 2 Loss:  0.5123\n",
            "Epoch73 | Batch: 3 Loss:  0.6430\n",
            "Epoch73 | Batch: 4 Loss:  0.3851\n",
            "Epoch73 | Batch: 5 Loss:  0.4254\n",
            "Epoch73 | Batch: 6 Loss:  0.3874\n",
            "Epoch73 | Batch: 7 Loss:  0.5978\n",
            "Epoch73 | Batch: 8 Loss:  0.4553\n",
            "Epoch73 | Batch: 9 Loss:  0.5246\n",
            "Epoch73 | Batch: 10 Loss:  0.4639\n",
            "Epoch73 | Batch: 11 Loss:  0.4175\n",
            "Epoch73 | Batch: 12 Loss:  0.3778\n",
            "Epoch73 | Batch: 13 Loss:  0.4739\n",
            "Epoch73 | Batch: 14 Loss:  0.4385\n",
            "Epoch73 | Batch: 15 Loss:  0.5015\n",
            "Epoch73 | Batch: 16 Loss:  0.3748\n",
            "Epoch73 | Batch: 17 Loss:  0.4299\n",
            "Epoch73 | Batch: 18 Loss:  0.4395\n",
            "Epoch73 | Batch: 19 Loss:  0.4249\n",
            "Epoch73 | Batch: 20 Loss:  0.4450\n",
            "Epoch73 | Batch: 21 Loss:  0.5288\n",
            "Epoch73 | Batch: 22 Loss:  0.4285\n",
            "Epoch73 | Batch: 23 Loss:  0.4729\n",
            "Epoch73 | Batch: 24 Loss:  0.5407\n",
            "Epoch74 | Batch: 1 Loss:  0.6464\n",
            "Epoch74 | Batch: 2 Loss:  0.4110\n",
            "Epoch74 | Batch: 3 Loss:  0.5378\n",
            "Epoch74 | Batch: 4 Loss:  0.4601\n",
            "Epoch74 | Batch: 5 Loss:  0.4695\n",
            "Epoch74 | Batch: 6 Loss:  0.5373\n",
            "Epoch74 | Batch: 7 Loss:  0.5515\n",
            "Epoch74 | Batch: 8 Loss:  0.3883\n",
            "Epoch74 | Batch: 9 Loss:  0.4067\n",
            "Epoch74 | Batch: 10 Loss:  0.6077\n",
            "Epoch74 | Batch: 11 Loss:  0.4468\n",
            "Epoch74 | Batch: 12 Loss:  0.4362\n",
            "Epoch74 | Batch: 13 Loss:  0.5629\n",
            "Epoch74 | Batch: 14 Loss:  0.4270\n",
            "Epoch74 | Batch: 15 Loss:  0.5487\n",
            "Epoch74 | Batch: 16 Loss:  0.4133\n",
            "Epoch74 | Batch: 17 Loss:  0.3916\n",
            "Epoch74 | Batch: 18 Loss:  0.5419\n",
            "Epoch74 | Batch: 19 Loss:  0.4813\n",
            "Epoch74 | Batch: 20 Loss:  0.5221\n",
            "Epoch74 | Batch: 21 Loss:  0.3059\n",
            "Epoch74 | Batch: 22 Loss:  0.3857\n",
            "Epoch74 | Batch: 23 Loss:  0.3300\n",
            "Epoch74 | Batch: 24 Loss:  0.3302\n",
            "Epoch75 | Batch: 1 Loss:  0.3405\n",
            "Epoch75 | Batch: 2 Loss:  0.5701\n",
            "Epoch75 | Batch: 3 Loss:  0.3865\n",
            "Epoch75 | Batch: 4 Loss:  0.4282\n",
            "Epoch75 | Batch: 5 Loss:  0.7281\n",
            "Epoch75 | Batch: 6 Loss:  0.3440\n",
            "Epoch75 | Batch: 7 Loss:  0.4097\n",
            "Epoch75 | Batch: 8 Loss:  0.3453\n",
            "Epoch75 | Batch: 9 Loss:  0.4624\n",
            "Epoch75 | Batch: 10 Loss:  0.4340\n",
            "Epoch75 | Batch: 11 Loss:  0.4886\n",
            "Epoch75 | Batch: 12 Loss:  0.5567\n",
            "Epoch75 | Batch: 13 Loss:  0.5514\n",
            "Epoch75 | Batch: 14 Loss:  0.4649\n",
            "Epoch75 | Batch: 15 Loss:  0.4489\n",
            "Epoch75 | Batch: 16 Loss:  0.5263\n",
            "Epoch75 | Batch: 17 Loss:  0.4017\n",
            "Epoch75 | Batch: 18 Loss:  0.4505\n",
            "Epoch75 | Batch: 19 Loss:  0.4034\n",
            "Epoch75 | Batch: 20 Loss:  0.4836\n",
            "Epoch75 | Batch: 21 Loss:  0.4885\n",
            "Epoch75 | Batch: 22 Loss:  0.4716\n",
            "Epoch75 | Batch: 23 Loss:  0.4118\n",
            "Epoch75 | Batch: 24 Loss:  0.5145\n",
            "Epoch76 | Batch: 1 Loss:  0.6222\n",
            "Epoch76 | Batch: 2 Loss:  0.5238\n",
            "Epoch76 | Batch: 3 Loss:  0.4692\n",
            "Epoch76 | Batch: 4 Loss:  0.5279\n",
            "Epoch76 | Batch: 5 Loss:  0.4607\n",
            "Epoch76 | Batch: 6 Loss:  0.4026\n",
            "Epoch76 | Batch: 7 Loss:  0.4445\n",
            "Epoch76 | Batch: 8 Loss:  0.5309\n",
            "Epoch76 | Batch: 9 Loss:  0.5212\n",
            "Epoch76 | Batch: 10 Loss:  0.4643\n",
            "Epoch76 | Batch: 11 Loss:  0.3062\n",
            "Epoch76 | Batch: 12 Loss:  0.4685\n",
            "Epoch76 | Batch: 13 Loss:  0.3376\n",
            "Epoch76 | Batch: 14 Loss:  0.5705\n",
            "Epoch76 | Batch: 15 Loss:  0.4463\n",
            "Epoch76 | Batch: 16 Loss:  0.4453\n",
            "Epoch76 | Batch: 17 Loss:  0.4052\n",
            "Epoch76 | Batch: 18 Loss:  0.4816\n",
            "Epoch76 | Batch: 19 Loss:  0.4608\n",
            "Epoch76 | Batch: 20 Loss:  0.5297\n",
            "Epoch76 | Batch: 21 Loss:  0.2898\n",
            "Epoch76 | Batch: 22 Loss:  0.4069\n",
            "Epoch76 | Batch: 23 Loss:  0.5091\n",
            "Epoch76 | Batch: 24 Loss:  0.5121\n",
            "Epoch77 | Batch: 1 Loss:  0.6150\n",
            "Epoch77 | Batch: 2 Loss:  0.4894\n",
            "Epoch77 | Batch: 3 Loss:  0.4716\n",
            "Epoch77 | Batch: 4 Loss:  0.5106\n",
            "Epoch77 | Batch: 5 Loss:  0.4086\n",
            "Epoch77 | Batch: 6 Loss:  0.5028\n",
            "Epoch77 | Batch: 7 Loss:  0.3796\n",
            "Epoch77 | Batch: 8 Loss:  0.8001\n",
            "Epoch77 | Batch: 9 Loss:  0.4139\n",
            "Epoch77 | Batch: 10 Loss:  0.5121\n",
            "Epoch77 | Batch: 11 Loss:  0.3475\n",
            "Epoch77 | Batch: 12 Loss:  0.4892\n",
            "Epoch77 | Batch: 13 Loss:  0.3033\n",
            "Epoch77 | Batch: 14 Loss:  0.4872\n",
            "Epoch77 | Batch: 15 Loss:  0.5742\n",
            "Epoch77 | Batch: 16 Loss:  0.3738\n",
            "Epoch77 | Batch: 17 Loss:  0.4968\n",
            "Epoch77 | Batch: 18 Loss:  0.3233\n",
            "Epoch77 | Batch: 19 Loss:  0.4321\n",
            "Epoch77 | Batch: 20 Loss:  0.4301\n",
            "Epoch77 | Batch: 21 Loss:  0.4273\n",
            "Epoch77 | Batch: 22 Loss:  0.4039\n",
            "Epoch77 | Batch: 23 Loss:  0.4847\n",
            "Epoch77 | Batch: 24 Loss:  0.4486\n",
            "Epoch78 | Batch: 1 Loss:  0.3781\n",
            "Epoch78 | Batch: 2 Loss:  0.5404\n",
            "Epoch78 | Batch: 3 Loss:  0.3759\n",
            "Epoch78 | Batch: 4 Loss:  0.4131\n",
            "Epoch78 | Batch: 5 Loss:  0.4754\n",
            "Epoch78 | Batch: 6 Loss:  0.4340\n",
            "Epoch78 | Batch: 7 Loss:  0.4353\n",
            "Epoch78 | Batch: 8 Loss:  0.3872\n",
            "Epoch78 | Batch: 9 Loss:  0.4582\n",
            "Epoch78 | Batch: 10 Loss:  0.5364\n",
            "Epoch78 | Batch: 11 Loss:  0.4393\n",
            "Epoch78 | Batch: 12 Loss:  0.5608\n",
            "Epoch78 | Batch: 13 Loss:  0.5232\n",
            "Epoch78 | Batch: 14 Loss:  0.3557\n",
            "Epoch78 | Batch: 15 Loss:  0.3931\n",
            "Epoch78 | Batch: 16 Loss:  0.5951\n",
            "Epoch78 | Batch: 17 Loss:  0.4591\n",
            "Epoch78 | Batch: 18 Loss:  0.4789\n",
            "Epoch78 | Batch: 19 Loss:  0.6375\n",
            "Epoch78 | Batch: 20 Loss:  0.4161\n",
            "Epoch78 | Batch: 21 Loss:  0.3896\n",
            "Epoch78 | Batch: 22 Loss:  0.5843\n",
            "Epoch78 | Batch: 23 Loss:  0.4592\n",
            "Epoch78 | Batch: 24 Loss:  0.3203\n",
            "Epoch79 | Batch: 1 Loss:  0.3006\n",
            "Epoch79 | Batch: 2 Loss:  0.3536\n",
            "Epoch79 | Batch: 3 Loss:  0.6142\n",
            "Epoch79 | Batch: 4 Loss:  0.4903\n",
            "Epoch79 | Batch: 5 Loss:  0.4582\n",
            "Epoch79 | Batch: 6 Loss:  0.3522\n",
            "Epoch79 | Batch: 7 Loss:  0.4209\n",
            "Epoch79 | Batch: 8 Loss:  0.4907\n",
            "Epoch79 | Batch: 9 Loss:  0.4623\n",
            "Epoch79 | Batch: 10 Loss:  0.5205\n",
            "Epoch79 | Batch: 11 Loss:  0.2927\n",
            "Epoch79 | Batch: 12 Loss:  0.6207\n",
            "Epoch79 | Batch: 13 Loss:  0.3529\n",
            "Epoch79 | Batch: 14 Loss:  0.5788\n",
            "Epoch79 | Batch: 15 Loss:  0.6048\n",
            "Epoch79 | Batch: 16 Loss:  0.5563\n",
            "Epoch79 | Batch: 17 Loss:  0.4333\n",
            "Epoch79 | Batch: 18 Loss:  0.3536\n",
            "Epoch79 | Batch: 19 Loss:  0.5783\n",
            "Epoch79 | Batch: 20 Loss:  0.4809\n",
            "Epoch79 | Batch: 21 Loss:  0.4837\n",
            "Epoch79 | Batch: 22 Loss:  0.3729\n",
            "Epoch79 | Batch: 23 Loss:  0.4785\n",
            "Epoch79 | Batch: 24 Loss:  0.4067\n",
            "Epoch80 | Batch: 1 Loss:  0.5730\n",
            "Epoch80 | Batch: 2 Loss:  0.3920\n",
            "Epoch80 | Batch: 3 Loss:  0.4779\n",
            "Epoch80 | Batch: 4 Loss:  0.4094\n",
            "Epoch80 | Batch: 5 Loss:  0.4077\n",
            "Epoch80 | Batch: 6 Loss:  0.3214\n",
            "Epoch80 | Batch: 7 Loss:  0.4603\n",
            "Epoch80 | Batch: 8 Loss:  0.4052\n",
            "Epoch80 | Batch: 9 Loss:  0.7154\n",
            "Epoch80 | Batch: 10 Loss:  0.6581\n",
            "Epoch80 | Batch: 11 Loss:  0.4279\n",
            "Epoch80 | Batch: 12 Loss:  0.3531\n",
            "Epoch80 | Batch: 13 Loss:  0.4430\n",
            "Epoch80 | Batch: 14 Loss:  0.4764\n",
            "Epoch80 | Batch: 15 Loss:  0.3980\n",
            "Epoch80 | Batch: 16 Loss:  0.3854\n",
            "Epoch80 | Batch: 17 Loss:  0.4269\n",
            "Epoch80 | Batch: 18 Loss:  0.4847\n",
            "Epoch80 | Batch: 19 Loss:  0.5326\n",
            "Epoch80 | Batch: 20 Loss:  0.4577\n",
            "Epoch80 | Batch: 21 Loss:  0.5333\n",
            "Epoch80 | Batch: 22 Loss:  0.4556\n",
            "Epoch80 | Batch: 23 Loss:  0.5218\n",
            "Epoch80 | Batch: 24 Loss:  0.3560\n",
            "Epoch81 | Batch: 1 Loss:  0.6676\n",
            "Epoch81 | Batch: 2 Loss:  0.4681\n",
            "Epoch81 | Batch: 3 Loss:  0.4530\n",
            "Epoch81 | Batch: 4 Loss:  0.3972\n",
            "Epoch81 | Batch: 5 Loss:  0.2113\n",
            "Epoch81 | Batch: 6 Loss:  0.5060\n",
            "Epoch81 | Batch: 7 Loss:  0.4960\n",
            "Epoch81 | Batch: 8 Loss:  0.4847\n",
            "Epoch81 | Batch: 9 Loss:  0.3754\n",
            "Epoch81 | Batch: 10 Loss:  0.3519\n",
            "Epoch81 | Batch: 11 Loss:  0.4581\n",
            "Epoch81 | Batch: 12 Loss:  0.3849\n",
            "Epoch81 | Batch: 13 Loss:  0.4666\n",
            "Epoch81 | Batch: 14 Loss:  0.5045\n",
            "Epoch81 | Batch: 15 Loss:  0.5678\n",
            "Epoch81 | Batch: 16 Loss:  0.4652\n",
            "Epoch81 | Batch: 17 Loss:  0.4417\n",
            "Epoch81 | Batch: 18 Loss:  0.4330\n",
            "Epoch81 | Batch: 19 Loss:  0.3509\n",
            "Epoch81 | Batch: 20 Loss:  0.5430\n",
            "Epoch81 | Batch: 21 Loss:  0.5419\n",
            "Epoch81 | Batch: 22 Loss:  0.6266\n",
            "Epoch81 | Batch: 23 Loss:  0.4073\n",
            "Epoch81 | Batch: 24 Loss:  0.4978\n",
            "Epoch82 | Batch: 1 Loss:  0.5132\n",
            "Epoch82 | Batch: 2 Loss:  0.4901\n",
            "Epoch82 | Batch: 3 Loss:  0.3581\n",
            "Epoch82 | Batch: 4 Loss:  0.5509\n",
            "Epoch82 | Batch: 5 Loss:  0.4024\n",
            "Epoch82 | Batch: 6 Loss:  0.3432\n",
            "Epoch82 | Batch: 7 Loss:  0.6063\n",
            "Epoch82 | Batch: 8 Loss:  0.5522\n",
            "Epoch82 | Batch: 9 Loss:  0.4518\n",
            "Epoch82 | Batch: 10 Loss:  0.4127\n",
            "Epoch82 | Batch: 11 Loss:  0.4287\n",
            "Epoch82 | Batch: 12 Loss:  0.4183\n",
            "Epoch82 | Batch: 13 Loss:  0.4898\n",
            "Epoch82 | Batch: 14 Loss:  0.5524\n",
            "Epoch82 | Batch: 15 Loss:  0.4474\n",
            "Epoch82 | Batch: 16 Loss:  0.4249\n",
            "Epoch82 | Batch: 17 Loss:  0.4234\n",
            "Epoch82 | Batch: 18 Loss:  0.3594\n",
            "Epoch82 | Batch: 19 Loss:  0.5635\n",
            "Epoch82 | Batch: 20 Loss:  0.5467\n",
            "Epoch82 | Batch: 21 Loss:  0.4210\n",
            "Epoch82 | Batch: 22 Loss:  0.4277\n",
            "Epoch82 | Batch: 23 Loss:  0.5910\n",
            "Epoch82 | Batch: 24 Loss:  0.2999\n",
            "Epoch83 | Batch: 1 Loss:  0.3820\n",
            "Epoch83 | Batch: 2 Loss:  0.5747\n",
            "Epoch83 | Batch: 3 Loss:  0.3926\n",
            "Epoch83 | Batch: 4 Loss:  0.4484\n",
            "Epoch83 | Batch: 5 Loss:  0.4747\n",
            "Epoch83 | Batch: 6 Loss:  0.2881\n",
            "Epoch83 | Batch: 7 Loss:  0.4240\n",
            "Epoch83 | Batch: 8 Loss:  0.5208\n",
            "Epoch83 | Batch: 9 Loss:  0.4139\n",
            "Epoch83 | Batch: 10 Loss:  0.4880\n",
            "Epoch83 | Batch: 11 Loss:  0.5322\n",
            "Epoch83 | Batch: 12 Loss:  0.4089\n",
            "Epoch83 | Batch: 13 Loss:  0.3250\n",
            "Epoch83 | Batch: 14 Loss:  0.3049\n",
            "Epoch83 | Batch: 15 Loss:  0.5194\n",
            "Epoch83 | Batch: 16 Loss:  0.5050\n",
            "Epoch83 | Batch: 17 Loss:  0.5483\n",
            "Epoch83 | Batch: 18 Loss:  0.4655\n",
            "Epoch83 | Batch: 19 Loss:  0.5481\n",
            "Epoch83 | Batch: 20 Loss:  0.6586\n",
            "Epoch83 | Batch: 21 Loss:  0.4835\n",
            "Epoch83 | Batch: 22 Loss:  0.4040\n",
            "Epoch83 | Batch: 23 Loss:  0.5457\n",
            "Epoch83 | Batch: 24 Loss:  0.3837\n",
            "Epoch84 | Batch: 1 Loss:  0.5665\n",
            "Epoch84 | Batch: 2 Loss:  0.4774\n",
            "Epoch84 | Batch: 3 Loss:  0.3984\n",
            "Epoch84 | Batch: 4 Loss:  0.5200\n",
            "Epoch84 | Batch: 5 Loss:  0.6257\n",
            "Epoch84 | Batch: 6 Loss:  0.2641\n",
            "Epoch84 | Batch: 7 Loss:  0.4992\n",
            "Epoch84 | Batch: 8 Loss:  0.4355\n",
            "Epoch84 | Batch: 9 Loss:  0.2874\n",
            "Epoch84 | Batch: 10 Loss:  0.4300\n",
            "Epoch84 | Batch: 11 Loss:  0.4960\n",
            "Epoch84 | Batch: 12 Loss:  0.4003\n",
            "Epoch84 | Batch: 13 Loss:  0.4396\n",
            "Epoch84 | Batch: 14 Loss:  0.4171\n",
            "Epoch84 | Batch: 15 Loss:  0.5410\n",
            "Epoch84 | Batch: 16 Loss:  0.4784\n",
            "Epoch84 | Batch: 17 Loss:  0.6053\n",
            "Epoch84 | Batch: 18 Loss:  0.4621\n",
            "Epoch84 | Batch: 19 Loss:  0.5677\n",
            "Epoch84 | Batch: 20 Loss:  0.4823\n",
            "Epoch84 | Batch: 21 Loss:  0.5014\n",
            "Epoch84 | Batch: 22 Loss:  0.3460\n",
            "Epoch84 | Batch: 23 Loss:  0.4059\n",
            "Epoch84 | Batch: 24 Loss:  0.3363\n",
            "Epoch85 | Batch: 1 Loss:  0.4893\n",
            "Epoch85 | Batch: 2 Loss:  0.5266\n",
            "Epoch85 | Batch: 3 Loss:  0.5464\n",
            "Epoch85 | Batch: 4 Loss:  0.4088\n",
            "Epoch85 | Batch: 5 Loss:  0.5312\n",
            "Epoch85 | Batch: 6 Loss:  0.3155\n",
            "Epoch85 | Batch: 7 Loss:  0.3245\n",
            "Epoch85 | Batch: 8 Loss:  0.6197\n",
            "Epoch85 | Batch: 9 Loss:  0.3909\n",
            "Epoch85 | Batch: 10 Loss:  0.4972\n",
            "Epoch85 | Batch: 11 Loss:  0.4090\n",
            "Epoch85 | Batch: 12 Loss:  0.4460\n",
            "Epoch85 | Batch: 13 Loss:  0.5328\n",
            "Epoch85 | Batch: 14 Loss:  0.5721\n",
            "Epoch85 | Batch: 15 Loss:  0.5791\n",
            "Epoch85 | Batch: 16 Loss:  0.3563\n",
            "Epoch85 | Batch: 17 Loss:  0.4700\n",
            "Epoch85 | Batch: 18 Loss:  0.3673\n",
            "Epoch85 | Batch: 19 Loss:  0.5536\n",
            "Epoch85 | Batch: 20 Loss:  0.3730\n",
            "Epoch85 | Batch: 21 Loss:  0.4423\n",
            "Epoch85 | Batch: 22 Loss:  0.4400\n",
            "Epoch85 | Batch: 23 Loss:  0.3944\n",
            "Epoch85 | Batch: 24 Loss:  0.5057\n",
            "Epoch86 | Batch: 1 Loss:  0.4468\n",
            "Epoch86 | Batch: 2 Loss:  0.3508\n",
            "Epoch86 | Batch: 3 Loss:  0.4346\n",
            "Epoch86 | Batch: 4 Loss:  0.5142\n",
            "Epoch86 | Batch: 5 Loss:  0.5229\n",
            "Epoch86 | Batch: 6 Loss:  0.5106\n",
            "Epoch86 | Batch: 7 Loss:  0.5296\n",
            "Epoch86 | Batch: 8 Loss:  0.2341\n",
            "Epoch86 | Batch: 9 Loss:  0.6185\n",
            "Epoch86 | Batch: 10 Loss:  0.6097\n",
            "Epoch86 | Batch: 11 Loss:  0.3441\n",
            "Epoch86 | Batch: 12 Loss:  0.3615\n",
            "Epoch86 | Batch: 13 Loss:  0.4777\n",
            "Epoch86 | Batch: 14 Loss:  0.4113\n",
            "Epoch86 | Batch: 15 Loss:  0.5440\n",
            "Epoch86 | Batch: 16 Loss:  0.3665\n",
            "Epoch86 | Batch: 17 Loss:  0.4606\n",
            "Epoch86 | Batch: 18 Loss:  0.5396\n",
            "Epoch86 | Batch: 19 Loss:  0.5208\n",
            "Epoch86 | Batch: 20 Loss:  0.3833\n",
            "Epoch86 | Batch: 21 Loss:  0.5761\n",
            "Epoch86 | Batch: 22 Loss:  0.3278\n",
            "Epoch86 | Batch: 23 Loss:  0.4385\n",
            "Epoch86 | Batch: 24 Loss:  0.5190\n",
            "Epoch87 | Batch: 1 Loss:  0.5056\n",
            "Epoch87 | Batch: 2 Loss:  0.5557\n",
            "Epoch87 | Batch: 3 Loss:  0.3661\n",
            "Epoch87 | Batch: 4 Loss:  0.3408\n",
            "Epoch87 | Batch: 5 Loss:  0.4845\n",
            "Epoch87 | Batch: 6 Loss:  0.4843\n",
            "Epoch87 | Batch: 7 Loss:  0.3041\n",
            "Epoch87 | Batch: 8 Loss:  0.4922\n",
            "Epoch87 | Batch: 9 Loss:  0.4835\n",
            "Epoch87 | Batch: 10 Loss:  0.5622\n",
            "Epoch87 | Batch: 11 Loss:  0.5245\n",
            "Epoch87 | Batch: 12 Loss:  0.5142\n",
            "Epoch87 | Batch: 13 Loss:  0.4258\n",
            "Epoch87 | Batch: 14 Loss:  0.5331\n",
            "Epoch87 | Batch: 15 Loss:  0.5911\n",
            "Epoch87 | Batch: 16 Loss:  0.4466\n",
            "Epoch87 | Batch: 17 Loss:  0.4096\n",
            "Epoch87 | Batch: 18 Loss:  0.4788\n",
            "Epoch87 | Batch: 19 Loss:  0.4915\n",
            "Epoch87 | Batch: 20 Loss:  0.4461\n",
            "Epoch87 | Batch: 21 Loss:  0.5353\n",
            "Epoch87 | Batch: 22 Loss:  0.2318\n",
            "Epoch87 | Batch: 23 Loss:  0.4438\n",
            "Epoch87 | Batch: 24 Loss:  0.3783\n",
            "Epoch88 | Batch: 1 Loss:  0.5323\n",
            "Epoch88 | Batch: 2 Loss:  0.4304\n",
            "Epoch88 | Batch: 3 Loss:  0.6412\n",
            "Epoch88 | Batch: 4 Loss:  0.3836\n",
            "Epoch88 | Batch: 5 Loss:  0.3878\n",
            "Epoch88 | Batch: 6 Loss:  0.4081\n",
            "Epoch88 | Batch: 7 Loss:  0.5077\n",
            "Epoch88 | Batch: 8 Loss:  0.4495\n",
            "Epoch88 | Batch: 9 Loss:  0.3275\n",
            "Epoch88 | Batch: 10 Loss:  0.4274\n",
            "Epoch88 | Batch: 11 Loss:  0.4744\n",
            "Epoch88 | Batch: 12 Loss:  0.4542\n",
            "Epoch88 | Batch: 13 Loss:  0.5228\n",
            "Epoch88 | Batch: 14 Loss:  0.4427\n",
            "Epoch88 | Batch: 15 Loss:  0.4495\n",
            "Epoch88 | Batch: 16 Loss:  0.3707\n",
            "Epoch88 | Batch: 17 Loss:  0.4218\n",
            "Epoch88 | Batch: 18 Loss:  0.5391\n",
            "Epoch88 | Batch: 19 Loss:  0.5240\n",
            "Epoch88 | Batch: 20 Loss:  0.3646\n",
            "Epoch88 | Batch: 21 Loss:  0.3809\n",
            "Epoch88 | Batch: 22 Loss:  0.5161\n",
            "Epoch88 | Batch: 23 Loss:  0.5330\n",
            "Epoch88 | Batch: 24 Loss:  0.5826\n",
            "Epoch89 | Batch: 1 Loss:  0.5646\n",
            "Epoch89 | Batch: 2 Loss:  0.3110\n",
            "Epoch89 | Batch: 3 Loss:  0.5338\n",
            "Epoch89 | Batch: 4 Loss:  0.3941\n",
            "Epoch89 | Batch: 5 Loss:  0.4327\n",
            "Epoch89 | Batch: 6 Loss:  0.4750\n",
            "Epoch89 | Batch: 7 Loss:  0.3872\n",
            "Epoch89 | Batch: 8 Loss:  0.4889\n",
            "Epoch89 | Batch: 9 Loss:  0.5380\n",
            "Epoch89 | Batch: 10 Loss:  0.5592\n",
            "Epoch89 | Batch: 11 Loss:  0.4061\n",
            "Epoch89 | Batch: 12 Loss:  0.3608\n",
            "Epoch89 | Batch: 13 Loss:  0.5813\n",
            "Epoch89 | Batch: 14 Loss:  0.5841\n",
            "Epoch89 | Batch: 15 Loss:  0.4218\n",
            "Epoch89 | Batch: 16 Loss:  0.4827\n",
            "Epoch89 | Batch: 17 Loss:  0.2447\n",
            "Epoch89 | Batch: 18 Loss:  0.5736\n",
            "Epoch89 | Batch: 19 Loss:  0.4607\n",
            "Epoch89 | Batch: 20 Loss:  0.4642\n",
            "Epoch89 | Batch: 21 Loss:  0.4474\n",
            "Epoch89 | Batch: 22 Loss:  0.3838\n",
            "Epoch89 | Batch: 23 Loss:  0.3983\n",
            "Epoch89 | Batch: 24 Loss:  0.4442\n",
            "Epoch90 | Batch: 1 Loss:  0.4017\n",
            "Epoch90 | Batch: 2 Loss:  0.4167\n",
            "Epoch90 | Batch: 3 Loss:  0.4201\n",
            "Epoch90 | Batch: 4 Loss:  0.3422\n",
            "Epoch90 | Batch: 5 Loss:  0.6877\n",
            "Epoch90 | Batch: 6 Loss:  0.4456\n",
            "Epoch90 | Batch: 7 Loss:  0.4560\n",
            "Epoch90 | Batch: 8 Loss:  0.5831\n",
            "Epoch90 | Batch: 9 Loss:  0.3878\n",
            "Epoch90 | Batch: 10 Loss:  0.4208\n",
            "Epoch90 | Batch: 11 Loss:  0.3773\n",
            "Epoch90 | Batch: 12 Loss:  0.4475\n",
            "Epoch90 | Batch: 13 Loss:  0.4874\n",
            "Epoch90 | Batch: 14 Loss:  0.3419\n",
            "Epoch90 | Batch: 15 Loss:  0.4902\n",
            "Epoch90 | Batch: 16 Loss:  0.5824\n",
            "Epoch90 | Batch: 17 Loss:  0.4670\n",
            "Epoch90 | Batch: 18 Loss:  0.3347\n",
            "Epoch90 | Batch: 19 Loss:  0.4663\n",
            "Epoch90 | Batch: 20 Loss:  0.6179\n",
            "Epoch90 | Batch: 21 Loss:  0.4064\n",
            "Epoch90 | Batch: 22 Loss:  0.4838\n",
            "Epoch90 | Batch: 23 Loss:  0.5193\n",
            "Epoch90 | Batch: 24 Loss:  0.5122\n",
            "Epoch91 | Batch: 1 Loss:  0.3748\n",
            "Epoch91 | Batch: 2 Loss:  0.5278\n",
            "Epoch91 | Batch: 3 Loss:  0.5022\n",
            "Epoch91 | Batch: 4 Loss:  0.4584\n",
            "Epoch91 | Batch: 5 Loss:  0.4511\n",
            "Epoch91 | Batch: 6 Loss:  0.5389\n",
            "Epoch91 | Batch: 7 Loss:  0.3743\n",
            "Epoch91 | Batch: 8 Loss:  0.3250\n",
            "Epoch91 | Batch: 9 Loss:  0.4969\n",
            "Epoch91 | Batch: 10 Loss:  0.4813\n",
            "Epoch91 | Batch: 11 Loss:  0.4915\n",
            "Epoch91 | Batch: 12 Loss:  0.4381\n",
            "Epoch91 | Batch: 13 Loss:  0.7519\n",
            "Epoch91 | Batch: 14 Loss:  0.4521\n",
            "Epoch91 | Batch: 15 Loss:  0.3077\n",
            "Epoch91 | Batch: 16 Loss:  0.3765\n",
            "Epoch91 | Batch: 17 Loss:  0.5017\n",
            "Epoch91 | Batch: 18 Loss:  0.6442\n",
            "Epoch91 | Batch: 19 Loss:  0.3933\n",
            "Epoch91 | Batch: 20 Loss:  0.4221\n",
            "Epoch91 | Batch: 21 Loss:  0.3797\n",
            "Epoch91 | Batch: 22 Loss:  0.3900\n",
            "Epoch91 | Batch: 23 Loss:  0.4953\n",
            "Epoch91 | Batch: 24 Loss:  0.3770\n",
            "Epoch92 | Batch: 1 Loss:  0.5871\n",
            "Epoch92 | Batch: 2 Loss:  0.4520\n",
            "Epoch92 | Batch: 3 Loss:  0.4571\n",
            "Epoch92 | Batch: 4 Loss:  0.4926\n",
            "Epoch92 | Batch: 5 Loss:  0.4846\n",
            "Epoch92 | Batch: 6 Loss:  0.5363\n",
            "Epoch92 | Batch: 7 Loss:  0.4575\n",
            "Epoch92 | Batch: 8 Loss:  0.3804\n",
            "Epoch92 | Batch: 9 Loss:  0.4316\n",
            "Epoch92 | Batch: 10 Loss:  0.3374\n",
            "Epoch92 | Batch: 11 Loss:  0.4893\n",
            "Epoch92 | Batch: 12 Loss:  0.4485\n",
            "Epoch92 | Batch: 13 Loss:  0.3525\n",
            "Epoch92 | Batch: 14 Loss:  0.4969\n",
            "Epoch92 | Batch: 15 Loss:  0.5290\n",
            "Epoch92 | Batch: 16 Loss:  0.4689\n",
            "Epoch92 | Batch: 17 Loss:  0.4464\n",
            "Epoch92 | Batch: 18 Loss:  0.5631\n",
            "Epoch92 | Batch: 19 Loss:  0.5771\n",
            "Epoch92 | Batch: 20 Loss:  0.3130\n",
            "Epoch92 | Batch: 21 Loss:  0.4708\n",
            "Epoch92 | Batch: 22 Loss:  0.3025\n",
            "Epoch92 | Batch: 23 Loss:  0.4483\n",
            "Epoch92 | Batch: 24 Loss:  0.4437\n",
            "Epoch93 | Batch: 1 Loss:  0.4629\n",
            "Epoch93 | Batch: 2 Loss:  0.3904\n",
            "Epoch93 | Batch: 3 Loss:  0.3916\n",
            "Epoch93 | Batch: 4 Loss:  0.5929\n",
            "Epoch93 | Batch: 5 Loss:  0.4503\n",
            "Epoch93 | Batch: 6 Loss:  0.5008\n",
            "Epoch93 | Batch: 7 Loss:  0.4450\n",
            "Epoch93 | Batch: 8 Loss:  0.2953\n",
            "Epoch93 | Batch: 9 Loss:  0.4497\n",
            "Epoch93 | Batch: 10 Loss:  0.5855\n",
            "Epoch93 | Batch: 11 Loss:  0.4863\n",
            "Epoch93 | Batch: 12 Loss:  0.4702\n",
            "Epoch93 | Batch: 13 Loss:  0.5206\n",
            "Epoch93 | Batch: 14 Loss:  0.5431\n",
            "Epoch93 | Batch: 15 Loss:  0.2710\n",
            "Epoch93 | Batch: 16 Loss:  0.3578\n",
            "Epoch93 | Batch: 17 Loss:  0.4057\n",
            "Epoch93 | Batch: 18 Loss:  0.3587\n",
            "Epoch93 | Batch: 19 Loss:  0.4301\n",
            "Epoch93 | Batch: 20 Loss:  0.4588\n",
            "Epoch93 | Batch: 21 Loss:  0.4227\n",
            "Epoch93 | Batch: 22 Loss:  0.4464\n",
            "Epoch93 | Batch: 23 Loss:  0.6360\n",
            "Epoch93 | Batch: 24 Loss:  0.5144\n",
            "Epoch94 | Batch: 1 Loss:  0.5465\n",
            "Epoch94 | Batch: 2 Loss:  0.3110\n",
            "Epoch94 | Batch: 3 Loss:  0.4176\n",
            "Epoch94 | Batch: 4 Loss:  0.4651\n",
            "Epoch94 | Batch: 5 Loss:  0.4070\n",
            "Epoch94 | Batch: 6 Loss:  0.5470\n",
            "Epoch94 | Batch: 7 Loss:  0.4218\n",
            "Epoch94 | Batch: 8 Loss:  0.5167\n",
            "Epoch94 | Batch: 9 Loss:  0.4434\n",
            "Epoch94 | Batch: 10 Loss:  0.4595\n",
            "Epoch94 | Batch: 11 Loss:  0.5511\n",
            "Epoch94 | Batch: 12 Loss:  0.5833\n",
            "Epoch94 | Batch: 13 Loss:  0.4181\n",
            "Epoch94 | Batch: 14 Loss:  0.4841\n",
            "Epoch94 | Batch: 15 Loss:  0.3684\n",
            "Epoch94 | Batch: 16 Loss:  0.4121\n",
            "Epoch94 | Batch: 17 Loss:  0.4273\n",
            "Epoch94 | Batch: 18 Loss:  0.4036\n",
            "Epoch94 | Batch: 19 Loss:  0.4101\n",
            "Epoch94 | Batch: 20 Loss:  0.4423\n",
            "Epoch94 | Batch: 21 Loss:  0.5177\n",
            "Epoch94 | Batch: 22 Loss:  0.4527\n",
            "Epoch94 | Batch: 23 Loss:  0.5761\n",
            "Epoch94 | Batch: 24 Loss:  0.3100\n",
            "Epoch95 | Batch: 1 Loss:  0.5632\n",
            "Epoch95 | Batch: 2 Loss:  0.4819\n",
            "Epoch95 | Batch: 3 Loss:  0.3636\n",
            "Epoch95 | Batch: 4 Loss:  0.3598\n",
            "Epoch95 | Batch: 5 Loss:  0.4250\n",
            "Epoch95 | Batch: 6 Loss:  0.4540\n",
            "Epoch95 | Batch: 7 Loss:  0.4263\n",
            "Epoch95 | Batch: 8 Loss:  0.4009\n",
            "Epoch95 | Batch: 9 Loss:  0.4581\n",
            "Epoch95 | Batch: 10 Loss:  0.3807\n",
            "Epoch95 | Batch: 11 Loss:  0.7599\n",
            "Epoch95 | Batch: 12 Loss:  0.4312\n",
            "Epoch95 | Batch: 13 Loss:  0.4220\n",
            "Epoch95 | Batch: 14 Loss:  0.4351\n",
            "Epoch95 | Batch: 15 Loss:  0.6120\n",
            "Epoch95 | Batch: 16 Loss:  0.5777\n",
            "Epoch95 | Batch: 17 Loss:  0.4744\n",
            "Epoch95 | Batch: 18 Loss:  0.4617\n",
            "Epoch95 | Batch: 19 Loss:  0.4159\n",
            "Epoch95 | Batch: 20 Loss:  0.3286\n",
            "Epoch95 | Batch: 21 Loss:  0.3757\n",
            "Epoch95 | Batch: 22 Loss:  0.5635\n",
            "Epoch95 | Batch: 23 Loss:  0.4218\n",
            "Epoch95 | Batch: 24 Loss:  0.4107\n",
            "Epoch96 | Batch: 1 Loss:  0.4145\n",
            "Epoch96 | Batch: 2 Loss:  0.6556\n",
            "Epoch96 | Batch: 3 Loss:  0.3672\n",
            "Epoch96 | Batch: 4 Loss:  0.3603\n",
            "Epoch96 | Batch: 5 Loss:  0.6505\n",
            "Epoch96 | Batch: 6 Loss:  0.4268\n",
            "Epoch96 | Batch: 7 Loss:  0.5645\n",
            "Epoch96 | Batch: 8 Loss:  0.5038\n",
            "Epoch96 | Batch: 9 Loss:  0.4386\n",
            "Epoch96 | Batch: 10 Loss:  0.4616\n",
            "Epoch96 | Batch: 11 Loss:  0.5745\n",
            "Epoch96 | Batch: 12 Loss:  0.4775\n",
            "Epoch96 | Batch: 13 Loss:  0.4159\n",
            "Epoch96 | Batch: 14 Loss:  0.4150\n",
            "Epoch96 | Batch: 15 Loss:  0.5822\n",
            "Epoch96 | Batch: 16 Loss:  0.3774\n",
            "Epoch96 | Batch: 17 Loss:  0.4639\n",
            "Epoch96 | Batch: 18 Loss:  0.3671\n",
            "Epoch96 | Batch: 19 Loss:  0.4417\n",
            "Epoch96 | Batch: 20 Loss:  0.2296\n",
            "Epoch96 | Batch: 21 Loss:  0.4672\n",
            "Epoch96 | Batch: 22 Loss:  0.4363\n",
            "Epoch96 | Batch: 23 Loss:  0.5563\n",
            "Epoch96 | Batch: 24 Loss:  0.4395\n",
            "Epoch97 | Batch: 1 Loss:  0.3282\n",
            "Epoch97 | Batch: 2 Loss:  0.5841\n",
            "Epoch97 | Batch: 3 Loss:  0.3417\n",
            "Epoch97 | Batch: 4 Loss:  0.3192\n",
            "Epoch97 | Batch: 5 Loss:  0.3768\n",
            "Epoch97 | Batch: 6 Loss:  0.3527\n",
            "Epoch97 | Batch: 7 Loss:  0.5909\n",
            "Epoch97 | Batch: 8 Loss:  0.4430\n",
            "Epoch97 | Batch: 9 Loss:  0.4758\n",
            "Epoch97 | Batch: 10 Loss:  0.5242\n",
            "Epoch97 | Batch: 11 Loss:  0.4154\n",
            "Epoch97 | Batch: 12 Loss:  0.4383\n",
            "Epoch97 | Batch: 13 Loss:  0.6314\n",
            "Epoch97 | Batch: 14 Loss:  0.4258\n",
            "Epoch97 | Batch: 15 Loss:  0.3671\n",
            "Epoch97 | Batch: 16 Loss:  0.5442\n",
            "Epoch97 | Batch: 17 Loss:  0.6173\n",
            "Epoch97 | Batch: 18 Loss:  0.4452\n",
            "Epoch97 | Batch: 19 Loss:  0.4415\n",
            "Epoch97 | Batch: 20 Loss:  0.3502\n",
            "Epoch97 | Batch: 21 Loss:  0.3793\n",
            "Epoch97 | Batch: 22 Loss:  0.5001\n",
            "Epoch97 | Batch: 23 Loss:  0.5786\n",
            "Epoch97 | Batch: 24 Loss:  0.4794\n",
            "Epoch98 | Batch: 1 Loss:  0.4403\n",
            "Epoch98 | Batch: 2 Loss:  0.4306\n",
            "Epoch98 | Batch: 3 Loss:  0.5029\n",
            "Epoch98 | Batch: 4 Loss:  0.5934\n",
            "Epoch98 | Batch: 5 Loss:  0.5922\n",
            "Epoch98 | Batch: 6 Loss:  0.6280\n",
            "Epoch98 | Batch: 7 Loss:  0.3637\n",
            "Epoch98 | Batch: 8 Loss:  0.4223\n",
            "Epoch98 | Batch: 9 Loss:  0.4617\n",
            "Epoch98 | Batch: 10 Loss:  0.5375\n",
            "Epoch98 | Batch: 11 Loss:  0.3887\n",
            "Epoch98 | Batch: 12 Loss:  0.4849\n",
            "Epoch98 | Batch: 13 Loss:  0.4865\n",
            "Epoch98 | Batch: 14 Loss:  0.3581\n",
            "Epoch98 | Batch: 15 Loss:  0.2995\n",
            "Epoch98 | Batch: 16 Loss:  0.4360\n",
            "Epoch98 | Batch: 17 Loss:  0.5520\n",
            "Epoch98 | Batch: 18 Loss:  0.4519\n",
            "Epoch98 | Batch: 19 Loss:  0.3464\n",
            "Epoch98 | Batch: 20 Loss:  0.5964\n",
            "Epoch98 | Batch: 21 Loss:  0.5250\n",
            "Epoch98 | Batch: 22 Loss:  0.3852\n",
            "Epoch98 | Batch: 23 Loss:  0.3551\n",
            "Epoch98 | Batch: 24 Loss:  0.2975\n",
            "Epoch99 | Batch: 1 Loss:  0.3829\n",
            "Epoch99 | Batch: 2 Loss:  0.3765\n",
            "Epoch99 | Batch: 3 Loss:  0.4238\n",
            "Epoch99 | Batch: 4 Loss:  0.4796\n",
            "Epoch99 | Batch: 5 Loss:  0.6521\n",
            "Epoch99 | Batch: 6 Loss:  0.4133\n",
            "Epoch99 | Batch: 7 Loss:  0.4214\n",
            "Epoch99 | Batch: 8 Loss:  0.4736\n",
            "Epoch99 | Batch: 9 Loss:  0.5139\n",
            "Epoch99 | Batch: 10 Loss:  0.3548\n",
            "Epoch99 | Batch: 11 Loss:  0.5226\n",
            "Epoch99 | Batch: 12 Loss:  0.4958\n",
            "Epoch99 | Batch: 13 Loss:  0.3668\n",
            "Epoch99 | Batch: 14 Loss:  0.4373\n",
            "Epoch99 | Batch: 15 Loss:  0.5513\n",
            "Epoch99 | Batch: 16 Loss:  0.4860\n",
            "Epoch99 | Batch: 17 Loss:  0.3759\n",
            "Epoch99 | Batch: 18 Loss:  0.4528\n",
            "Epoch99 | Batch: 19 Loss:  0.4591\n",
            "Epoch99 | Batch: 20 Loss:  0.3731\n",
            "Epoch99 | Batch: 21 Loss:  0.5451\n",
            "Epoch99 | Batch: 22 Loss:  0.3533\n",
            "Epoch99 | Batch: 23 Loss:  0.5066\n",
            "Epoch99 | Batch: 24 Loss:  0.5670\n",
            "Epoch100 | Batch: 1 Loss:  0.4973\n",
            "Epoch100 | Batch: 2 Loss:  0.3733\n",
            "Epoch100 | Batch: 3 Loss:  0.3936\n",
            "Epoch100 | Batch: 4 Loss:  0.4124\n",
            "Epoch100 | Batch: 5 Loss:  0.4737\n",
            "Epoch100 | Batch: 6 Loss:  0.4716\n",
            "Epoch100 | Batch: 7 Loss:  0.4133\n",
            "Epoch100 | Batch: 8 Loss:  0.4847\n",
            "Epoch100 | Batch: 9 Loss:  0.4527\n",
            "Epoch100 | Batch: 10 Loss:  0.4681\n",
            "Epoch100 | Batch: 11 Loss:  0.5519\n",
            "Epoch100 | Batch: 12 Loss:  0.5302\n",
            "Epoch100 | Batch: 13 Loss:  0.5492\n",
            "Epoch100 | Batch: 14 Loss:  0.4648\n",
            "Epoch100 | Batch: 15 Loss:  0.4379\n",
            "Epoch100 | Batch: 16 Loss:  0.4465\n",
            "Epoch100 | Batch: 17 Loss:  0.4673\n",
            "Epoch100 | Batch: 18 Loss:  0.6434\n",
            "Epoch100 | Batch: 19 Loss:  0.3320\n",
            "Epoch100 | Batch: 20 Loss:  0.3882\n",
            "Epoch100 | Batch: 21 Loss:  0.4121\n",
            "Epoch100 | Batch: 22 Loss:  0.4226\n",
            "Epoch100 | Batch: 23 Loss:  0.4489\n",
            "Epoch100 | Batch: 24 Loss:  0.3919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l-XUsvthdo7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYF04F9Luc-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a6f1b2-3ba8-4153-de95-22694e9d376c"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,22)\n",
        "    self.l2=nn.Linear(22,20)\n",
        "    self.l3=nn.Linear(20,18)    \n",
        "    self.l4=nn.Linear(18,16)\n",
        "    self.l5=nn.Linear(16,14)\n",
        "    self.l6=nn.Linear(14,12)\n",
        "    self.l7=nn.Linear(12,10)\n",
        "    self.l8=nn.Linear(10,8)\n",
        "    self.l9=nn.Linear(8,6)\n",
        "    self.l10=nn.Linear(6,4)\n",
        "    self.l11=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    out1=self.sigmoid(self.l1(x))\n",
        "    out2=self.sigmoid(self.l2(out1))\n",
        "    out3=self.sigmoid(self.l3(out2))\n",
        "    out4=self.sigmoid(self.l4(out3))\n",
        "    out5=self.sigmoid(self.l5(out4))\n",
        "    out6=self.sigmoid(self.l6(out5))\n",
        "    out7=self.sigmoid(self.l7(out6))\n",
        "    out8=self.sigmoid(self.l8(out7))\n",
        "    out9=self.sigmoid(self.l9(out8))\n",
        "    out10=self.sigmoid(self.l10(out9))\n",
        "    y_pred=self.sigmoid(self.l11(out10))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
            "Loss:  0.6453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAdUtDRH38ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185bf7cd-a693-4871-c88d-db551b68e790"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
            "Loss:  0.6388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScLzAVoN_fJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca81ac0-dc00-443b-b864-ec84ae3abf60"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,4)\n",
        "    self.l2=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    y_pred=self.sigmoid(self.l2(out1))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
            "Loss:  0.5712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCFrShgJFWHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68d265f-9cbb-4448-fc34-ec39b20a577e"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/email.csv', delimiter=',',dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(3000,2000)\n",
        "    self.l2=nn.Linear(2000,1000)\n",
        "    self.l3=nn.Linear(1000,100)\n",
        "    self.l4=nn.Linear(100,10)\n",
        "    self.l5=nn.Linear(10,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    out4=self.relu(self.l4(out3))\n",
        "    y_pred=self.sigmoid(self.l5(out4))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "X's shape: torch.Size([5172, 3000]) | Y's shape: torch.Size([5172, 1])\n",
            "Loss:  0.5928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN9p9m--FhgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "5baf69c0-a414-46dc-cc2c-4bc1d6121f9e"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "def lable(labels):\n",
        "  target=[]\n",
        "  target_lable=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']\n",
        "  for i in labels:\n",
        "    target.append(target_lable.index(i))\n",
        "  return target\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    with open(\"/content/gdrive/MyDrive/Colab Notebooks/train.csv\") as f:\n",
        "      n_cols=len(f.readline().split(\",\"))\n",
        "    x=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(1,n_cols-1), delimiter=',',skiprows=1)\n",
        "    self.x_data=from_numpy(x).float()\n",
        "    labels=\n",
        "    self.y_data=label(labels)\n",
        "    self.len=x.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(93,64)\n",
        "    self.l2=nn.Linear(64,32)\n",
        "    self.l3=nn.Linear(32,16)\n",
        "    self.l4=nn.Linear(16,9)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    y_pred=self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    print(f'Epoch{epoch+1} | Batch: {i+1} Loss: {loss.item(): .4f}')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-825f129ef3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch{epoch+1} | Batch: {i+1} Loss: {loss.item(): .4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2L2dPuhepZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "caba5cd5-d97d-4130-c8e9-93f9a640605b"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "with open(\"/content/gdrive/MyDrive/Colab Notebooks/train.csv\") as f:\n",
        "  n_cols=len(f.readline().split(\",\"))\n",
        "x=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(1,n_cols-1), delimiter=',',skiprows=1)\n",
        "y=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(n_cols-1,n_cols), delimiter=',',skiprows=1,dtype='str')\n",
        "x_data=from_numpy(x)\n",
        "y=list(y)\n",
        "\n",
        "print(y)\n",
        "#print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0462304e3b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mn_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Colab Notebooks/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Colab Notebooks/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipvkYHCZj6dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7642b8e4-ad03-46d6-d5d8-b4af982e6a19"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.init\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./mnist_data/', train=True, download=True, transform=transforms.Compose([\n",
        "        transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Net, self).__init__()\n",
        "    self.l1 = nn.Linear(784, 520)\n",
        "    self.l2 = nn.Linear(520, 320)\n",
        "    self.l3 = nn.Linear(320, 240)\n",
        "    self.l4 = nn.Linear(240, 120)\n",
        "    self.l5 = nn.Linear(120, 10)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
        "    x = self.relu(self.l1(x))\n",
        "    x = self.relu(self.l2(x))\n",
        "    x = self.relu(self.l3(x))\n",
        "    x = self.relu(self.l4(x))\n",
        "    return self.l5(x)\n",
        "\n",
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max log-probability\n",
        "        pred =output.data.max(1,keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "         f' ({100.*correct/len(test_loader.dataset):.0f}%)')\n",
        "if __name__=='__main__':\n",
        "  for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303456\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302423\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.295739\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.301792\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.291770\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.300852\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.294379\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.294065\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.281256\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.287761\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.286816\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.285154\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.270629\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.283566\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.271409\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.266114\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.265628\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.244603\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.234406\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.239965\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.225309\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.218625\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.197205\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.199981\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.163618\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.149776\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.142469\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.102084\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.039642\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.984645\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.010915\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.905704\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.681026\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.698208\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.411324\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.493589\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.253334\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.148542\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.248569\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.050548\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.971216\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.863830\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.838515\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.796467\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.760183\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.662526\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.852456\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.796357\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.920398\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.558369\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.407191\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.456331\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.417530\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.611288\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.673182\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.487030\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.302001\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.425404\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.729617\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.559313\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.415251\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.514217\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.561907\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.470439\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.332924\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.367925\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.489550\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.348323\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.376712\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.310440\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.307957\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.350843\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.326543\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.289993\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.470745\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.372963\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.398555\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.406500\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.402683\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.403162\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.362928\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.509490\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.417728\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.265499\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.446168\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.273761\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.255868\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.615451\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.380602\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.339844\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.243656\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.378152\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.438902\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.476977\n",
            "\n",
            "Test set: Average loss: 0.0050, Accuracy: 9084/10000 (91%)\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.304527\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.097093\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.402550\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.234432\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.213953\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.964631\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.287860\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.389380\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.213889\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.167311\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.206791\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.299141\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.146351\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.205223\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.381806\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.374803\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.334454\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.376520\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.552239\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.337860\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.166602\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.213581\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.224107\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.431115\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.284722\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.120817\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.276795\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.237534\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.326570\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.174818\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.294474\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.181553\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.285253\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.234137\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.194642\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.254991\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.152369\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.332782\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.191536\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.223657\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.273500\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.506975\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.273819\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.298154\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.175148\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.209498\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.213293\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.221355\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.465844\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.299450\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.375017\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.291762\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.193166\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.145871\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.226463\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.159934\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.154391\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.081904\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.160119\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.242429\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.201705\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.254762\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.286191\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.106258\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.105722\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.087221\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.255995\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.293221\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.398469\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.312522\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.084540\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.183111\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.235990\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.280539\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.139174\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.210418\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.186521\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.292199\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.187770\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.101802\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.309235\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.113805\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.140587\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.221993\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.203095\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.250096\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.180014\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.293895\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.105069\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.230972\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.118415\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.157029\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.148189\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.312486\n",
            "\n",
            "Test set: Average loss: 0.0034, Accuracy: 9356/10000 (94%)\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.249343\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.194011\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.103308\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.148697\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.199677\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.178659\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.102675\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.145096\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.191335\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.261734\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.283762\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.093416\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.180149\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.286483\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.164017\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.123414\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.229740\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.135996\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.250005\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.219421\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.346892\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.262800\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.066176\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.100307\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.100414\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.306553\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.135618\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.051982\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.078779\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.203865\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.276516\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.180119\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.306713\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.109502\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.194568\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.265643\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.101672\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.066360\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.204154\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.236140\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.090200\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.162867\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.095449\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.342328\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.280524\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.158125\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.229773\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.243609\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.086028\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.321065\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.122967\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.097131\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.082531\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.231538\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.181236\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.137744\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.227675\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.205738\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.109418\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.161568\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.240276\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.235859\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.153415\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.163922\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.183857\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.045130\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.064683\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.283920\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.232185\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.263323\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.228576\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.147055\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.183977\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.099194\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.140241\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.267830\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.160630\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.180402\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.191040\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.276430\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.153691\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.142023\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.100361\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.200114\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.126140\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.130931\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.160960\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.091466\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.054069\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.202875\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.135829\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.173614\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.154292\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.142962\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 9564/10000 (96%)\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.097837\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.110872\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.118527\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.185170\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.070729\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.070344\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.169971\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.162913\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.102215\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.327476\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.113372\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.086872\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.053942\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.171996\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.090978\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.185257\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.064285\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.200582\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.123153\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.084333\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.212628\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.110209\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.197609\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.068398\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.120469\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.153845\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.077383\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.182752\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.146101\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.061690\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.258637\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.238260\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.049181\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.065628\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.100211\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.150601\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.152818\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.077987\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.136970\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.141104\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.090323\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.178934\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.310222\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.235250\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.100006\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.178631\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.208918\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.050609\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.145331\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.149682\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.140247\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.113718\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.215154\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.141465\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.186987\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.103829\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.184448\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.190928\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.184592\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.160560\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.176717\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.093744\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.072666\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.184167\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.046182\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.156867\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.079450\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.096904\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.107998\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.079530\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.022152\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.118715\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.087479\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.120566\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.188772\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.298990\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.050715\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.074268\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.030088\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.063241\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.184577\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.171080\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.089413\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.069154\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.099332\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.179969\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.099737\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.106958\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.182901\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.021388\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.085170\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.188565\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.074506\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.021041\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 9666/10000 (97%)\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.014610\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.017141\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.104187\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.077609\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.061537\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.098198\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.080568\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.064171\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.062952\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.093730\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.153193\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.054949\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.146045\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.095204\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.053542\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.108934\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.111371\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.059404\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.100722\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.073879\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.146108\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.226014\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.131823\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.036041\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.089512\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.114903\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.197988\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.033474\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.072246\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.169358\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.210638\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.051282\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.044609\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.043194\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.065867\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.062184\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.060214\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.075512\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.042381\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.032925\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.111580\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.054435\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.141095\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.056651\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.078152\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.016468\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.029701\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.019274\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.059356\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.088423\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.081829\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.155979\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.071199\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.054732\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.047912\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.039259\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.071429\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.024590\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.192427\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.130646\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.044824\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.132080\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.167351\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.127394\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.086746\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.034287\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.042760\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.057277\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.051917\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.051772\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.028415\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.079644\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.179589\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.141476\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.097754\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.080517\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.067173\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.092007\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.075572\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.026481\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.022859\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.046927\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.067370\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.036187\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.067795\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.040703\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.123047\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.059313\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.138782\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.078126\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.355132\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.099164\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.059116\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.057894\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 9658/10000 (97%)\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.064775\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.032150\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.062984\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.280623\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.052625\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.062940\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.087095\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.091536\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.083213\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.091238\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.288817\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.210212\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.042752\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.011118\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.034337\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.033890\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.056667\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.072397\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.050405\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.046806\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.138433\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.029634\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.078532\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.029609\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.045577\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.101709\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.124965\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.121199\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.162124\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.069406\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.126173\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.018360\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.093423\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.084032\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.127759\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.061471\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.096519\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.147232\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.027302\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.033207\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.127207\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.134384\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.107920\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.037706\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.120364\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.051817\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.046751\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.037140\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.044877\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.076181\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.131319\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.036686\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.123351\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.145735\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.126169\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.113096\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.048149\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.035244\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.128133\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.088109\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.077000\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.037200\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.022831\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.104584\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.108182\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.085583\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.036577\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.039394\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.055457\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.071385\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.072660\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.181430\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.044524\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.038330\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.073301\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.086712\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.048560\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.108922\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.035384\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.025589\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.055149\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.102350\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.054625\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.069044\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.044659\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.052432\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.125785\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.015127\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.016157\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.028250\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.103431\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.102485\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.087455\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.042808\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 9719/10000 (97%)\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.083953\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.037470\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.096734\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.050940\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.069316\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.076028\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.016251\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.107988\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.030924\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.061026\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.018054\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.022939\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.208934\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.058322\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.085446\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.131821\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.056817\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.048506\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.117791\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.037146\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.041960\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.043464\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.173080\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.025026\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.032699\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.059588\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.047469\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.005727\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.286869\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.034153\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.065730\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.108241\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.411977\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.056709\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.015545\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.016514\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.026421\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.010550\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.012769\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.076190\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.058634\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.041865\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.056339\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.037681\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.102037\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.149610\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.066247\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.040253\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.146304\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.063924\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.090398\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.055975\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.037272\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.042893\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.177098\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.031820\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.068095\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.026962\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.025181\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.296209\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.048633\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.029920\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.106642\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.019113\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.028375\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.131841\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.021664\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.021337\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.033163\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.023574\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.096314\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.026952\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.046883\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.009189\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.007451\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.016675\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.062638\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.025176\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.116136\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.095618\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.146228\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.055382\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.105617\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.013048\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.017925\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.067081\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.037506\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.035332\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.093562\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.014614\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.114205\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.023609\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.036681\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.097888\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 9728/10000 (97%)\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.151357\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.077713\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.010024\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.099010\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.013476\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.044072\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.034449\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.062040\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.023264\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.029003\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.055050\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.111315\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.030695\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.012810\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.104177\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.035094\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.037778\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.013139\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.013075\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.021608\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.007033\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.092843\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.142845\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.049589\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.029371\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.017627\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.014674\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.148256\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.049378\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.024603\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.025203\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.068109\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.037628\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.139911\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.016985\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.106340\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.006734\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.023049\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.097329\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.010646\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.044047\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.016394\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.075276\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.039085\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.013873\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.063686\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.013829\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.013537\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.091544\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.056784\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.033362\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.070461\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.040326\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.141587\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.004242\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.027098\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.054189\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.052755\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.016989\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.092042\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.024750\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.109142\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.089976\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.027985\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.052723\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.066376\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.006698\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.039081\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.007852\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.111773\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.057945\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.055021\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.017445\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.029470\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.013445\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.231700\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.063152\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.046133\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.037295\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.033010\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.079498\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.080498\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.139482\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.039621\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.031875\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.132669\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.029143\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.016111\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.170892\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.069669\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.022503\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.031269\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.031017\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.017331\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 9731/10000 (97%)\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.031112\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.064780\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.034637\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.019505\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.060386\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.043432\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.074921\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.013347\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.012764\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.083466\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.010240\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.019282\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.032653\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.044416\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.042894\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.018660\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.045332\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.027739\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.017494\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.033813\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.020775\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.164779\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.035925\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.014765\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.013652\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.007970\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.121631\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.024184\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.031845\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.015684\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.026286\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.016182\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.043747\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.052923\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.087099\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.010592\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.037245\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.026979\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.024386\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.070557\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.053918\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.065819\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.026691\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.020060\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.027212\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.040720\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.023794\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.021271\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.018009\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.027726\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.021053\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.038706\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.074830\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.012197\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.025401\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.029875\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.010971\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.023789\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.042516\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.006264\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.055602\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.055657\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.008647\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.048266\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.146752\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.036646\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.025985\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.044595\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.155951\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.129517\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.030498\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.009160\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.023476\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.057295\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.005157\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.020654\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.014933\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.086476\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.018058\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.046199\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.015982\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.009669\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.030429\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.052564\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.063186\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.074872\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.033975\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.007745\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.031321\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.009453\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.066563\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.046521\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.042738\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.079154\n",
            "\n",
            "Test set: Average loss: 0.0012, Accuracy: 9766/10000 (98%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWkQF1sT2Lvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e0ade6-f058-446d-adfb-7a9090871235"
      },
      "source": [
        "A=['1','2','3','4','5']\n",
        "A=np.array(A)\n",
        "print(A[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-xw83bJpaRs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "ee36f641-83be-46b4-95e4-1a33bc3d54df"
      },
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "def lables2id(lables):\n",
        "    target_id = []\n",
        "    target_lables = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\n",
        "    for lable in lables:\n",
        "        target_id.append(target_lables.index(lable))\n",
        "    return target_id\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        data = pd.read_csv(filepath)\n",
        "        lables = data['target']\n",
        "        self.len = data.shape[0]\n",
        "        self.data_x = torch.tensor(np.array(data)[:, 1:-1].astype(float))\n",
        "        self.data_y = lables2id(lables)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data_x[index], self.data_y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset = MyDataset('/content/gdrive/MyDrive/Colab Notebooks/train.csv')\n",
        "train_loader = DataLoader(dataset=dataset, shuffle=True, batch_size=64, num_workers=0)\n",
        "\n",
        "\n",
        "# \n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(93, 64)\n",
        "        self.l2 = torch.nn.Linear(64, 32)\n",
        "        self.l3 = torch.nn.Linear(32, 16)\n",
        "        self.l4 = torch.nn.Linear(16, 9)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.l1(x))\n",
        "        x = self.relu(self.l2(x))\n",
        "        x = self.relu(self.l3(x))\n",
        "        return self.l4(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.relu(self.l1(x))\n",
        "            x = self.relu(self.l2(x))\n",
        "            x = self.relu(self.l3(x))\n",
        "            x = self.relu(self.l4(x))\n",
        "            # \n",
        "            _, predicted = torch.max(x, dim=1)\n",
        "            # one-hotcsv\n",
        "            y = pd.get_dummies(predicted)\n",
        "            return y\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.5, lr=0.01)\n",
        "\n",
        "model=Net()\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for batch_idx, data in enumerate(train_loader, 0):\n",
        "    inputs,labels=data\n",
        "    inputs = inputs.float()\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Epoch{epoch+1} | Batch: {batch_idx+1} Loss: {loss.item(): .4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch100 | Batch: 967 Loss:  0.4015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    for epoch in range(10):\\n        train(epoch)\\nprint(f\\'Loss: {loss.item(): .4f}\\')\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcBpTmdXyPKs"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    with open(\"/content/gdrive/MyDrive/Colab Notebooks/train.csv\") as f:\n",
        "      n_cols=len(f.readline().split(\",\"))\n",
        "    x=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(1,n_cols-1), delimiter=',',skiprows=1)\n",
        "    y=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(n_cols-1,n_cols), delimiter=',',skiprows=1,dtype='str')\n",
        "    self.x_data=from_numpy(x).float()\n",
        "    self.y_data=y\n",
        "    self.len=x.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(93,64)\n",
        "    self.l2=nn.Linear(64,32)\n",
        "    self.l3=nn.Linear(32,16)\n",
        "    self.l4=nn.Linear(16,9)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    y_pred=self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv2ods2yPfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499c37b5-3310-400b-f218-7b24199e6ed0"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    with open(\"/content/gdrive/MyDrive/Colab Notebooks/train.csv\") as f:\n",
        "      n_cols=len(f.readline().split(\",\"))\n",
        "    x=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(1,n_cols-1), delimiter=',',skiprows=1)\n",
        "    y=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(n_cols-1,n_cols), delimiter=',',skiprows=1,dtype='str')\n",
        "    self.x_data=from_numpy(x).float()\n",
        "    self.y_data=y\n",
        "    self.len=x.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(93,64)\n",
        "    self.l2=nn.Linear(64,32)\n",
        "    self.l3=nn.Linear(32,16)\n",
        "    self.l4=nn.Linear(16,9)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    y_pred=self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Loss:  0.6477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5jOA7hzs6ft"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    with open(\"/content/gdrive/MyDrive/Colab Notebooks/train.csv\") as f:\n",
        "      n_cols=len(f.readline().split(\",\"))\n",
        "    x=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(1,n_cols-1), delimiter=',',skiprows=1)\n",
        "    y=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/train.csv',usecols=range(n_cols-1,n_cols), delimiter=',',skiprows=1,dtype='str')\n",
        "    self.x_data=from_numpy(x).float()\n",
        "    self.y_data=y\n",
        "    self.len=x.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(93,64)\n",
        "    self.l2=nn.Linear(64,32)\n",
        "    self.l3=nn.Linear(32,16)\n",
        "    self.l4=nn.Linear(16,9)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    y_pred=self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZmGDvdESV6S",
        "outputId": "eca6661b-2f71-4c97-9681-a5a97b8244d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "xy=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/titanic dataset.csv')\n",
        "xy.head()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCJesWKGSccY",
        "outputId": "d466d88f-c4c6-4195-8c44-1b84868a2e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "xy=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/titanic dataset.csv')\n",
        "xy=xy.drop(['PassengerId','Name','Ticket','Cabin','Embarked'],axis=1)\n",
        "xy[\"Age\"].fillna(xy.groupby(\"Sex\")[\"Age\"].transform(\"mean\"), inplace=True)\n",
        "smap = {\"male\": 0, \"female\": 1}\n",
        "xy['Sex'] = xy['Sex'].map(smap)\n",
        "x = xy.drop(['Survived'], axis=1)\n",
        "y = xy['Survived']\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "0      0\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "886    0\n",
            "887    1\n",
            "888    0\n",
            "889    1\n",
            "890    0\n",
            "Name: Survived, Length: 891, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LYxMUXVXYO2",
        "outputId": "bec7db5b-8d63-463a-d353-103133a6d4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TitanicDataset(Dataset):\n",
        "    def __init__(self, path, drop_features, train=True):\n",
        "        self.data = pd.read_csv(path)\n",
        "        self.data['Sex'] = self.data['Sex'].map({'male':0, 'female':1})\n",
        "        self.data['Embarked'] = self.data['Embarked'].map({'S':0, 'C':1, 'Q':2})\n",
        "       \n",
        "        self.data = self.data.drop(drop_features, axis=1)\n",
        "        \n",
        "        self.X = self.data\n",
        "        self.y = self.data.pop('Survived')\n",
        "        \n",
        "        self.features = list(self.data.columns)\n",
        "        self.classes = ['Survived', 'Dead'] ## dead \n",
        "        self.train = train\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        len_dataset = len(self.data)\n",
        "        return len_dataset\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X.loc[idx]\n",
        "        if self.train:\n",
        "          y = self.y.loc[idx]\n",
        "        # print(\"getitefm:: \",X, y)\n",
        "\n",
        "        return torch.tensor(X), torch.tensor(y)\n",
        "dataset_train_titanic = TitanicDataset('/content/gdrive/MyDrive/Colab Notebooks/titanic dataset.csv', \n",
        "                                       drop_features=['PassengerId', 'Name', 'Ticket', 'Cabin'],\n",
        "                                       train=True)\n",
        "dataloader_train_titanic = DataLoader(dataset=dataset_train_titanic,\n",
        "                                      batch_size=8,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=4,\n",
        "                                      )\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,64)\n",
        "    self.l2=nn.Linear(64,32)\n",
        "    self.l3=nn.Linear(32,16)\n",
        "    self.l4=nn.Linear(16,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    out3=self.relu(self.l3(out2))\n",
        "    y_pred=self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(dataloader_train_titanic):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Loss: {loss.item(): .4f}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-900413e4cc8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train_titanic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-21-900413e4cc8f>\", line 33, in __getitem__\n    return torch.tensor(X), torch.tensor(y)\nNameError: name 'torch' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UsdjJj7a7r3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbFfzNmwZ755"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}