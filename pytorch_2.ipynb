{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch#2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYYe7MBUnZW7eq9gT7+q86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/pytorch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF_0Xs6bUN92",
        "outputId": "ce3c47ee-22b7-41de-b607-3c0783f89100"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x_data=[1.0,2.0,3.0]\n",
        "y_data=[2.0,4.0,6.0]\n",
        "\n",
        "w=torch.tensor([1.0],requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "def loss(x,y):\n",
        "  y_pred=forward(x)\n",
        "  return (y_pred-y)**2\n",
        "\n",
        "for epoch in range(10):\n",
        "  for x_val,y_val in zip(x_data,y_data):\n",
        "    y_pred=forward(x_val)\n",
        "    l=loss(x_val,y_val)\n",
        "    l.backward() # Back propagation to update weights\n",
        "    print(\"\\tgrad: \",x_val,y_val,w.grad.item())\n",
        "    w.data=w.data-0.01*w.grad.item()\n",
        "    w.grad.data.zero_()\n",
        "\n",
        "  print(f\"Epoch: {epoch} l Loss: {l.item()}\")\n",
        "print(\"Prediction (after training\", 4, forward(2).item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 l Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 l Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 l Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 l Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 l Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 l Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 l Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 l Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 l Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 l Loss: 0.03185431286692619\n",
            "Prediction (after training 4 3.9024322032928467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB9w04J5Y80x",
        "outputId": "a3bf8bad-314f-413b-98fe-06aaadc86c2c"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "    #(1,1) x->y( y= wx)\n",
        "    #(2,1) (x1,x2)->y : y=x1*x1 + x2*w2\n",
        "    #(2, 2) (x1,x2)->(y1, y2)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "  \n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr= 0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() # w=w-0.01*w.grad().item()\n",
        "\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 47.762786865234375\n",
            "Epoch: 1 | Loss: 21.411907196044922\n",
            "Epoch: 2 | Loss: 9.679096221923828\n",
            "Epoch: 3 | Loss: 4.453868865966797\n",
            "Epoch: 4 | Loss: 2.1256611347198486\n",
            "Epoch: 5 | Loss: 1.087154746055603\n",
            "Epoch: 6 | Loss: 0.6228173971176147\n",
            "Epoch: 7 | Loss: 0.4141119122505188\n",
            "Epoch: 8 | Loss: 0.31923481822013855\n",
            "Epoch: 9 | Loss: 0.27506023645401\n",
            "Epoch: 10 | Loss: 0.25348442792892456\n",
            "Epoch: 11 | Loss: 0.24199619889259338\n",
            "Epoch: 12 | Loss: 0.2350255250930786\n",
            "Epoch: 13 | Loss: 0.2300931215286255\n",
            "Epoch: 14 | Loss: 0.22609442472457886\n",
            "Epoch: 15 | Loss: 0.22253699600696564\n",
            "Epoch: 16 | Loss: 0.2192014902830124\n",
            "Epoch: 17 | Loss: 0.21599040925502777\n",
            "Epoch: 18 | Loss: 0.21285894513130188\n",
            "Epoch: 19 | Loss: 0.20978763699531555\n",
            "Epoch: 20 | Loss: 0.20676717162132263\n",
            "Epoch: 21 | Loss: 0.20379340648651123\n",
            "Epoch: 22 | Loss: 0.20086337625980377\n",
            "Epoch: 23 | Loss: 0.19797612726688385\n",
            "Epoch: 24 | Loss: 0.19513073563575745\n",
            "Epoch: 25 | Loss: 0.1923263818025589\n",
            "Epoch: 26 | Loss: 0.18956226110458374\n",
            "Epoch: 27 | Loss: 0.18683801591396332\n",
            "Epoch: 28 | Loss: 0.18415279686450958\n",
            "Epoch: 29 | Loss: 0.18150615692138672\n",
            "Epoch: 30 | Loss: 0.17889750003814697\n",
            "Epoch: 31 | Loss: 0.17632661759853363\n",
            "Epoch: 32 | Loss: 0.17379246652126312\n",
            "Epoch: 33 | Loss: 0.17129503190517426\n",
            "Epoch: 34 | Loss: 0.168832927942276\n",
            "Epoch: 35 | Loss: 0.16640663146972656\n",
            "Epoch: 36 | Loss: 0.16401509940624237\n",
            "Epoch: 37 | Loss: 0.16165798902511597\n",
            "Epoch: 38 | Loss: 0.15933459997177124\n",
            "Epoch: 39 | Loss: 0.15704472362995148\n",
            "Epoch: 40 | Loss: 0.15478774905204773\n",
            "Epoch: 41 | Loss: 0.15256330370903015\n",
            "Epoch: 42 | Loss: 0.15037088096141815\n",
            "Epoch: 43 | Loss: 0.14820978045463562\n",
            "Epoch: 44 | Loss: 0.14607977867126465\n",
            "Epoch: 45 | Loss: 0.14398008584976196\n",
            "Epoch: 46 | Loss: 0.14191091060638428\n",
            "Epoch: 47 | Loss: 0.1398714780807495\n",
            "Epoch: 48 | Loss: 0.13786117732524872\n",
            "Epoch: 49 | Loss: 0.13588014245033264\n",
            "Epoch: 50 | Loss: 0.13392730057239532\n",
            "Epoch: 51 | Loss: 0.13200247287750244\n",
            "Epoch: 52 | Loss: 0.13010552525520325\n",
            "Epoch: 53 | Loss: 0.12823565304279327\n",
            "Epoch: 54 | Loss: 0.12639246881008148\n",
            "Epoch: 55 | Loss: 0.12457624077796936\n",
            "Epoch: 56 | Loss: 0.12278599292039871\n",
            "Epoch: 57 | Loss: 0.1210210770368576\n",
            "Epoch: 58 | Loss: 0.1192818358540535\n",
            "Epoch: 59 | Loss: 0.11756773293018341\n",
            "Epoch: 60 | Loss: 0.11587798595428467\n",
            "Epoch: 61 | Loss: 0.11421268433332443\n",
            "Epoch: 62 | Loss: 0.11257124692201614\n",
            "Epoch: 63 | Loss: 0.11095338314771652\n",
            "Epoch: 64 | Loss: 0.10935887694358826\n",
            "Epoch: 65 | Loss: 0.10778715461492538\n",
            "Epoch: 66 | Loss: 0.10623820126056671\n",
            "Epoch: 67 | Loss: 0.10471124947071075\n",
            "Epoch: 68 | Loss: 0.10320651531219482\n",
            "Epoch: 69 | Loss: 0.1017233356833458\n",
            "Epoch: 70 | Loss: 0.10026121884584427\n",
            "Epoch: 71 | Loss: 0.09882029891014099\n",
            "Epoch: 72 | Loss: 0.0974000096321106\n",
            "Epoch: 73 | Loss: 0.09600038081407547\n",
            "Epoch: 74 | Loss: 0.09462064504623413\n",
            "Epoch: 75 | Loss: 0.09326093643903732\n",
            "Epoch: 76 | Loss: 0.09192057698965073\n",
            "Epoch: 77 | Loss: 0.09059961885213852\n",
            "Epoch: 78 | Loss: 0.08929740637540817\n",
            "Epoch: 79 | Loss: 0.08801419287919998\n",
            "Epoch: 80 | Loss: 0.08674920350313187\n",
            "Epoch: 81 | Loss: 0.08550233393907547\n",
            "Epoch: 82 | Loss: 0.08427365869283676\n",
            "Epoch: 83 | Loss: 0.08306248486042023\n",
            "Epoch: 84 | Loss: 0.08186877518892288\n",
            "Epoch: 85 | Loss: 0.08069221675395966\n",
            "Epoch: 86 | Loss: 0.07953251898288727\n",
            "Epoch: 87 | Loss: 0.07838963717222214\n",
            "Epoch: 88 | Loss: 0.07726296037435532\n",
            "Epoch: 89 | Loss: 0.07615260034799576\n",
            "Epoch: 90 | Loss: 0.07505811750888824\n",
            "Epoch: 91 | Loss: 0.07397949695587158\n",
            "Epoch: 92 | Loss: 0.07291627675294876\n",
            "Epoch: 93 | Loss: 0.07186824083328247\n",
            "Epoch: 94 | Loss: 0.0708354040980339\n",
            "Epoch: 95 | Loss: 0.06981730461120605\n",
            "Epoch: 96 | Loss: 0.0688139796257019\n",
            "Epoch: 97 | Loss: 0.06782509386539459\n",
            "Epoch: 98 | Loss: 0.06685040891170502\n",
            "Epoch: 99 | Loss: 0.0658896267414093\n",
            "Epoch: 100 | Loss: 0.06494271755218506\n",
            "Epoch: 101 | Loss: 0.06400935351848602\n",
            "Epoch: 102 | Loss: 0.06308940052986145\n",
            "Epoch: 103 | Loss: 0.06218267232179642\n",
            "Epoch: 104 | Loss: 0.06128909811377525\n",
            "Epoch: 105 | Loss: 0.060408126562833786\n",
            "Epoch: 106 | Loss: 0.059540044516325\n",
            "Epoch: 107 | Loss: 0.058684367686510086\n",
            "Epoch: 108 | Loss: 0.05784095078706741\n",
            "Epoch: 109 | Loss: 0.057009853422641754\n",
            "Epoch: 110 | Loss: 0.056190453469753265\n",
            "Epoch: 111 | Loss: 0.055382903665304184\n",
            "Epoch: 112 | Loss: 0.054586995393037796\n",
            "Epoch: 113 | Loss: 0.05380234867334366\n",
            "Epoch: 114 | Loss: 0.05302925407886505\n",
            "Epoch: 115 | Loss: 0.0522671714425087\n",
            "Epoch: 116 | Loss: 0.05151592195034027\n",
            "Epoch: 117 | Loss: 0.050775621086359024\n",
            "Epoch: 118 | Loss: 0.0500459223985672\n",
            "Epoch: 119 | Loss: 0.04932660982012749\n",
            "Epoch: 120 | Loss: 0.04861775040626526\n",
            "Epoch: 121 | Loss: 0.04791902378201485\n",
            "Epoch: 122 | Loss: 0.0472303070127964\n",
            "Epoch: 123 | Loss: 0.04655158147215843\n",
            "Epoch: 124 | Loss: 0.045882485806941986\n",
            "Epoch: 125 | Loss: 0.045223135501146317\n",
            "Epoch: 126 | Loss: 0.04457325488328934\n",
            "Epoch: 127 | Loss: 0.043932609260082245\n",
            "Epoch: 128 | Loss: 0.043301209807395935\n",
            "Epoch: 129 | Loss: 0.04267900064587593\n",
            "Epoch: 130 | Loss: 0.04206560552120209\n",
            "Epoch: 131 | Loss: 0.041461024433374405\n",
            "Epoch: 132 | Loss: 0.04086511582136154\n",
            "Epoch: 133 | Loss: 0.04027785360813141\n",
            "Epoch: 134 | Loss: 0.0396990105509758\n",
            "Epoch: 135 | Loss: 0.03912840038537979\n",
            "Epoch: 136 | Loss: 0.038566119968891144\n",
            "Epoch: 137 | Loss: 0.03801187500357628\n",
            "Epoch: 138 | Loss: 0.03746560215950012\n",
            "Epoch: 139 | Loss: 0.03692708909511566\n",
            "Epoch: 140 | Loss: 0.03639647364616394\n",
            "Epoch: 141 | Loss: 0.035873427987098694\n",
            "Epoch: 142 | Loss: 0.03535781055688858\n",
            "Epoch: 143 | Loss: 0.034849680960178375\n",
            "Epoch: 144 | Loss: 0.03434883803129196\n",
            "Epoch: 145 | Loss: 0.03385521471500397\n",
            "Epoch: 146 | Loss: 0.033368658274412155\n",
            "Epoch: 147 | Loss: 0.03288900852203369\n",
            "Epoch: 148 | Loss: 0.03241638094186783\n",
            "Epoch: 149 | Loss: 0.031950563192367554\n",
            "Epoch: 150 | Loss: 0.03149126097559929\n",
            "Epoch: 151 | Loss: 0.031038768589496613\n",
            "Epoch: 152 | Loss: 0.03059273399412632\n",
            "Epoch: 153 | Loss: 0.030152924358844757\n",
            "Epoch: 154 | Loss: 0.02971974015235901\n",
            "Epoch: 155 | Loss: 0.02929254248738289\n",
            "Epoch: 156 | Loss: 0.028871610760688782\n",
            "Epoch: 157 | Loss: 0.028456639498472214\n",
            "Epoch: 158 | Loss: 0.028047651052474976\n",
            "Epoch: 159 | Loss: 0.027644522488117218\n",
            "Epoch: 160 | Loss: 0.027247201651334763\n",
            "Epoch: 161 | Loss: 0.026855643838644028\n",
            "Epoch: 162 | Loss: 0.026469692587852478\n",
            "Epoch: 163 | Loss: 0.02608933113515377\n",
            "Epoch: 164 | Loss: 0.025714371353387833\n",
            "Epoch: 165 | Loss: 0.025344839319586754\n",
            "Epoch: 166 | Loss: 0.024980591610074043\n",
            "Epoch: 167 | Loss: 0.02462156116962433\n",
            "Epoch: 168 | Loss: 0.024267762899398804\n",
            "Epoch: 169 | Loss: 0.023918965831398964\n",
            "Epoch: 170 | Loss: 0.023575201630592346\n",
            "Epoch: 171 | Loss: 0.02323639765381813\n",
            "Epoch: 172 | Loss: 0.022902384400367737\n",
            "Epoch: 173 | Loss: 0.022573299705982208\n",
            "Epoch: 174 | Loss: 0.022248927503824234\n",
            "Epoch: 175 | Loss: 0.021929126232862473\n",
            "Epoch: 176 | Loss: 0.02161397412419319\n",
            "Epoch: 177 | Loss: 0.021303288638591766\n",
            "Epoch: 178 | Loss: 0.020997215062379837\n",
            "Epoch: 179 | Loss: 0.020695455372333527\n",
            "Epoch: 180 | Loss: 0.020398013293743134\n",
            "Epoch: 181 | Loss: 0.020104801282286644\n",
            "Epoch: 182 | Loss: 0.019815856590867043\n",
            "Epoch: 183 | Loss: 0.0195311326533556\n",
            "Epoch: 184 | Loss: 0.019250381737947464\n",
            "Epoch: 185 | Loss: 0.01897371932864189\n",
            "Epoch: 186 | Loss: 0.018701087683439255\n",
            "Epoch: 187 | Loss: 0.01843227818608284\n",
            "Epoch: 188 | Loss: 0.018167372792959213\n",
            "Epoch: 189 | Loss: 0.017906315624713898\n",
            "Epoch: 190 | Loss: 0.017648976296186447\n",
            "Epoch: 191 | Loss: 0.017395278438925743\n",
            "Epoch: 192 | Loss: 0.017145346850156784\n",
            "Epoch: 193 | Loss: 0.016898920759558678\n",
            "Epoch: 194 | Loss: 0.016656072810292244\n",
            "Epoch: 195 | Loss: 0.016416652128100395\n",
            "Epoch: 196 | Loss: 0.016180813312530518\n",
            "Epoch: 197 | Loss: 0.01594824343919754\n",
            "Epoch: 198 | Loss: 0.01571904867887497\n",
            "Epoch: 199 | Loss: 0.015493093058466911\n",
            "Epoch: 200 | Loss: 0.015270431526005268\n",
            "Epoch: 201 | Loss: 0.01505096536129713\n",
            "Epoch: 202 | Loss: 0.014834688976407051\n",
            "Epoch: 203 | Loss: 0.014621509239077568\n",
            "Epoch: 204 | Loss: 0.014411380514502525\n",
            "Epoch: 205 | Loss: 0.014204268343746662\n",
            "Epoch: 206 | Loss: 0.01400009449571371\n",
            "Epoch: 207 | Loss: 0.013798881322145462\n",
            "Epoch: 208 | Loss: 0.013600587844848633\n",
            "Epoch: 209 | Loss: 0.013405095785856247\n",
            "Epoch: 210 | Loss: 0.013212510384619236\n",
            "Epoch: 211 | Loss: 0.013022572733461857\n",
            "Epoch: 212 | Loss: 0.012835484929382801\n",
            "Epoch: 213 | Loss: 0.012651014141738415\n",
            "Epoch: 214 | Loss: 0.012469183653593063\n",
            "Epoch: 215 | Loss: 0.012289975769817829\n",
            "Epoch: 216 | Loss: 0.01211335975676775\n",
            "Epoch: 217 | Loss: 0.011939202435314655\n",
            "Epoch: 218 | Loss: 0.011767623946070671\n",
            "Epoch: 219 | Loss: 0.011598504148423672\n",
            "Epoch: 220 | Loss: 0.011431857943534851\n",
            "Epoch: 221 | Loss: 0.011267532594501972\n",
            "Epoch: 222 | Loss: 0.011105635203421116\n",
            "Epoch: 223 | Loss: 0.010945999063551426\n",
            "Epoch: 224 | Loss: 0.01078873872756958\n",
            "Epoch: 225 | Loss: 0.010633653961122036\n",
            "Epoch: 226 | Loss: 0.010480880737304688\n",
            "Epoch: 227 | Loss: 0.01033022440969944\n",
            "Epoch: 228 | Loss: 0.010181756690144539\n",
            "Epoch: 229 | Loss: 0.01003541611135006\n",
            "Epoch: 230 | Loss: 0.009891165420413017\n",
            "Epoch: 231 | Loss: 0.009749089367687702\n",
            "Epoch: 232 | Loss: 0.009608913213014603\n",
            "Epoch: 233 | Loss: 0.00947081670165062\n",
            "Epoch: 234 | Loss: 0.009334742091596127\n",
            "Epoch: 235 | Loss: 0.009200593456625938\n",
            "Epoch: 236 | Loss: 0.009068375453352928\n",
            "Epoch: 237 | Loss: 0.008938008919358253\n",
            "Epoch: 238 | Loss: 0.008809568360447884\n",
            "Epoch: 239 | Loss: 0.008682952262461185\n",
            "Epoch: 240 | Loss: 0.00855818297713995\n",
            "Epoch: 241 | Loss: 0.008435149677097797\n",
            "Epoch: 242 | Loss: 0.008313950151205063\n",
            "Epoch: 243 | Loss: 0.008194508031010628\n",
            "Epoch: 244 | Loss: 0.008076738566160202\n",
            "Epoch: 245 | Loss: 0.007960638031363487\n",
            "Epoch: 246 | Loss: 0.00784625206142664\n",
            "Epoch: 247 | Loss: 0.007733476348221302\n",
            "Epoch: 248 | Loss: 0.007622349075973034\n",
            "Epoch: 249 | Loss: 0.00751279853284359\n",
            "Epoch: 250 | Loss: 0.00740480562672019\n",
            "Epoch: 251 | Loss: 0.0072983745485544205\n",
            "Epoch: 252 | Loss: 0.007193522062152624\n",
            "Epoch: 253 | Loss: 0.007090119179338217\n",
            "Epoch: 254 | Loss: 0.006988232489675283\n",
            "Epoch: 255 | Loss: 0.006887797266244888\n",
            "Epoch: 256 | Loss: 0.006788774859160185\n",
            "Epoch: 257 | Loss: 0.006691220682114363\n",
            "Epoch: 258 | Loss: 0.0065951040014624596\n",
            "Epoch: 259 | Loss: 0.0065002781338989735\n",
            "Epoch: 260 | Loss: 0.006406869739294052\n",
            "Epoch: 261 | Loss: 0.006314793135970831\n",
            "Epoch: 262 | Loss: 0.006224039010703564\n",
            "Epoch: 263 | Loss: 0.0061345938593149185\n",
            "Epoch: 264 | Loss: 0.006046433001756668\n",
            "Epoch: 265 | Loss: 0.00595957413315773\n",
            "Epoch: 266 | Loss: 0.005873856600373983\n",
            "Epoch: 267 | Loss: 0.005789471790194511\n",
            "Epoch: 268 | Loss: 0.00570629583671689\n",
            "Epoch: 269 | Loss: 0.00562424398958683\n",
            "Epoch: 270 | Loss: 0.005543397273868322\n",
            "Epoch: 271 | Loss: 0.005463750101625919\n",
            "Epoch: 272 | Loss: 0.005385255441069603\n",
            "Epoch: 273 | Loss: 0.005307832732796669\n",
            "Epoch: 274 | Loss: 0.005231555551290512\n",
            "Epoch: 275 | Loss: 0.005156406667083502\n",
            "Epoch: 276 | Loss: 0.005082302261143923\n",
            "Epoch: 277 | Loss: 0.005009245127439499\n",
            "Epoch: 278 | Loss: 0.00493723526597023\n",
            "Epoch: 279 | Loss: 0.004866295028477907\n",
            "Epoch: 280 | Loss: 0.00479632243514061\n",
            "Epoch: 281 | Loss: 0.004727412946522236\n",
            "Epoch: 282 | Loss: 0.004659458063542843\n",
            "Epoch: 283 | Loss: 0.004592495504766703\n",
            "Epoch: 284 | Loss: 0.004526510834693909\n",
            "Epoch: 285 | Loss: 0.0044614458456635475\n",
            "Epoch: 286 | Loss: 0.004397365264594555\n",
            "Epoch: 287 | Loss: 0.004334146156907082\n",
            "Epoch: 288 | Loss: 0.004271875135600567\n",
            "Epoch: 289 | Loss: 0.004210461862385273\n",
            "Epoch: 290 | Loss: 0.004149931482970715\n",
            "Epoch: 291 | Loss: 0.004090330097824335\n",
            "Epoch: 292 | Loss: 0.0040315184742212296\n",
            "Epoch: 293 | Loss: 0.003973572049289942\n",
            "Epoch: 294 | Loss: 0.0039164782501757145\n",
            "Epoch: 295 | Loss: 0.003860180964693427\n",
            "Epoch: 296 | Loss: 0.0038046925328671932\n",
            "Epoch: 297 | Loss: 0.0037500294856727123\n",
            "Epoch: 298 | Loss: 0.003696154337376356\n",
            "Epoch: 299 | Loss: 0.0036430375184863806\n",
            "Epoch: 300 | Loss: 0.003590670879930258\n",
            "Epoch: 301 | Loss: 0.003539098659530282\n",
            "Epoch: 302 | Loss: 0.003488212125375867\n",
            "Epoch: 303 | Loss: 0.0034380690194666386\n",
            "Epoch: 304 | Loss: 0.0033886744640767574\n",
            "Epoch: 305 | Loss: 0.003339977003633976\n",
            "Epoch: 306 | Loss: 0.003291965462267399\n",
            "Epoch: 307 | Loss: 0.003244645893573761\n",
            "Epoch: 308 | Loss: 0.0031980108469724655\n",
            "Epoch: 309 | Loss: 0.0031520628836005926\n",
            "Epoch: 310 | Loss: 0.0031067775562405586\n",
            "Epoch: 311 | Loss: 0.003062106668949127\n",
            "Epoch: 312 | Loss: 0.003018100978806615\n",
            "Epoch: 313 | Loss: 0.0029747183434665203\n",
            "Epoch: 314 | Loss: 0.002931983210146427\n",
            "Epoch: 315 | Loss: 0.0028898355085402727\n",
            "Epoch: 316 | Loss: 0.0028483159840106964\n",
            "Epoch: 317 | Loss: 0.0028073706198483706\n",
            "Epoch: 318 | Loss: 0.002767021767795086\n",
            "Epoch: 319 | Loss: 0.002727256156504154\n",
            "Epoch: 320 | Loss: 0.002688055858016014\n",
            "Epoch: 321 | Loss: 0.0026494269259274006\n",
            "Epoch: 322 | Loss: 0.002611338859423995\n",
            "Epoch: 323 | Loss: 0.0025738007389009\n",
            "Epoch: 324 | Loss: 0.0025368332862854004\n",
            "Epoch: 325 | Loss: 0.0025003713089972734\n",
            "Epoch: 326 | Loss: 0.0024644392542541027\n",
            "Epoch: 327 | Loss: 0.0024290019646286964\n",
            "Epoch: 328 | Loss: 0.0023941146209836006\n",
            "Epoch: 329 | Loss: 0.0023596882820129395\n",
            "Epoch: 330 | Loss: 0.0023257918655872345\n",
            "Epoch: 331 | Loss: 0.002292363438755274\n",
            "Epoch: 332 | Loss: 0.0022594230249524117\n",
            "Epoch: 333 | Loss: 0.002226948505267501\n",
            "Epoch: 334 | Loss: 0.0021949377842247486\n",
            "Epoch: 335 | Loss: 0.0021633997093886137\n",
            "Epoch: 336 | Loss: 0.002132307505235076\n",
            "Epoch: 337 | Loss: 0.0021016800310462713\n",
            "Epoch: 338 | Loss: 0.0020714516285806894\n",
            "Epoch: 339 | Loss: 0.002041680971160531\n",
            "Epoch: 340 | Loss: 0.00201233197003603\n",
            "Epoch: 341 | Loss: 0.0019834155682474375\n",
            "Epoch: 342 | Loss: 0.0019549219869077206\n",
            "Epoch: 343 | Loss: 0.0019268256146460772\n",
            "Epoch: 344 | Loss: 0.0018991330871358514\n",
            "Epoch: 345 | Loss: 0.0018718570936471224\n",
            "Epoch: 346 | Loss: 0.0018449409399181604\n",
            "Epoch: 347 | Loss: 0.0018184180371463299\n",
            "Epoch: 348 | Loss: 0.001792296301573515\n",
            "Epoch: 349 | Loss: 0.0017665468621999025\n",
            "Epoch: 350 | Loss: 0.0017411415465176105\n",
            "Epoch: 351 | Loss: 0.0017161249415948987\n",
            "Epoch: 352 | Loss: 0.0016914638690650463\n",
            "Epoch: 353 | Loss: 0.0016671355115249753\n",
            "Epoch: 354 | Loss: 0.0016431908588856459\n",
            "Epoch: 355 | Loss: 0.001619564602151513\n",
            "Epoch: 356 | Loss: 0.0015963091282173991\n",
            "Epoch: 357 | Loss: 0.0015733533073216677\n",
            "Epoch: 358 | Loss: 0.0015507354401051998\n",
            "Epoch: 359 | Loss: 0.0015284722903743386\n",
            "Epoch: 360 | Loss: 0.0015064873732626438\n",
            "Epoch: 361 | Loss: 0.0014848433202132583\n",
            "Epoch: 362 | Loss: 0.0014635048573836684\n",
            "Epoch: 363 | Loss: 0.0014424711698666215\n",
            "Epoch: 364 | Loss: 0.001421754015609622\n",
            "Epoch: 365 | Loss: 0.001401302171871066\n",
            "Epoch: 366 | Loss: 0.00138117338065058\n",
            "Epoch: 367 | Loss: 0.0013613166520372033\n",
            "Epoch: 368 | Loss: 0.0013417679583653808\n",
            "Epoch: 369 | Loss: 0.001322464202530682\n",
            "Epoch: 370 | Loss: 0.0013034765142947435\n",
            "Epoch: 371 | Loss: 0.0012847200268879533\n",
            "Epoch: 372 | Loss: 0.0012662712251767516\n",
            "Epoch: 373 | Loss: 0.0012480770237743855\n",
            "Epoch: 374 | Loss: 0.0012301347451284528\n",
            "Epoch: 375 | Loss: 0.0012124620843678713\n",
            "Epoch: 376 | Loss: 0.0011950219050049782\n",
            "Epoch: 377 | Loss: 0.0011778536718338728\n",
            "Epoch: 378 | Loss: 0.0011609324719756842\n",
            "Epoch: 379 | Loss: 0.001144245732575655\n",
            "Epoch: 380 | Loss: 0.001127799041569233\n",
            "Epoch: 381 | Loss: 0.0011115914676338434\n",
            "Epoch: 382 | Loss: 0.0010956110199913383\n",
            "Epoch: 383 | Loss: 0.0010798608418554068\n",
            "Epoch: 384 | Loss: 0.0010643688729032874\n",
            "Epoch: 385 | Loss: 0.001049058511853218\n",
            "Epoch: 386 | Loss: 0.0010339751606807113\n",
            "Epoch: 387 | Loss: 0.0010191177716478705\n",
            "Epoch: 388 | Loss: 0.0010044840164482594\n",
            "Epoch: 389 | Loss: 0.0009900329168885946\n",
            "Epoch: 390 | Loss: 0.0009758137166500092\n",
            "Epoch: 391 | Loss: 0.0009617875330150127\n",
            "Epoch: 392 | Loss: 0.0009479577420279384\n",
            "Epoch: 393 | Loss: 0.0009343507117591798\n",
            "Epoch: 394 | Loss: 0.0009209162672050297\n",
            "Epoch: 395 | Loss: 0.0009076736168935895\n",
            "Epoch: 396 | Loss: 0.0008946416201069951\n",
            "Epoch: 397 | Loss: 0.0008817868074402213\n",
            "Epoch: 398 | Loss: 0.0008691123803146183\n",
            "Epoch: 399 | Loss: 0.0008566147880628705\n",
            "Epoch: 400 | Loss: 0.0008442989783361554\n",
            "Epoch: 401 | Loss: 0.0008321579662151635\n",
            "Epoch: 402 | Loss: 0.000820206303615123\n",
            "Epoch: 403 | Loss: 0.0008084155851975083\n",
            "Epoch: 404 | Loss: 0.000796795473434031\n",
            "Epoch: 405 | Loss: 0.0007853589486330748\n",
            "Epoch: 406 | Loss: 0.0007740730652585626\n",
            "Epoch: 407 | Loss: 0.0007629314204677939\n",
            "Epoch: 408 | Loss: 0.0007519806968048215\n",
            "Epoch: 409 | Loss: 0.0007411612314172089\n",
            "Epoch: 410 | Loss: 0.0007305207545869052\n",
            "Epoch: 411 | Loss: 0.0007200275431387126\n",
            "Epoch: 412 | Loss: 0.0007096627959981561\n",
            "Epoch: 413 | Loss: 0.0006994777941145003\n",
            "Epoch: 414 | Loss: 0.0006894136895425618\n",
            "Epoch: 415 | Loss: 0.0006795119843445718\n",
            "Epoch: 416 | Loss: 0.0006697525968775153\n",
            "Epoch: 417 | Loss: 0.0006601146305911243\n",
            "Epoch: 418 | Loss: 0.0006506399367935956\n",
            "Epoch: 419 | Loss: 0.0006412838120013475\n",
            "Epoch: 420 | Loss: 0.0006320655811578035\n",
            "Epoch: 421 | Loss: 0.0006229750579223037\n",
            "Epoch: 422 | Loss: 0.0006140397163107991\n",
            "Epoch: 423 | Loss: 0.0006052132230252028\n",
            "Epoch: 424 | Loss: 0.0005965117597952485\n",
            "Epoch: 425 | Loss: 0.0005879352684132755\n",
            "Epoch: 426 | Loss: 0.0005794853204861283\n",
            "Epoch: 427 | Loss: 0.0005711576086468995\n",
            "Epoch: 428 | Loss: 0.000562939909286797\n",
            "Epoch: 429 | Loss: 0.0005548584158532321\n",
            "Epoch: 430 | Loss: 0.0005468842573463917\n",
            "Epoch: 431 | Loss: 0.000539017841219902\n",
            "Epoch: 432 | Loss: 0.0005312754074111581\n",
            "Epoch: 433 | Loss: 0.0005236448487266898\n",
            "Epoch: 434 | Loss: 0.0005161130684427917\n",
            "Epoch: 435 | Loss: 0.0005086974706500769\n",
            "Epoch: 436 | Loss: 0.0005013954360038042\n",
            "Epoch: 437 | Loss: 0.0004941733786836267\n",
            "Epoch: 438 | Loss: 0.0004870793782174587\n",
            "Epoch: 439 | Loss: 0.00048008636804297566\n",
            "Epoch: 440 | Loss: 0.0004731793305836618\n",
            "Epoch: 441 | Loss: 0.0004663801228161901\n",
            "Epoch: 442 | Loss: 0.0004596762009896338\n",
            "Epoch: 443 | Loss: 0.00045306907850317657\n",
            "Epoch: 444 | Loss: 0.00044656265527009964\n",
            "Epoch: 445 | Loss: 0.0004401386540848762\n",
            "Epoch: 446 | Loss: 0.0004338181170169264\n",
            "Epoch: 447 | Loss: 0.00042757694609463215\n",
            "Epoch: 448 | Loss: 0.0004214370856061578\n",
            "Epoch: 449 | Loss: 0.00041537481592968106\n",
            "Epoch: 450 | Loss: 0.0004094117903150618\n",
            "Epoch: 451 | Loss: 0.00040352571522817016\n",
            "Epoch: 452 | Loss: 0.0003977240121457726\n",
            "Epoch: 453 | Loss: 0.00039200924220494926\n",
            "Epoch: 454 | Loss: 0.00038637692341580987\n",
            "Epoch: 455 | Loss: 0.0003808183246292174\n",
            "Epoch: 456 | Loss: 0.0003753438941203058\n",
            "Epoch: 457 | Loss: 0.00036996163544245064\n",
            "Epoch: 458 | Loss: 0.00036463828291743994\n",
            "Epoch: 459 | Loss: 0.0003593909787014127\n",
            "Epoch: 460 | Loss: 0.00035423639928922057\n",
            "Epoch: 461 | Loss: 0.0003491429379209876\n",
            "Epoch: 462 | Loss: 0.0003441188018769026\n",
            "Epoch: 463 | Loss: 0.00033918130793608725\n",
            "Epoch: 464 | Loss: 0.00033430717303417623\n",
            "Epoch: 465 | Loss: 0.0003295041969977319\n",
            "Epoch: 466 | Loss: 0.000324771594023332\n",
            "Epoch: 467 | Loss: 0.0003200898936484009\n",
            "Epoch: 468 | Loss: 0.00031549794948659837\n",
            "Epoch: 469 | Loss: 0.0003109699464403093\n",
            "Epoch: 470 | Loss: 0.00030649296240881085\n",
            "Epoch: 471 | Loss: 0.00030209089163690805\n",
            "Epoch: 472 | Loss: 0.00029774976428598166\n",
            "Epoch: 473 | Loss: 0.0002934689400717616\n",
            "Epoch: 474 | Loss: 0.0002892497868742794\n",
            "Epoch: 475 | Loss: 0.000285102374618873\n",
            "Epoch: 476 | Loss: 0.000281003478448838\n",
            "Epoch: 477 | Loss: 0.0002769594138953835\n",
            "Epoch: 478 | Loss: 0.0002729830448515713\n",
            "Epoch: 479 | Loss: 0.00026905740378424525\n",
            "Epoch: 480 | Loss: 0.0002651933173183352\n",
            "Epoch: 481 | Loss: 0.00026137882377952337\n",
            "Epoch: 482 | Loss: 0.0002576199476607144\n",
            "Epoch: 483 | Loss: 0.00025392440147697926\n",
            "Epoch: 484 | Loss: 0.0002502684947103262\n",
            "Epoch: 485 | Loss: 0.00024667923571541905\n",
            "Epoch: 486 | Loss: 0.00024312858295161277\n",
            "Epoch: 487 | Loss: 0.00023963248531799763\n",
            "Epoch: 488 | Loss: 0.00023619388230144978\n",
            "Epoch: 489 | Loss: 0.00023280229652300477\n",
            "Epoch: 490 | Loss: 0.0002294511068612337\n",
            "Epoch: 491 | Loss: 0.00022615306079387665\n",
            "Epoch: 492 | Loss: 0.00022290318156592548\n",
            "Epoch: 493 | Loss: 0.00021969839872326702\n",
            "Epoch: 494 | Loss: 0.00021654076408594847\n",
            "Epoch: 495 | Loss: 0.00021343230037018657\n",
            "Epoch: 496 | Loss: 0.00021036573161836714\n",
            "Epoch: 497 | Loss: 0.00020733893325086683\n",
            "Epoch: 498 | Loss: 0.00020435899205040187\n",
            "Epoch: 499 | Loss: 0.00020142043649684638\n",
            "Prediction (after training 4 7.98368501663208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r6govn3eu8F",
        "outputId": "af951169-4a19-47da-d25e-3b7716c98851"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print(f'Epoch: {epoch+1} | Loss: {loss.item()}')\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() # w=w-0.01*w.grad().item()\n",
        "#After training\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 0.7001646757125854\n",
            "Epoch: 2 | Loss: 0.6983529925346375\n",
            "Epoch: 3 | Loss: 0.6966046094894409\n",
            "Epoch: 4 | Loss: 0.6949169635772705\n",
            "Epoch: 5 | Loss: 0.6932874917984009\n",
            "Epoch: 6 | Loss: 0.691713809967041\n",
            "Epoch: 7 | Loss: 0.6901935935020447\n",
            "Epoch: 8 | Loss: 0.6887243986129761\n",
            "Epoch: 9 | Loss: 0.6873043775558472\n",
            "Epoch: 10 | Loss: 0.6859310865402222\n",
            "Epoch: 11 | Loss: 0.6846029758453369\n",
            "Epoch: 12 | Loss: 0.6833178997039795\n",
            "Epoch: 13 | Loss: 0.6820740103721619\n",
            "Epoch: 14 | Loss: 0.6808698773384094\n",
            "Epoch: 15 | Loss: 0.6797034740447998\n",
            "Epoch: 16 | Loss: 0.6785736083984375\n",
            "Epoch: 17 | Loss: 0.6774783730506897\n",
            "Epoch: 18 | Loss: 0.6764166355133057\n",
            "Epoch: 19 | Loss: 0.6753867268562317\n",
            "Epoch: 20 | Loss: 0.6743873953819275\n",
            "Epoch: 21 | Loss: 0.6734175682067871\n",
            "Epoch: 22 | Loss: 0.6724758744239807\n",
            "Epoch: 23 | Loss: 0.671561062335968\n",
            "Epoch: 24 | Loss: 0.6706721186637878\n",
            "Epoch: 25 | Loss: 0.669808030128479\n",
            "Epoch: 26 | Loss: 0.6689676642417908\n",
            "Epoch: 27 | Loss: 0.6681500673294067\n",
            "Epoch: 28 | Loss: 0.6673543453216553\n",
            "Epoch: 29 | Loss: 0.66657954454422\n",
            "Epoch: 30 | Loss: 0.6658248901367188\n",
            "Epoch: 31 | Loss: 0.6650894284248352\n",
            "Epoch: 32 | Loss: 0.6643725037574768\n",
            "Epoch: 33 | Loss: 0.6636732816696167\n",
            "Epoch: 34 | Loss: 0.6629911065101624\n",
            "Epoch: 35 | Loss: 0.6623252034187317\n",
            "Epoch: 36 | Loss: 0.6616749167442322\n",
            "Epoch: 37 | Loss: 0.6610397100448608\n",
            "Epoch: 38 | Loss: 0.6604189276695251\n",
            "Epoch: 39 | Loss: 0.6598120927810669\n",
            "Epoch: 40 | Loss: 0.659218430519104\n",
            "Epoch: 41 | Loss: 0.6586374044418335\n",
            "Epoch: 42 | Loss: 0.6580687165260315\n",
            "Epoch: 43 | Loss: 0.6575118899345398\n",
            "Epoch: 44 | Loss: 0.6569662690162659\n",
            "Epoch: 45 | Loss: 0.6564314365386963\n",
            "Epoch: 46 | Loss: 0.6559069752693176\n",
            "Epoch: 47 | Loss: 0.6553927063941956\n",
            "Epoch: 48 | Loss: 0.6548879146575928\n",
            "Epoch: 49 | Loss: 0.6543923020362854\n",
            "Epoch: 50 | Loss: 0.6539056897163391\n",
            "Epoch: 51 | Loss: 0.6534274816513062\n",
            "Epoch: 52 | Loss: 0.652957558631897\n",
            "Epoch: 53 | Loss: 0.6524954438209534\n",
            "Epoch: 54 | Loss: 0.6520407795906067\n",
            "Epoch: 55 | Loss: 0.6515935659408569\n",
            "Epoch: 56 | Loss: 0.6511533260345459\n",
            "Epoch: 57 | Loss: 0.6507197022438049\n",
            "Epoch: 58 | Loss: 0.6502925753593445\n",
            "Epoch: 59 | Loss: 0.6498716473579407\n",
            "Epoch: 60 | Loss: 0.6494567394256592\n",
            "Epoch: 61 | Loss: 0.6490476131439209\n",
            "Epoch: 62 | Loss: 0.648643970489502\n",
            "Epoch: 63 | Loss: 0.6482455730438232\n",
            "Epoch: 64 | Loss: 0.6478524208068848\n",
            "Epoch: 65 | Loss: 0.6474641561508179\n",
            "Epoch: 66 | Loss: 0.6470806002616882\n",
            "Epoch: 67 | Loss: 0.6467016339302063\n",
            "Epoch: 68 | Loss: 0.646327018737793\n",
            "Epoch: 69 | Loss: 0.6459567546844482\n",
            "Epoch: 70 | Loss: 0.6455904245376587\n",
            "Epoch: 71 | Loss: 0.6452280879020691\n",
            "Epoch: 72 | Loss: 0.6448695063591003\n",
            "Epoch: 73 | Loss: 0.6445145606994629\n",
            "Epoch: 74 | Loss: 0.6441631317138672\n",
            "Epoch: 75 | Loss: 0.6438151597976685\n",
            "Epoch: 76 | Loss: 0.6434704065322876\n",
            "Epoch: 77 | Loss: 0.6431287527084351\n",
            "Epoch: 78 | Loss: 0.6427900791168213\n",
            "Epoch: 79 | Loss: 0.6424543261528015\n",
            "Epoch: 80 | Loss: 0.642121434211731\n",
            "Epoch: 81 | Loss: 0.6417912244796753\n",
            "Epoch: 82 | Loss: 0.6414636373519897\n",
            "Epoch: 83 | Loss: 0.6411386132240295\n",
            "Epoch: 84 | Loss: 0.6408160328865051\n",
            "Epoch: 85 | Loss: 0.6404956579208374\n",
            "Epoch: 86 | Loss: 0.6401776671409607\n",
            "Epoch: 87 | Loss: 0.6398618817329407\n",
            "Epoch: 88 | Loss: 0.6395480632781982\n",
            "Epoch: 89 | Loss: 0.6392365097999573\n",
            "Epoch: 90 | Loss: 0.6389267444610596\n",
            "Epoch: 91 | Loss: 0.6386189460754395\n",
            "Epoch: 92 | Loss: 0.6383129954338074\n",
            "Epoch: 93 | Loss: 0.638008713722229\n",
            "Epoch: 94 | Loss: 0.6377061605453491\n",
            "Epoch: 95 | Loss: 0.6374053955078125\n",
            "Epoch: 96 | Loss: 0.63710618019104\n",
            "Epoch: 97 | Loss: 0.6368083953857422\n",
            "Epoch: 98 | Loss: 0.6365121603012085\n",
            "Epoch: 99 | Loss: 0.6362174153327942\n",
            "Epoch: 100 | Loss: 0.6359241008758545\n",
            "Epoch: 101 | Loss: 0.6356320977210999\n",
            "Epoch: 102 | Loss: 0.6353413462638855\n",
            "Epoch: 103 | Loss: 0.6350519061088562\n",
            "Epoch: 104 | Loss: 0.6347637176513672\n",
            "Epoch: 105 | Loss: 0.6344766616821289\n",
            "Epoch: 106 | Loss: 0.6341908574104309\n",
            "Epoch: 107 | Loss: 0.6339061856269836\n",
            "Epoch: 108 | Loss: 0.6336223483085632\n",
            "Epoch: 109 | Loss: 0.6333397626876831\n",
            "Epoch: 110 | Loss: 0.6330581307411194\n",
            "Epoch: 111 | Loss: 0.6327775120735168\n",
            "Epoch: 112 | Loss: 0.6324978470802307\n",
            "Epoch: 113 | Loss: 0.6322190165519714\n",
            "Epoch: 114 | Loss: 0.6319411396980286\n",
            "Epoch: 115 | Loss: 0.6316641569137573\n",
            "Epoch: 116 | Loss: 0.6313880681991577\n",
            "Epoch: 117 | Loss: 0.6311126947402954\n",
            "Epoch: 118 | Loss: 0.63083815574646\n",
            "Epoch: 119 | Loss: 0.6305643916130066\n",
            "Epoch: 120 | Loss: 0.6302914023399353\n",
            "Epoch: 121 | Loss: 0.6300191283226013\n",
            "Epoch: 122 | Loss: 0.6297476291656494\n",
            "Epoch: 123 | Loss: 0.6294767260551453\n",
            "Epoch: 124 | Loss: 0.6292064785957336\n",
            "Epoch: 125 | Loss: 0.6289368867874146\n",
            "Epoch: 126 | Loss: 0.6286680102348328\n",
            "Epoch: 127 | Loss: 0.6283997297286987\n",
            "Epoch: 128 | Loss: 0.6281320452690125\n",
            "Epoch: 129 | Loss: 0.6278648972511292\n",
            "Epoch: 130 | Loss: 0.6275984048843384\n",
            "Epoch: 131 | Loss: 0.6273323893547058\n",
            "Epoch: 132 | Loss: 0.6270670294761658\n",
            "Epoch: 133 | Loss: 0.6268021464347839\n",
            "Epoch: 134 | Loss: 0.6265377998352051\n",
            "Epoch: 135 | Loss: 0.6262738108634949\n",
            "Epoch: 136 | Loss: 0.6260104179382324\n",
            "Epoch: 137 | Loss: 0.625747561454773\n",
            "Epoch: 138 | Loss: 0.6254851222038269\n",
            "Epoch: 139 | Loss: 0.6252231597900391\n",
            "Epoch: 140 | Loss: 0.6249615550041199\n",
            "Epoch: 141 | Loss: 0.6247005462646484\n",
            "Epoch: 142 | Loss: 0.6244398951530457\n",
            "Epoch: 143 | Loss: 0.6241797208786011\n",
            "Epoch: 144 | Loss: 0.6239198446273804\n",
            "Epoch: 145 | Loss: 0.6236604452133179\n",
            "Epoch: 146 | Loss: 0.623401403427124\n",
            "Epoch: 147 | Loss: 0.6231427788734436\n",
            "Epoch: 148 | Loss: 0.6228845119476318\n",
            "Epoch: 149 | Loss: 0.6226266622543335\n",
            "Epoch: 150 | Loss: 0.6223691701889038\n",
            "Epoch: 151 | Loss: 0.6221119165420532\n",
            "Epoch: 152 | Loss: 0.6218551397323608\n",
            "Epoch: 153 | Loss: 0.6215986609458923\n",
            "Epoch: 154 | Loss: 0.6213425993919373\n",
            "Epoch: 155 | Loss: 0.6210867166519165\n",
            "Epoch: 156 | Loss: 0.6208313703536987\n",
            "Epoch: 157 | Loss: 0.6205762028694153\n",
            "Epoch: 158 | Loss: 0.6203213334083557\n",
            "Epoch: 159 | Loss: 0.6200668215751648\n",
            "Epoch: 160 | Loss: 0.6198126077651978\n",
            "Epoch: 161 | Loss: 0.6195586919784546\n",
            "Epoch: 162 | Loss: 0.6193050146102905\n",
            "Epoch: 163 | Loss: 0.6190517544746399\n",
            "Epoch: 164 | Loss: 0.6187987327575684\n",
            "Epoch: 165 | Loss: 0.6185458898544312\n",
            "Epoch: 166 | Loss: 0.6182934641838074\n",
            "Epoch: 167 | Loss: 0.6180412769317627\n",
            "Epoch: 168 | Loss: 0.6177893877029419\n",
            "Epoch: 169 | Loss: 0.6175377368927002\n",
            "Epoch: 170 | Loss: 0.6172863245010376\n",
            "Epoch: 171 | Loss: 0.6170351505279541\n",
            "Epoch: 172 | Loss: 0.6167842745780945\n",
            "Epoch: 173 | Loss: 0.6165336966514587\n",
            "Epoch: 174 | Loss: 0.6162833571434021\n",
            "Epoch: 175 | Loss: 0.6160332560539246\n",
            "Epoch: 176 | Loss: 0.6157833337783813\n",
            "Epoch: 177 | Loss: 0.615533709526062\n",
            "Epoch: 178 | Loss: 0.615284264087677\n",
            "Epoch: 179 | Loss: 0.6150351762771606\n",
            "Epoch: 180 | Loss: 0.6147862672805786\n",
            "Epoch: 181 | Loss: 0.6145375967025757\n",
            "Epoch: 182 | Loss: 0.6142891645431519\n",
            "Epoch: 183 | Loss: 0.6140409111976624\n",
            "Epoch: 184 | Loss: 0.6137929558753967\n",
            "Epoch: 185 | Loss: 0.6135451197624207\n",
            "Epoch: 186 | Loss: 0.6132976412773132\n",
            "Epoch: 187 | Loss: 0.6130502820014954\n",
            "Epoch: 188 | Loss: 0.6128032207489014\n",
            "Epoch: 189 | Loss: 0.6125563383102417\n",
            "Epoch: 190 | Loss: 0.6123096346855164\n",
            "Epoch: 191 | Loss: 0.6120631694793701\n",
            "Epoch: 192 | Loss: 0.6118168830871582\n",
            "Epoch: 193 | Loss: 0.6115708351135254\n",
            "Epoch: 194 | Loss: 0.6113250255584717\n",
            "Epoch: 195 | Loss: 0.6110793948173523\n",
            "Epoch: 196 | Loss: 0.610834002494812\n",
            "Epoch: 197 | Loss: 0.610588788986206\n",
            "Epoch: 198 | Loss: 0.6103438138961792\n",
            "Epoch: 199 | Loss: 0.6100990176200867\n",
            "Epoch: 200 | Loss: 0.6098544001579285\n",
            "Epoch: 201 | Loss: 0.6096100211143494\n",
            "Epoch: 202 | Loss: 0.6093658208847046\n",
            "Epoch: 203 | Loss: 0.6091217994689941\n",
            "Epoch: 204 | Loss: 0.608877956867218\n",
            "Epoch: 205 | Loss: 0.6086344122886658\n",
            "Epoch: 206 | Loss: 0.6083909869194031\n",
            "Epoch: 207 | Loss: 0.6081477999687195\n",
            "Epoch: 208 | Loss: 0.6079046726226807\n",
            "Epoch: 209 | Loss: 0.6076619029045105\n",
            "Epoch: 210 | Loss: 0.6074192523956299\n",
            "Epoch: 211 | Loss: 0.6071768403053284\n",
            "Epoch: 212 | Loss: 0.6069346070289612\n",
            "Epoch: 213 | Loss: 0.606692373752594\n",
            "Epoch: 214 | Loss: 0.6064505577087402\n",
            "Epoch: 215 | Loss: 0.6062088012695312\n",
            "Epoch: 216 | Loss: 0.6059672832489014\n",
            "Epoch: 217 | Loss: 0.6057260036468506\n",
            "Epoch: 218 | Loss: 0.6054848432540894\n",
            "Epoch: 219 | Loss: 0.6052439212799072\n",
            "Epoch: 220 | Loss: 0.6050030589103699\n",
            "Epoch: 221 | Loss: 0.6047624945640564\n",
            "Epoch: 222 | Loss: 0.6045221090316772\n",
            "Epoch: 223 | Loss: 0.6042818427085876\n",
            "Epoch: 224 | Loss: 0.6040417551994324\n",
            "Epoch: 225 | Loss: 0.6038019061088562\n",
            "Epoch: 226 | Loss: 0.6035622358322144\n",
            "Epoch: 227 | Loss: 0.6033226251602173\n",
            "Epoch: 228 | Loss: 0.6030833721160889\n",
            "Epoch: 229 | Loss: 0.6028441786766052\n",
            "Epoch: 230 | Loss: 0.6026052236557007\n",
            "Epoch: 231 | Loss: 0.6023663878440857\n",
            "Epoch: 232 | Loss: 0.6021277904510498\n",
            "Epoch: 233 | Loss: 0.6018893122673035\n",
            "Epoch: 234 | Loss: 0.6016510725021362\n",
            "Epoch: 235 | Loss: 0.6014129519462585\n",
            "Epoch: 236 | Loss: 0.6011749505996704\n",
            "Epoch: 237 | Loss: 0.6009372472763062\n",
            "Epoch: 238 | Loss: 0.6006996631622314\n",
            "Epoch: 239 | Loss: 0.6004623174667358\n",
            "Epoch: 240 | Loss: 0.600225031375885\n",
            "Epoch: 241 | Loss: 0.5999879837036133\n",
            "Epoch: 242 | Loss: 0.5997510552406311\n",
            "Epoch: 243 | Loss: 0.5995144248008728\n",
            "Epoch: 244 | Loss: 0.5992777943611145\n",
            "Epoch: 245 | Loss: 0.5990414619445801\n",
            "Epoch: 246 | Loss: 0.5988051295280457\n",
            "Epoch: 247 | Loss: 0.5985692143440247\n",
            "Epoch: 248 | Loss: 0.5983333587646484\n",
            "Epoch: 249 | Loss: 0.5980976223945618\n",
            "Epoch: 250 | Loss: 0.5978620648384094\n",
            "Epoch: 251 | Loss: 0.5976267457008362\n",
            "Epoch: 252 | Loss: 0.5973915457725525\n",
            "Epoch: 253 | Loss: 0.5971565842628479\n",
            "Epoch: 254 | Loss: 0.5969217419624329\n",
            "Epoch: 255 | Loss: 0.5966870784759521\n",
            "Epoch: 256 | Loss: 0.5964525938034058\n",
            "Epoch: 257 | Loss: 0.5962182283401489\n",
            "Epoch: 258 | Loss: 0.5959839820861816\n",
            "Epoch: 259 | Loss: 0.595750093460083\n",
            "Epoch: 260 | Loss: 0.5955161452293396\n",
            "Epoch: 261 | Loss: 0.5952825546264648\n",
            "Epoch: 262 | Loss: 0.5950490236282349\n",
            "Epoch: 263 | Loss: 0.5948156118392944\n",
            "Epoch: 264 | Loss: 0.5945825576782227\n",
            "Epoch: 265 | Loss: 0.5943495631217957\n",
            "Epoch: 266 | Loss: 0.5941166281700134\n",
            "Epoch: 267 | Loss: 0.5938840508460999\n",
            "Epoch: 268 | Loss: 0.593651533126831\n",
            "Epoch: 269 | Loss: 0.593419075012207\n",
            "Epoch: 270 | Loss: 0.5931869745254517\n",
            "Epoch: 271 | Loss: 0.5929549932479858\n",
            "Epoch: 272 | Loss: 0.5927231311798096\n",
            "Epoch: 273 | Loss: 0.5924913883209229\n",
            "Epoch: 274 | Loss: 0.5922600030899048\n",
            "Epoch: 275 | Loss: 0.5920285582542419\n",
            "Epoch: 276 | Loss: 0.5917973518371582\n",
            "Epoch: 277 | Loss: 0.5915663242340088\n",
            "Epoch: 278 | Loss: 0.5913355946540833\n",
            "Epoch: 279 | Loss: 0.5911047458648682\n",
            "Epoch: 280 | Loss: 0.5908743143081665\n",
            "Epoch: 281 | Loss: 0.5906438827514648\n",
            "Epoch: 282 | Loss: 0.5904136896133423\n",
            "Epoch: 283 | Loss: 0.5901836156845093\n",
            "Epoch: 284 | Loss: 0.5899537205696106\n",
            "Epoch: 285 | Loss: 0.5897240042686462\n",
            "Epoch: 286 | Loss: 0.5894944071769714\n",
            "Epoch: 287 | Loss: 0.5892651081085205\n",
            "Epoch: 288 | Loss: 0.5890358686447144\n",
            "Epoch: 289 | Loss: 0.5888067483901978\n",
            "Epoch: 290 | Loss: 0.5885778069496155\n",
            "Epoch: 291 | Loss: 0.5883490443229675\n",
            "Epoch: 292 | Loss: 0.5881205201148987\n",
            "Epoch: 293 | Loss: 0.5878920555114746\n",
            "Epoch: 294 | Loss: 0.5876637697219849\n",
            "Epoch: 295 | Loss: 0.5874356627464294\n",
            "Epoch: 296 | Loss: 0.5872077345848083\n",
            "Epoch: 297 | Loss: 0.586979866027832\n",
            "Epoch: 298 | Loss: 0.5867523550987244\n",
            "Epoch: 299 | Loss: 0.5865248441696167\n",
            "Epoch: 300 | Loss: 0.5862975120544434\n",
            "Epoch: 301 | Loss: 0.5860704183578491\n",
            "Epoch: 302 | Loss: 0.5858433842658997\n",
            "Epoch: 303 | Loss: 0.5856165885925293\n",
            "Epoch: 304 | Loss: 0.5853898525238037\n",
            "Epoch: 305 | Loss: 0.585163414478302\n",
            "Epoch: 306 | Loss: 0.5849370360374451\n",
            "Epoch: 307 | Loss: 0.5847108364105225\n",
            "Epoch: 308 | Loss: 0.5844848155975342\n",
            "Epoch: 309 | Loss: 0.5842589139938354\n",
            "Epoch: 310 | Loss: 0.584033191204071\n",
            "Epoch: 311 | Loss: 0.583807647228241\n",
            "Epoch: 312 | Loss: 0.5835822820663452\n",
            "Epoch: 313 | Loss: 0.583357036113739\n",
            "Epoch: 314 | Loss: 0.5831319093704224\n",
            "Epoch: 315 | Loss: 0.58290696144104\n",
            "Epoch: 316 | Loss: 0.582682192325592\n",
            "Epoch: 317 | Loss: 0.5824576020240784\n",
            "Epoch: 318 | Loss: 0.582233190536499\n",
            "Epoch: 319 | Loss: 0.5820088386535645\n",
            "Epoch: 320 | Loss: 0.581784725189209\n",
            "Epoch: 321 | Loss: 0.5815607309341431\n",
            "Epoch: 322 | Loss: 0.5813369154930115\n",
            "Epoch: 323 | Loss: 0.5811132192611694\n",
            "Epoch: 324 | Loss: 0.5808897018432617\n",
            "Epoch: 325 | Loss: 0.5806664228439331\n",
            "Epoch: 326 | Loss: 0.580443263053894\n",
            "Epoch: 327 | Loss: 0.580220103263855\n",
            "Epoch: 328 | Loss: 0.5799972414970398\n",
            "Epoch: 329 | Loss: 0.5797745585441589\n",
            "Epoch: 330 | Loss: 0.5795519351959229\n",
            "Epoch: 331 | Loss: 0.5793294906616211\n",
            "Epoch: 332 | Loss: 0.5791072845458984\n",
            "Epoch: 333 | Loss: 0.5788851976394653\n",
            "Epoch: 334 | Loss: 0.578663170337677\n",
            "Epoch: 335 | Loss: 0.5784413814544678\n",
            "Epoch: 336 | Loss: 0.5782197713851929\n",
            "Epoch: 337 | Loss: 0.5779983997344971\n",
            "Epoch: 338 | Loss: 0.5777770280838013\n",
            "Epoch: 339 | Loss: 0.5775558352470398\n",
            "Epoch: 340 | Loss: 0.5773347616195679\n",
            "Epoch: 341 | Loss: 0.577113926410675\n",
            "Epoch: 342 | Loss: 0.576893150806427\n",
            "Epoch: 343 | Loss: 0.5766726732254028\n",
            "Epoch: 344 | Loss: 0.5764522552490234\n",
            "Epoch: 345 | Loss: 0.5762320160865784\n",
            "Epoch: 346 | Loss: 0.5760119557380676\n",
            "Epoch: 347 | Loss: 0.5757919549942017\n",
            "Epoch: 348 | Loss: 0.5755721926689148\n",
            "Epoch: 349 | Loss: 0.5753525495529175\n",
            "Epoch: 350 | Loss: 0.5751330852508545\n",
            "Epoch: 351 | Loss: 0.5749136805534363\n",
            "Epoch: 352 | Loss: 0.5746946334838867\n",
            "Epoch: 353 | Loss: 0.5744755864143372\n",
            "Epoch: 354 | Loss: 0.5742566585540771\n",
            "Epoch: 355 | Loss: 0.5740380883216858\n",
            "Epoch: 356 | Loss: 0.5738193988800049\n",
            "Epoch: 357 | Loss: 0.5736010670661926\n",
            "Epoch: 358 | Loss: 0.5733828544616699\n",
            "Epoch: 359 | Loss: 0.5731647610664368\n",
            "Epoch: 360 | Loss: 0.5729467272758484\n",
            "Epoch: 361 | Loss: 0.5727289319038391\n",
            "Epoch: 362 | Loss: 0.5725113749504089\n",
            "Epoch: 363 | Loss: 0.5722938179969788\n",
            "Epoch: 364 | Loss: 0.5720764398574829\n",
            "Epoch: 365 | Loss: 0.5718592405319214\n",
            "Epoch: 366 | Loss: 0.5716422200202942\n",
            "Epoch: 367 | Loss: 0.5714253783226013\n",
            "Epoch: 368 | Loss: 0.571208655834198\n",
            "Epoch: 369 | Loss: 0.5709919929504395\n",
            "Epoch: 370 | Loss: 0.5707756280899048\n",
            "Epoch: 371 | Loss: 0.5705593824386597\n",
            "Epoch: 372 | Loss: 0.5703431963920593\n",
            "Epoch: 373 | Loss: 0.5701272487640381\n",
            "Epoch: 374 | Loss: 0.5699114203453064\n",
            "Epoch: 375 | Loss: 0.569695770740509\n",
            "Epoch: 376 | Loss: 0.5694802403450012\n",
            "Epoch: 377 | Loss: 0.569264829158783\n",
            "Epoch: 378 | Loss: 0.569049596786499\n",
            "Epoch: 379 | Loss: 0.5688345432281494\n",
            "Epoch: 380 | Loss: 0.5686197280883789\n",
            "Epoch: 381 | Loss: 0.5684049129486084\n",
            "Epoch: 382 | Loss: 0.5681902766227722\n",
            "Epoch: 383 | Loss: 0.5679757595062256\n",
            "Epoch: 384 | Loss: 0.5677614808082581\n",
            "Epoch: 385 | Loss: 0.5675472617149353\n",
            "Epoch: 386 | Loss: 0.5673332214355469\n",
            "Epoch: 387 | Loss: 0.5671193599700928\n",
            "Epoch: 388 | Loss: 0.5669056177139282\n",
            "Epoch: 389 | Loss: 0.5666921138763428\n",
            "Epoch: 390 | Loss: 0.5664786696434021\n",
            "Epoch: 391 | Loss: 0.5662654042243958\n",
            "Epoch: 392 | Loss: 0.5660521984100342\n",
            "Epoch: 393 | Loss: 0.5658392906188965\n",
            "Epoch: 394 | Loss: 0.5656264424324036\n",
            "Epoch: 395 | Loss: 0.565413773059845\n",
            "Epoch: 396 | Loss: 0.5652012228965759\n",
            "Epoch: 397 | Loss: 0.5649887919425964\n",
            "Epoch: 398 | Loss: 0.564776599407196\n",
            "Epoch: 399 | Loss: 0.5645645260810852\n",
            "Epoch: 400 | Loss: 0.5643525123596191\n",
            "Epoch: 401 | Loss: 0.564140796661377\n",
            "Epoch: 402 | Loss: 0.5639291405677795\n",
            "Epoch: 403 | Loss: 0.5637176036834717\n",
            "Epoch: 404 | Loss: 0.5635062456130981\n",
            "Epoch: 405 | Loss: 0.5632950663566589\n",
            "Epoch: 406 | Loss: 0.5630840063095093\n",
            "Epoch: 407 | Loss: 0.5628730654716492\n",
            "Epoch: 408 | Loss: 0.5626623034477234\n",
            "Epoch: 409 | Loss: 0.5624517202377319\n",
            "Epoch: 410 | Loss: 0.56224125623703\n",
            "Epoch: 411 | Loss: 0.5620309114456177\n",
            "Epoch: 412 | Loss: 0.5618207454681396\n",
            "Epoch: 413 | Loss: 0.561610758304596\n",
            "Epoch: 414 | Loss: 0.5614008903503418\n",
            "Epoch: 415 | Loss: 0.5611910820007324\n",
            "Epoch: 416 | Loss: 0.5609815120697021\n",
            "Epoch: 417 | Loss: 0.5607720613479614\n",
            "Epoch: 418 | Loss: 0.560562789440155\n",
            "Epoch: 419 | Loss: 0.5603536367416382\n",
            "Epoch: 420 | Loss: 0.5601446032524109\n",
            "Epoch: 421 | Loss: 0.5599356889724731\n",
            "Epoch: 422 | Loss: 0.5597270131111145\n",
            "Epoch: 423 | Loss: 0.5595184564590454\n",
            "Epoch: 424 | Loss: 0.5593100786209106\n",
            "Epoch: 425 | Loss: 0.5591017007827759\n",
            "Epoch: 426 | Loss: 0.5588935613632202\n",
            "Epoch: 427 | Loss: 0.5586856007575989\n",
            "Epoch: 428 | Loss: 0.5584778785705566\n",
            "Epoch: 429 | Loss: 0.5582700967788696\n",
            "Epoch: 430 | Loss: 0.5580625534057617\n",
            "Epoch: 431 | Loss: 0.5578550696372986\n",
            "Epoch: 432 | Loss: 0.5576478242874146\n",
            "Epoch: 433 | Loss: 0.5574406981468201\n",
            "Epoch: 434 | Loss: 0.5572337508201599\n",
            "Epoch: 435 | Loss: 0.5570269227027893\n",
            "Epoch: 436 | Loss: 0.5568202137947083\n",
            "Epoch: 437 | Loss: 0.5566136837005615\n",
            "Epoch: 438 | Loss: 0.5564072728157043\n",
            "Epoch: 439 | Loss: 0.5562009811401367\n",
            "Epoch: 440 | Loss: 0.5559949278831482\n",
            "Epoch: 441 | Loss: 0.5557889938354492\n",
            "Epoch: 442 | Loss: 0.555583119392395\n",
            "Epoch: 443 | Loss: 0.5553774237632751\n",
            "Epoch: 444 | Loss: 0.5551718473434448\n",
            "Epoch: 445 | Loss: 0.5549664497375488\n",
            "Epoch: 446 | Loss: 0.5547612309455872\n",
            "Epoch: 447 | Loss: 0.5545560717582703\n",
            "Epoch: 448 | Loss: 0.5543510913848877\n",
            "Epoch: 449 | Loss: 0.5541463494300842\n",
            "Epoch: 450 | Loss: 0.5539416074752808\n",
            "Epoch: 451 | Loss: 0.5537371039390564\n",
            "Epoch: 452 | Loss: 0.5535327196121216\n",
            "Epoch: 453 | Loss: 0.5533283948898315\n",
            "Epoch: 454 | Loss: 0.5531243085861206\n",
            "Epoch: 455 | Loss: 0.5529203414916992\n",
            "Epoch: 456 | Loss: 0.5527164936065674\n",
            "Epoch: 457 | Loss: 0.5525127649307251\n",
            "Epoch: 458 | Loss: 0.5523092150688171\n",
            "Epoch: 459 | Loss: 0.5521059036254883\n",
            "Epoch: 460 | Loss: 0.5519025325775146\n",
            "Epoch: 461 | Loss: 0.5516994595527649\n",
            "Epoch: 462 | Loss: 0.5514964461326599\n",
            "Epoch: 463 | Loss: 0.5512936115264893\n",
            "Epoch: 464 | Loss: 0.5510909557342529\n",
            "Epoch: 465 | Loss: 0.5508883595466614\n",
            "Epoch: 466 | Loss: 0.5506859421730042\n",
            "Epoch: 467 | Loss: 0.5504835844039917\n",
            "Epoch: 468 | Loss: 0.5502815246582031\n",
            "Epoch: 469 | Loss: 0.5500794649124146\n",
            "Epoch: 470 | Loss: 0.5498775839805603\n",
            "Epoch: 471 | Loss: 0.5496759414672852\n",
            "Epoch: 472 | Loss: 0.5494743585586548\n",
            "Epoch: 473 | Loss: 0.549272894859314\n",
            "Epoch: 474 | Loss: 0.5490716099739075\n",
            "Epoch: 475 | Loss: 0.5488704442977905\n",
            "Epoch: 476 | Loss: 0.5486694574356079\n",
            "Epoch: 477 | Loss: 0.5484684705734253\n",
            "Epoch: 478 | Loss: 0.5482678413391113\n",
            "Epoch: 479 | Loss: 0.5480672121047974\n",
            "Epoch: 480 | Loss: 0.5478667616844177\n",
            "Epoch: 481 | Loss: 0.5476664900779724\n",
            "Epoch: 482 | Loss: 0.5474662780761719\n",
            "Epoch: 483 | Loss: 0.5472662448883057\n",
            "Epoch: 484 | Loss: 0.547066330909729\n",
            "Epoch: 485 | Loss: 0.5468665361404419\n",
            "Epoch: 486 | Loss: 0.5466669201850891\n",
            "Epoch: 487 | Loss: 0.5464674234390259\n",
            "Epoch: 488 | Loss: 0.546268105506897\n",
            "Epoch: 489 | Loss: 0.5460688471794128\n",
            "Epoch: 490 | Loss: 0.545869767665863\n",
            "Epoch: 491 | Loss: 0.5456708669662476\n",
            "Epoch: 492 | Loss: 0.5454720258712769\n",
            "Epoch: 493 | Loss: 0.5452734231948853\n",
            "Epoch: 494 | Loss: 0.5450748205184937\n",
            "Epoch: 495 | Loss: 0.5448765158653259\n",
            "Epoch: 496 | Loss: 0.5446782112121582\n",
            "Epoch: 497 | Loss: 0.5444800853729248\n",
            "Epoch: 498 | Loss: 0.5442821383476257\n",
            "Epoch: 499 | Loss: 0.5440843105316162\n",
            "Epoch: 500 | Loss: 0.5438866019248962\n",
            "Epoch: 501 | Loss: 0.5436890125274658\n",
            "Epoch: 502 | Loss: 0.5434916019439697\n",
            "Epoch: 503 | Loss: 0.5432943105697632\n",
            "Epoch: 504 | Loss: 0.543097198009491\n",
            "Epoch: 505 | Loss: 0.5429001450538635\n",
            "Epoch: 506 | Loss: 0.5427032709121704\n",
            "Epoch: 507 | Loss: 0.5425065159797668\n",
            "Epoch: 508 | Loss: 0.5423099994659424\n",
            "Epoch: 509 | Loss: 0.5421134829521179\n",
            "Epoch: 510 | Loss: 0.541917085647583\n",
            "Epoch: 511 | Loss: 0.5417209267616272\n",
            "Epoch: 512 | Loss: 0.5415248870849609\n",
            "Epoch: 513 | Loss: 0.5413289666175842\n",
            "Epoch: 514 | Loss: 0.5411331057548523\n",
            "Epoch: 515 | Loss: 0.5409374833106995\n",
            "Epoch: 516 | Loss: 0.5407419800758362\n",
            "Epoch: 517 | Loss: 0.5405465960502625\n",
            "Epoch: 518 | Loss: 0.5403513312339783\n",
            "Epoch: 519 | Loss: 0.5401561856269836\n",
            "Epoch: 520 | Loss: 0.5399612188339233\n",
            "Epoch: 521 | Loss: 0.5397663712501526\n",
            "Epoch: 522 | Loss: 0.5395717024803162\n",
            "Epoch: 523 | Loss: 0.5393770933151245\n",
            "Epoch: 524 | Loss: 0.5391826033592224\n",
            "Epoch: 525 | Loss: 0.5389883518218994\n",
            "Epoch: 526 | Loss: 0.5387941598892212\n",
            "Epoch: 527 | Loss: 0.5386002063751221\n",
            "Epoch: 528 | Loss: 0.538406252861023\n",
            "Epoch: 529 | Loss: 0.5382124781608582\n",
            "Epoch: 530 | Loss: 0.5380188226699829\n",
            "Epoch: 531 | Loss: 0.537825345993042\n",
            "Epoch: 532 | Loss: 0.5376320481300354\n",
            "Epoch: 533 | Loss: 0.5374387502670288\n",
            "Epoch: 534 | Loss: 0.5372456312179565\n",
            "Epoch: 535 | Loss: 0.5370526909828186\n",
            "Epoch: 536 | Loss: 0.5368598699569702\n",
            "Epoch: 537 | Loss: 0.5366671085357666\n",
            "Epoch: 538 | Loss: 0.5364745259284973\n",
            "Epoch: 539 | Loss: 0.5362821221351624\n",
            "Epoch: 540 | Loss: 0.5360898375511169\n",
            "Epoch: 541 | Loss: 0.5358976125717163\n",
            "Epoch: 542 | Loss: 0.5357056260108948\n",
            "Epoch: 543 | Loss: 0.5355137586593628\n",
            "Epoch: 544 | Loss: 0.5353219509124756\n",
            "Epoch: 545 | Loss: 0.5351303815841675\n",
            "Epoch: 546 | Loss: 0.5349388122558594\n",
            "Epoch: 547 | Loss: 0.5347474217414856\n",
            "Epoch: 548 | Loss: 0.5345561504364014\n",
            "Epoch: 549 | Loss: 0.5343651175498962\n",
            "Epoch: 550 | Loss: 0.5341741442680359\n",
            "Epoch: 551 | Loss: 0.5339832901954651\n",
            "Epoch: 552 | Loss: 0.5337925553321838\n",
            "Epoch: 553 | Loss: 0.5336020588874817\n",
            "Epoch: 554 | Loss: 0.5334115028381348\n",
            "Epoch: 555 | Loss: 0.5332212448120117\n",
            "Epoch: 556 | Loss: 0.5330310463905334\n",
            "Epoch: 557 | Loss: 0.5328410267829895\n",
            "Epoch: 558 | Loss: 0.5326510667800903\n",
            "Epoch: 559 | Loss: 0.5324612855911255\n",
            "Epoch: 560 | Loss: 0.5322716236114502\n",
            "Epoch: 561 | Loss: 0.5320820808410645\n",
            "Epoch: 562 | Loss: 0.5318925976753235\n",
            "Epoch: 563 | Loss: 0.5317034125328064\n",
            "Epoch: 564 | Loss: 0.5315141677856445\n",
            "Epoch: 565 | Loss: 0.5313252806663513\n",
            "Epoch: 566 | Loss: 0.5311363935470581\n",
            "Epoch: 567 | Loss: 0.5309475660324097\n",
            "Epoch: 568 | Loss: 0.5307589769363403\n",
            "Epoch: 569 | Loss: 0.5305705070495605\n",
            "Epoch: 570 | Loss: 0.5303820967674255\n",
            "Epoch: 571 | Loss: 0.5301939249038696\n",
            "Epoch: 572 | Loss: 0.5300058126449585\n",
            "Epoch: 573 | Loss: 0.5298178195953369\n",
            "Epoch: 574 | Loss: 0.5296300649642944\n",
            "Epoch: 575 | Loss: 0.5294422507286072\n",
            "Epoch: 576 | Loss: 0.5292547345161438\n",
            "Epoch: 577 | Loss: 0.5290672183036804\n",
            "Epoch: 578 | Loss: 0.5288798809051514\n",
            "Epoch: 579 | Loss: 0.5286927819252014\n",
            "Epoch: 580 | Loss: 0.5285056829452515\n",
            "Epoch: 581 | Loss: 0.5283187627792358\n",
            "Epoch: 582 | Loss: 0.5281318426132202\n",
            "Epoch: 583 | Loss: 0.5279452204704285\n",
            "Epoch: 584 | Loss: 0.5277586579322815\n",
            "Epoch: 585 | Loss: 0.5275722742080688\n",
            "Epoch: 586 | Loss: 0.5273858904838562\n",
            "Epoch: 587 | Loss: 0.5271998047828674\n",
            "Epoch: 588 | Loss: 0.5270137190818787\n",
            "Epoch: 589 | Loss: 0.526827871799469\n",
            "Epoch: 590 | Loss: 0.5266420245170593\n",
            "Epoch: 591 | Loss: 0.5264564752578735\n",
            "Epoch: 592 | Loss: 0.526270866394043\n",
            "Epoch: 593 | Loss: 0.5260854959487915\n",
            "Epoch: 594 | Loss: 0.5259002447128296\n",
            "Epoch: 595 | Loss: 0.5257150530815125\n",
            "Epoch: 596 | Loss: 0.5255299806594849\n",
            "Epoch: 597 | Loss: 0.5253451466560364\n",
            "Epoch: 598 | Loss: 0.5251603722572327\n",
            "Epoch: 599 | Loss: 0.5249757170677185\n",
            "Epoch: 600 | Loss: 0.5247912406921387\n",
            "Epoch: 601 | Loss: 0.5246068239212036\n",
            "Epoch: 602 | Loss: 0.5244225859642029\n",
            "Epoch: 603 | Loss: 0.5242384076118469\n",
            "Epoch: 604 | Loss: 0.5240544676780701\n",
            "Epoch: 605 | Loss: 0.5238705277442932\n",
            "Epoch: 606 | Loss: 0.5236868262290955\n",
            "Epoch: 607 | Loss: 0.5235031843185425\n",
            "Epoch: 608 | Loss: 0.5233197212219238\n",
            "Epoch: 609 | Loss: 0.52313631772995\n",
            "Epoch: 610 | Loss: 0.5229530930519104\n",
            "Epoch: 611 | Loss: 0.5227699875831604\n",
            "Epoch: 612 | Loss: 0.5225869417190552\n",
            "Epoch: 613 | Loss: 0.5224040746688843\n",
            "Epoch: 614 | Loss: 0.5222213268280029\n",
            "Epoch: 615 | Loss: 0.5220387578010559\n",
            "Epoch: 616 | Loss: 0.5218562483787537\n",
            "Epoch: 617 | Loss: 0.5216737985610962\n",
            "Epoch: 618 | Loss: 0.5214916467666626\n",
            "Epoch: 619 | Loss: 0.521309494972229\n",
            "Epoch: 620 | Loss: 0.5211275219917297\n",
            "Epoch: 621 | Loss: 0.5209456086158752\n",
            "Epoch: 622 | Loss: 0.5207638740539551\n",
            "Epoch: 623 | Loss: 0.5205821990966797\n",
            "Epoch: 624 | Loss: 0.5204007625579834\n",
            "Epoch: 625 | Loss: 0.5202193856239319\n",
            "Epoch: 626 | Loss: 0.5200381278991699\n",
            "Epoch: 627 | Loss: 0.5198570489883423\n",
            "Epoch: 628 | Loss: 0.5196760892868042\n",
            "Epoch: 629 | Loss: 0.5194951891899109\n",
            "Epoch: 630 | Loss: 0.5193143486976624\n",
            "Epoch: 631 | Loss: 0.5191337466239929\n",
            "Epoch: 632 | Loss: 0.518953263759613\n",
            "Epoch: 633 | Loss: 0.5187729001045227\n",
            "Epoch: 634 | Loss: 0.5185925960540771\n",
            "Epoch: 635 | Loss: 0.5184124708175659\n",
            "Epoch: 636 | Loss: 0.5182324647903442\n",
            "Epoch: 637 | Loss: 0.5180525779724121\n",
            "Epoch: 638 | Loss: 0.5178727507591248\n",
            "Epoch: 639 | Loss: 0.5176932215690613\n",
            "Epoch: 640 | Loss: 0.5175136923789978\n",
            "Epoch: 641 | Loss: 0.5173342227935791\n",
            "Epoch: 642 | Loss: 0.5171549916267395\n",
            "Epoch: 643 | Loss: 0.5169758796691895\n",
            "Epoch: 644 | Loss: 0.5167967677116394\n",
            "Epoch: 645 | Loss: 0.5166178345680237\n",
            "Epoch: 646 | Loss: 0.5164391398429871\n",
            "Epoch: 647 | Loss: 0.5162605047225952\n",
            "Epoch: 648 | Loss: 0.5160819292068481\n",
            "Epoch: 649 | Loss: 0.5159035325050354\n",
            "Epoch: 650 | Loss: 0.5157251954078674\n",
            "Epoch: 651 | Loss: 0.5155470371246338\n",
            "Epoch: 652 | Loss: 0.5153689980506897\n",
            "Epoch: 653 | Loss: 0.5151910185813904\n",
            "Epoch: 654 | Loss: 0.5150132775306702\n",
            "Epoch: 655 | Loss: 0.5148354768753052\n",
            "Epoch: 656 | Loss: 0.5146579742431641\n",
            "Epoch: 657 | Loss: 0.5144805312156677\n",
            "Epoch: 658 | Loss: 0.5143032073974609\n",
            "Epoch: 659 | Loss: 0.5141260623931885\n",
            "Epoch: 660 | Loss: 0.5139489769935608\n",
            "Epoch: 661 | Loss: 0.5137719511985779\n",
            "Epoch: 662 | Loss: 0.5135951638221741\n",
            "Epoch: 663 | Loss: 0.5134183764457703\n",
            "Epoch: 664 | Loss: 0.5132418274879456\n",
            "Epoch: 665 | Loss: 0.5130653381347656\n",
            "Epoch: 666 | Loss: 0.5128889679908752\n",
            "Epoch: 667 | Loss: 0.5127127766609192\n",
            "Epoch: 668 | Loss: 0.5125366449356079\n",
            "Epoch: 669 | Loss: 0.5123606324195862\n",
            "Epoch: 670 | Loss: 0.512184739112854\n",
            "Epoch: 671 | Loss: 0.5120090246200562\n",
            "Epoch: 672 | Loss: 0.5118333697319031\n",
            "Epoch: 673 | Loss: 0.5116578340530396\n",
            "Epoch: 674 | Loss: 0.5114824771881104\n",
            "Epoch: 675 | Loss: 0.5113071799278259\n",
            "Epoch: 676 | Loss: 0.511132001876831\n",
            "Epoch: 677 | Loss: 0.5109570026397705\n",
            "Epoch: 678 | Loss: 0.51078200340271\n",
            "Epoch: 679 | Loss: 0.5106071829795837\n",
            "Epoch: 680 | Loss: 0.5104324817657471\n",
            "Epoch: 681 | Loss: 0.5102580189704895\n",
            "Epoch: 682 | Loss: 0.5100836157798767\n",
            "Epoch: 683 | Loss: 0.5099092721939087\n",
            "Epoch: 684 | Loss: 0.5097349882125854\n",
            "Epoch: 685 | Loss: 0.5095609426498413\n",
            "Epoch: 686 | Loss: 0.5093870162963867\n",
            "Epoch: 687 | Loss: 0.5092131495475769\n",
            "Epoch: 688 | Loss: 0.5090394020080566\n",
            "Epoch: 689 | Loss: 0.5088657140731812\n",
            "Epoch: 690 | Loss: 0.5086922645568848\n",
            "Epoch: 691 | Loss: 0.5085188746452332\n",
            "Epoch: 692 | Loss: 0.5083455443382263\n",
            "Epoch: 693 | Loss: 0.5081725120544434\n",
            "Epoch: 694 | Loss: 0.5079994201660156\n",
            "Epoch: 695 | Loss: 0.507826566696167\n",
            "Epoch: 696 | Loss: 0.5076537728309631\n",
            "Epoch: 697 | Loss: 0.5074810981750488\n",
            "Epoch: 698 | Loss: 0.5073084831237793\n",
            "Epoch: 699 | Loss: 0.5071361064910889\n",
            "Epoch: 700 | Loss: 0.5069637298583984\n",
            "Epoch: 701 | Loss: 0.5067915916442871\n",
            "Epoch: 702 | Loss: 0.5066195130348206\n",
            "Epoch: 703 | Loss: 0.506447434425354\n",
            "Epoch: 704 | Loss: 0.5062756538391113\n",
            "Epoch: 705 | Loss: 0.5061038732528687\n",
            "Epoch: 706 | Loss: 0.5059322118759155\n",
            "Epoch: 707 | Loss: 0.5057607889175415\n",
            "Epoch: 708 | Loss: 0.5055893659591675\n",
            "Epoch: 709 | Loss: 0.5054181218147278\n",
            "Epoch: 710 | Loss: 0.5052469968795776\n",
            "Epoch: 711 | Loss: 0.5050759315490723\n",
            "Epoch: 712 | Loss: 0.5049049854278564\n",
            "Epoch: 713 | Loss: 0.5047341585159302\n",
            "Epoch: 714 | Loss: 0.5045634508132935\n",
            "Epoch: 715 | Loss: 0.5043929219245911\n",
            "Epoch: 716 | Loss: 0.5042224526405334\n",
            "Epoch: 717 | Loss: 0.5040520429611206\n",
            "Epoch: 718 | Loss: 0.5038818717002869\n",
            "Epoch: 719 | Loss: 0.5037117600440979\n",
            "Epoch: 720 | Loss: 0.5035417675971985\n",
            "Epoch: 721 | Loss: 0.5033718347549438\n",
            "Epoch: 722 | Loss: 0.5032021403312683\n",
            "Epoch: 723 | Loss: 0.5030325055122375\n",
            "Epoch: 724 | Loss: 0.5028629302978516\n",
            "Epoch: 725 | Loss: 0.5026935338973999\n",
            "Epoch: 726 | Loss: 0.502524197101593\n",
            "Epoch: 727 | Loss: 0.5023549795150757\n",
            "Epoch: 728 | Loss: 0.5021858811378479\n",
            "Epoch: 729 | Loss: 0.5020169019699097\n",
            "Epoch: 730 | Loss: 0.5018481016159058\n",
            "Epoch: 731 | Loss: 0.5016793012619019\n",
            "Epoch: 732 | Loss: 0.5015107989311218\n",
            "Epoch: 733 | Loss: 0.501342236995697\n",
            "Epoch: 734 | Loss: 0.5011737942695618\n",
            "Epoch: 735 | Loss: 0.5010054707527161\n",
            "Epoch: 736 | Loss: 0.5008373856544495\n",
            "Epoch: 737 | Loss: 0.5006693601608276\n",
            "Epoch: 738 | Loss: 0.5005013942718506\n",
            "Epoch: 739 | Loss: 0.5003335475921631\n",
            "Epoch: 740 | Loss: 0.5001658797264099\n",
            "Epoch: 741 | Loss: 0.4999982714653015\n",
            "Epoch: 742 | Loss: 0.49983078241348267\n",
            "Epoch: 743 | Loss: 0.49966341257095337\n",
            "Epoch: 744 | Loss: 0.4994962215423584\n",
            "Epoch: 745 | Loss: 0.4993290305137634\n",
            "Epoch: 746 | Loss: 0.4991619884967804\n",
            "Epoch: 747 | Loss: 0.4989950656890869\n",
            "Epoch: 748 | Loss: 0.49882829189300537\n",
            "Epoch: 749 | Loss: 0.4986615777015686\n",
            "Epoch: 750 | Loss: 0.4984950125217438\n",
            "Epoch: 751 | Loss: 0.4983285367488861\n",
            "Epoch: 752 | Loss: 0.4981621205806732\n",
            "Epoch: 753 | Loss: 0.49799591302871704\n",
            "Epoch: 754 | Loss: 0.497829794883728\n",
            "Epoch: 755 | Loss: 0.4976637065410614\n",
            "Epoch: 756 | Loss: 0.4974977970123291\n",
            "Epoch: 757 | Loss: 0.49733200669288635\n",
            "Epoch: 758 | Loss: 0.49716633558273315\n",
            "Epoch: 759 | Loss: 0.4970008134841919\n",
            "Epoch: 760 | Loss: 0.49683529138565063\n",
            "Epoch: 761 | Loss: 0.4966699481010437\n",
            "Epoch: 762 | Loss: 0.4965047240257263\n",
            "Epoch: 763 | Loss: 0.4963395893573761\n",
            "Epoch: 764 | Loss: 0.49617457389831543\n",
            "Epoch: 765 | Loss: 0.49600961804389954\n",
            "Epoch: 766 | Loss: 0.4958449602127075\n",
            "Epoch: 767 | Loss: 0.49568018317222595\n",
            "Epoch: 768 | Loss: 0.4955156445503235\n",
            "Epoch: 769 | Loss: 0.4953511357307434\n",
            "Epoch: 770 | Loss: 0.49518686532974243\n",
            "Epoch: 771 | Loss: 0.4950225353240967\n",
            "Epoch: 772 | Loss: 0.49485844373703003\n",
            "Epoch: 773 | Loss: 0.49469438195228577\n",
            "Epoch: 774 | Loss: 0.4945305287837982\n",
            "Epoch: 775 | Loss: 0.49436670541763306\n",
            "Epoch: 776 | Loss: 0.49420300126075745\n",
            "Epoch: 777 | Loss: 0.49403947591781616\n",
            "Epoch: 778 | Loss: 0.49387601017951965\n",
            "Epoch: 779 | Loss: 0.4937126040458679\n",
            "Epoch: 780 | Loss: 0.4935493469238281\n",
            "Epoch: 781 | Loss: 0.4933862090110779\n",
            "Epoch: 782 | Loss: 0.4932232201099396\n",
            "Epoch: 783 | Loss: 0.49306032061576843\n",
            "Epoch: 784 | Loss: 0.4928974211215973\n",
            "Epoch: 785 | Loss: 0.49273476004600525\n",
            "Epoch: 786 | Loss: 0.4925721287727356\n",
            "Epoch: 787 | Loss: 0.49240967631340027\n",
            "Epoch: 788 | Loss: 0.49224725365638733\n",
            "Epoch: 789 | Loss: 0.4920850396156311\n",
            "Epoch: 790 | Loss: 0.4919228255748749\n",
            "Epoch: 791 | Loss: 0.491760790348053\n",
            "Epoch: 792 | Loss: 0.49159887433052063\n",
            "Epoch: 793 | Loss: 0.4914371073246002\n",
            "Epoch: 794 | Loss: 0.49127528071403503\n",
            "Epoch: 795 | Loss: 0.49111369252204895\n",
            "Epoch: 796 | Loss: 0.4909522533416748\n",
            "Epoch: 797 | Loss: 0.49079084396362305\n",
            "Epoch: 798 | Loss: 0.49062952399253845\n",
            "Epoch: 799 | Loss: 0.4904683828353882\n",
            "Epoch: 800 | Loss: 0.4903072714805603\n",
            "Epoch: 801 | Loss: 0.49014630913734436\n",
            "Epoch: 802 | Loss: 0.48998546600341797\n",
            "Epoch: 803 | Loss: 0.48982468247413635\n",
            "Epoch: 804 | Loss: 0.48966410756111145\n",
            "Epoch: 805 | Loss: 0.48950356245040894\n",
            "Epoch: 806 | Loss: 0.4893431067466736\n",
            "Epoch: 807 | Loss: 0.48918282985687256\n",
            "Epoch: 808 | Loss: 0.48902255296707153\n",
            "Epoch: 809 | Loss: 0.488862544298172\n",
            "Epoch: 810 | Loss: 0.4887024760246277\n",
            "Epoch: 811 | Loss: 0.4885425865650177\n",
            "Epoch: 812 | Loss: 0.48838284611701965\n",
            "Epoch: 813 | Loss: 0.488223135471344\n",
            "Epoch: 814 | Loss: 0.4880635440349579\n",
            "Epoch: 815 | Loss: 0.4879041612148285\n",
            "Epoch: 816 | Loss: 0.4877447485923767\n",
            "Epoch: 817 | Loss: 0.48758551478385925\n",
            "Epoch: 818 | Loss: 0.4874263405799866\n",
            "Epoch: 819 | Loss: 0.48726731538772583\n",
            "Epoch: 820 | Loss: 0.487108439207077\n",
            "Epoch: 821 | Loss: 0.4869495928287506\n",
            "Epoch: 822 | Loss: 0.48679083585739136\n",
            "Epoch: 823 | Loss: 0.48663225769996643\n",
            "Epoch: 824 | Loss: 0.48647376894950867\n",
            "Epoch: 825 | Loss: 0.48631536960601807\n",
            "Epoch: 826 | Loss: 0.486157089471817\n",
            "Epoch: 827 | Loss: 0.48599886894226074\n",
            "Epoch: 828 | Loss: 0.4858408570289612\n",
            "Epoch: 829 | Loss: 0.48568281531333923\n",
            "Epoch: 830 | Loss: 0.4855249524116516\n",
            "Epoch: 831 | Loss: 0.48536717891693115\n",
            "Epoch: 832 | Loss: 0.48520955443382263\n",
            "Epoch: 833 | Loss: 0.4850519895553589\n",
            "Epoch: 834 | Loss: 0.4848945438861847\n",
            "Epoch: 835 | Loss: 0.48473721742630005\n",
            "Epoch: 836 | Loss: 0.4845799207687378\n",
            "Epoch: 837 | Loss: 0.4844227731227875\n",
            "Epoch: 838 | Loss: 0.4842657446861267\n",
            "Epoch: 839 | Loss: 0.4841088652610779\n",
            "Epoch: 840 | Loss: 0.48395201563835144\n",
            "Epoch: 841 | Loss: 0.48379525542259216\n",
            "Epoch: 842 | Loss: 0.4836387038230896\n",
            "Epoch: 843 | Loss: 0.4834822118282318\n",
            "Epoch: 844 | Loss: 0.4833257794380188\n",
            "Epoch: 845 | Loss: 0.48316943645477295\n",
            "Epoch: 846 | Loss: 0.48301324248313904\n",
            "Epoch: 847 | Loss: 0.4828571677207947\n",
            "Epoch: 848 | Loss: 0.4827011823654175\n",
            "Epoch: 849 | Loss: 0.4825453460216522\n",
            "Epoch: 850 | Loss: 0.48238953948020935\n",
            "Epoch: 851 | Loss: 0.4822339415550232\n",
            "Epoch: 852 | Loss: 0.48207834362983704\n",
            "Epoch: 853 | Loss: 0.48192286491394043\n",
            "Epoch: 854 | Loss: 0.4817675054073334\n",
            "Epoch: 855 | Loss: 0.4816122055053711\n",
            "Epoch: 856 | Loss: 0.48145705461502075\n",
            "Epoch: 857 | Loss: 0.48130202293395996\n",
            "Epoch: 858 | Loss: 0.48114705085754395\n",
            "Epoch: 859 | Loss: 0.48099225759506226\n",
            "Epoch: 860 | Loss: 0.48083749413490295\n",
            "Epoch: 861 | Loss: 0.4806828200817108\n",
            "Epoch: 862 | Loss: 0.4805282652378082\n",
            "Epoch: 863 | Loss: 0.4803738594055176\n",
            "Epoch: 864 | Loss: 0.4802194833755493\n",
            "Epoch: 865 | Loss: 0.4800652861595154\n",
            "Epoch: 866 | Loss: 0.479911208152771\n",
            "Epoch: 867 | Loss: 0.4797571003437042\n",
            "Epoch: 868 | Loss: 0.47960320115089417\n",
            "Epoch: 869 | Loss: 0.47944939136505127\n",
            "Epoch: 870 | Loss: 0.47929564118385315\n",
            "Epoch: 871 | Loss: 0.4791420102119446\n",
            "Epoch: 872 | Loss: 0.47898855805397034\n",
            "Epoch: 873 | Loss: 0.4788351356983185\n",
            "Epoch: 874 | Loss: 0.4786818027496338\n",
            "Epoch: 875 | Loss: 0.47852861881256104\n",
            "Epoch: 876 | Loss: 0.4783754050731659\n",
            "Epoch: 877 | Loss: 0.47822242975234985\n",
            "Epoch: 878 | Loss: 0.4780695140361786\n",
            "Epoch: 879 | Loss: 0.4779167175292969\n",
            "Epoch: 880 | Loss: 0.4777640402317047\n",
            "Epoch: 881 | Loss: 0.4776114225387573\n",
            "Epoch: 882 | Loss: 0.4774588942527771\n",
            "Epoch: 883 | Loss: 0.4773064851760864\n",
            "Epoch: 884 | Loss: 0.4771541953086853\n",
            "Epoch: 885 | Loss: 0.47700196504592896\n",
            "Epoch: 886 | Loss: 0.47684991359710693\n",
            "Epoch: 887 | Loss: 0.4766978621482849\n",
            "Epoch: 888 | Loss: 0.4765460193157196\n",
            "Epoch: 889 | Loss: 0.4763942062854767\n",
            "Epoch: 890 | Loss: 0.4762425124645233\n",
            "Epoch: 891 | Loss: 0.4760909080505371\n",
            "Epoch: 892 | Loss: 0.47593939304351807\n",
            "Epoch: 893 | Loss: 0.4757879972457886\n",
            "Epoch: 894 | Loss: 0.47563672065734863\n",
            "Epoch: 895 | Loss: 0.47548553347587585\n",
            "Epoch: 896 | Loss: 0.47533440589904785\n",
            "Epoch: 897 | Loss: 0.4751834273338318\n",
            "Epoch: 898 | Loss: 0.4750325083732605\n",
            "Epoch: 899 | Loss: 0.47488167881965637\n",
            "Epoch: 900 | Loss: 0.47473105788230896\n",
            "Epoch: 901 | Loss: 0.47458043694496155\n",
            "Epoch: 902 | Loss: 0.4744298756122589\n",
            "Epoch: 903 | Loss: 0.474279522895813\n",
            "Epoch: 904 | Loss: 0.47412925958633423\n",
            "Epoch: 905 | Loss: 0.47397899627685547\n",
            "Epoch: 906 | Loss: 0.47382891178131104\n",
            "Epoch: 907 | Loss: 0.47367897629737854\n",
            "Epoch: 908 | Loss: 0.47352904081344604\n",
            "Epoch: 909 | Loss: 0.4733791947364807\n",
            "Epoch: 910 | Loss: 0.4732295274734497\n",
            "Epoch: 911 | Loss: 0.4730798900127411\n",
            "Epoch: 912 | Loss: 0.4729304313659668\n",
            "Epoch: 913 | Loss: 0.4727810323238373\n",
            "Epoch: 914 | Loss: 0.47263166308403015\n",
            "Epoch: 915 | Loss: 0.47248250246047974\n",
            "Epoch: 916 | Loss: 0.4723334014415741\n",
            "Epoch: 917 | Loss: 0.47218436002731323\n",
            "Epoch: 918 | Loss: 0.4720354378223419\n",
            "Epoch: 919 | Loss: 0.47188660502433777\n",
            "Epoch: 920 | Loss: 0.4717378616333008\n",
            "Epoch: 921 | Loss: 0.47158926725387573\n",
            "Epoch: 922 | Loss: 0.47144076228141785\n",
            "Epoch: 923 | Loss: 0.47129228711128235\n",
            "Epoch: 924 | Loss: 0.4711439609527588\n",
            "Epoch: 925 | Loss: 0.47099569439888\n",
            "Epoch: 926 | Loss: 0.47084760665893555\n",
            "Epoch: 927 | Loss: 0.4706995487213135\n",
            "Epoch: 928 | Loss: 0.47055160999298096\n",
            "Epoch: 929 | Loss: 0.4704037606716156\n",
            "Epoch: 930 | Loss: 0.4702560007572174\n",
            "Epoch: 931 | Loss: 0.47010838985443115\n",
            "Epoch: 932 | Loss: 0.4699608385562897\n",
            "Epoch: 933 | Loss: 0.46981334686279297\n",
            "Epoch: 934 | Loss: 0.4696660041809082\n",
            "Epoch: 935 | Loss: 0.469518780708313\n",
            "Epoch: 936 | Loss: 0.4693715572357178\n",
            "Epoch: 937 | Loss: 0.4692245125770569\n",
            "Epoch: 938 | Loss: 0.46907752752304077\n",
            "Epoch: 939 | Loss: 0.4689306914806366\n",
            "Epoch: 940 | Loss: 0.4687838852405548\n",
            "Epoch: 941 | Loss: 0.4686371982097626\n",
            "Epoch: 942 | Loss: 0.4684906005859375\n",
            "Epoch: 943 | Loss: 0.4683440923690796\n",
            "Epoch: 944 | Loss: 0.4681977331638336\n",
            "Epoch: 945 | Loss: 0.4680514335632324\n",
            "Epoch: 946 | Loss: 0.467905193567276\n",
            "Epoch: 947 | Loss: 0.4677591025829315\n",
            "Epoch: 948 | Loss: 0.4676130414009094\n",
            "Epoch: 949 | Loss: 0.46746718883514404\n",
            "Epoch: 950 | Loss: 0.46732133626937866\n",
            "Epoch: 951 | Loss: 0.4671756327152252\n",
            "Epoch: 952 | Loss: 0.46702995896339417\n",
            "Epoch: 953 | Loss: 0.46688440442085266\n",
            "Epoch: 954 | Loss: 0.46673905849456787\n",
            "Epoch: 955 | Loss: 0.4665937125682831\n",
            "Epoch: 956 | Loss: 0.46644842624664307\n",
            "Epoch: 957 | Loss: 0.4663032591342926\n",
            "Epoch: 958 | Loss: 0.4661582410335541\n",
            "Epoch: 959 | Loss: 0.46601325273513794\n",
            "Epoch: 960 | Loss: 0.46586841344833374\n",
            "Epoch: 961 | Loss: 0.46572360396385193\n",
            "Epoch: 962 | Loss: 0.46557900309562683\n",
            "Epoch: 963 | Loss: 0.46543440222740173\n",
            "Epoch: 964 | Loss: 0.4652899205684662\n",
            "Epoch: 965 | Loss: 0.465145468711853\n",
            "Epoch: 966 | Loss: 0.4650011658668518\n",
            "Epoch: 967 | Loss: 0.4648570120334625\n",
            "Epoch: 968 | Loss: 0.46471279859542847\n",
            "Epoch: 969 | Loss: 0.4645687937736511\n",
            "Epoch: 970 | Loss: 0.46442490816116333\n",
            "Epoch: 971 | Loss: 0.4642810523509979\n",
            "Epoch: 972 | Loss: 0.46413737535476685\n",
            "Epoch: 973 | Loss: 0.46399375796318054\n",
            "Epoch: 974 | Loss: 0.4638501703739166\n",
            "Epoch: 975 | Loss: 0.46370673179626465\n",
            "Epoch: 976 | Loss: 0.46356332302093506\n",
            "Epoch: 977 | Loss: 0.4634201228618622\n",
            "Epoch: 978 | Loss: 0.4632769227027893\n",
            "Epoch: 979 | Loss: 0.4631338119506836\n",
            "Epoch: 980 | Loss: 0.4629908502101898\n",
            "Epoch: 981 | Loss: 0.4628479480743408\n",
            "Epoch: 982 | Loss: 0.4627051055431366\n",
            "Epoch: 983 | Loss: 0.4625623822212219\n",
            "Epoch: 984 | Loss: 0.4624197781085968\n",
            "Epoch: 985 | Loss: 0.46227726340293884\n",
            "Epoch: 986 | Loss: 0.46213483810424805\n",
            "Epoch: 987 | Loss: 0.4619925022125244\n",
            "Epoch: 988 | Loss: 0.46185025572776794\n",
            "Epoch: 989 | Loss: 0.461708128452301\n",
            "Epoch: 990 | Loss: 0.4615660607814789\n",
            "Epoch: 991 | Loss: 0.4614240527153015\n",
            "Epoch: 992 | Loss: 0.46128225326538086\n",
            "Epoch: 993 | Loss: 0.4611404240131378\n",
            "Epoch: 994 | Loss: 0.4609987437725067\n",
            "Epoch: 995 | Loss: 0.4608571231365204\n",
            "Epoch: 996 | Loss: 0.460715651512146\n",
            "Epoch: 997 | Loss: 0.460574209690094\n",
            "Epoch: 998 | Loss: 0.46043288707733154\n",
            "Epoch: 999 | Loss: 0.46029165387153625\n",
            "Epoch: 1000 | Loss: 0.4601505398750305\n",
            "Prediction after 1 hour of training: 0.3856 | Above 50%: False\n",
            "Prediction after 7 hour of training: 0.9692 | Above 50%: True\n",
            "Prediction after 20 hour of training: 1.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpDCssRNkod7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5014e34-7659-41b4-c18d-8276d63e398a"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "def loss(x,y):\n",
        "  y_pred=forward(x)\n",
        "  return (y_pred-y)**2\n",
        "w=torch.tensor([1.0],requires_grad=True)\n",
        "l=loss(x=2, y=4)\n",
        "l.backward()\n",
        "print(w.grad.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0je8Jq-sO9_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9111318-0f33-4caf-ee9a-8b043ac2e811"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def forward(x):\n",
        "  return x*x*w2+x*w1+b\n",
        "def loss(x,y):\n",
        "  y_pred=forward(x)\n",
        "  return (y_pred-y)**2\n",
        "b=torch.tensor([2.0],requires_grad=True)\n",
        "w1=torch.tensor([1.0],requires_grad=True)\n",
        "w2=torch.tensor([2.0],requires_grad=True)\n",
        "l=loss(x=1, y=2)\n",
        "l.backward()\n",
        "print(w1.grad.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz2yvp73RV1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b381be13-911a-41aa-dd20-4978088dc59f"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def forward(x):\n",
        "  return x*x*w2+x*w1+b\n",
        "def loss(x,y):\n",
        "  y_pred=forward(x)\n",
        "  return (y_pred-y)**2\n",
        "b=torch.tensor([2.0],requires_grad=True)\n",
        "w1=torch.tensor([1.0],requires_grad=True)\n",
        "w2=torch.tensor([2.0],requires_grad=True)\n",
        "l=loss(x=1, y=2)\n",
        "l.backward()\n",
        "print(w2.grad.data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX_oeEynUkA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "a20a947f-a1af-4e92-ab58-45f8833f112b"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.974133491516113\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f18c5c2f990>]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylrtSNlHmheu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "8de4d674-baf3-482d-9b54-f052baacc76b"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.Adagrad(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 -0.31801313161849976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f06015d99d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYX9BY5Rp-vh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "28fe9a30-e4c9-4878-f302-5133a43be7fd"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.122629165649414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0601558a50>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1DvPxS2p_g0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "ebaa5102-689a-4d54-976e-436ea4850f93"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.Adamax(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 6.636087417602539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f06014cdbd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StmT_Cxpp_jb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "d9834c15-550f-4687-958c-3c7e80d37380"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.ASGD(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.984743595123291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f060144bbd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9RQq7gNp_ls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "f84845ae-222c-47fa-fe89-745573f090c1"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.LBFGS(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "def closure():\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  return loss\n",
        "for epoch in range(500):\n",
        "  optimizer.step(closure)\n",
        "  loss=closure()\n",
        "  losses.append(loss)\n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.999974250793457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1c5ba3c9d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5iMp39up_q7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "b8cc88c5-f1a0-44f5-8bf8-d1ed746643d1"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.RMSprop(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.999117851257324\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1c5c2f9510>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeC1YM6Up_te",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "41af67f3-1916-41c8-9421-8a82bf139139"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.Rprop(model.parameters(), lr= 0.01)\n",
        "losses=[]\n",
        "for epoch in range(500):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  losses.append(loss)\n",
        "  optimizer.step() \n",
        "epoch=range(1,501)\n",
        "hour_var = tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"Prediction (after training\", 4, model(hour_var).item() )\n",
        "plt.plot(epoch,losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training 4 7.999993324279785\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1c5bdf1450>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKYM7NKTtIjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8aa759-05a8-4cdb-f7ad-5943b610ef99"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4814285337924957\n",
            "Prediction after 1 hour of training: 0.4107 | Above 50%: False\n",
            "Prediction after 7 hour of training: 0.9606 | Above 50%: True\n",
            "Prediction after 20 hour of training: 1.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghF6eyqtwBSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a5c86c-43aa-4118-acaa-a8e6089e54d0"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import relu\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=relu(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.042622651904821396\n",
            "Prediction after 1 hour of training: 0.0000 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.5055 | Above 50%: True\n",
            "Prediction after 20 hour of training: 8.4541 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2AgIFNJwBcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072ae756-bd78-4d7c-e642-20c8b9df2cf3"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "relu6=nn.ReLU6()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=relu6(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04720795527100563\n",
            "Prediction after 1 hour of training: 0.0000 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.2787 | Above 50%: True\n",
            "Prediction after 20 hour of training: 6.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENz2vOahyNxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce59a61e-2d0a-4cc5-af67-162973aa5506"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "elu=nn.ELU()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=elu(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.049808576703071594\n",
            "Prediction after 1 hour of training: -0.0909 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.2855 | Above 50%: True\n",
            "Prediction after 20 hour of training: 7.4439 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPKcrORO4MnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66046923-ae3c-4ed9-c216-b06aa1c3b7f3"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "selu=nn.SELU()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=selu(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05194928124547005\n",
            "Prediction after 1 hour of training: -0.0690 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.2038 | Above 50%: True\n",
            "Prediction after 20 hour of training: 7.0698 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e977BaqX4Mpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4807ac-2671-4794-a176-2a28ddbe86e4"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "prelu=nn.PReLU()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=prelu(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.043432656675577164\n",
            "Prediction after 1 hour of training: -0.0839 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.6780 | Above 50%: True\n",
            "Prediction after 20 hour of training: 9.2072 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsFt_bb64MsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f216dbf-f51d-4c76-f450-6b5ae9f8c442"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "leakyrelu=nn.LeakyReLU()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=leakyrelu(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5190020203590393\n",
            "Prediction after 1 hour of training: -0.0076 | Above 50%: False\n",
            "Prediction after 7 hour of training: -0.0344 | Above 50%: False\n",
            "Prediction after 20 hour of training: -0.0926 | Above 50%: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2XU9m-W4MvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda48f3d-56d2-4169-e55a-c31bc54a8966"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "threshold=nn.Threshold(0.001,0.5)\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=threshold(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11054679751396179\n",
            "Prediction after 1 hour of training: 0.5000 | Above 50%: False\n",
            "Prediction after 7 hour of training: 2.2504 | Above 50%: True\n",
            "Prediction after 20 hour of training: 7.3259 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5r4G5fr4Mxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82e16e4-711b-4cab-f1f0-6b619e0f4c1b"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "hardtanh=nn.Hardtanh()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=hardtanh(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47037845849990845\n",
            "Prediction after 1 hour of training: -0.6314 | Above 50%: False\n",
            "Prediction after 7 hour of training: 1.0000 | Above 50%: True\n",
            "Prediction after 20 hour of training: 1.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJdtP1IY4M0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9533833e-8953-4097-b837-0d4ac5bdbde0"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "tanh=nn.Tanh()\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=tanh(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "print(loss.item())\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06633114814758301\n",
            "Prediction after 1 hour of training: -0.0393 | Above 50%: False\n",
            "Prediction after 7 hour of training: 0.9891 | Above 50%: True\n",
            "Prediction after 20 hour of training: 1.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE4h8dOb6GKd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}