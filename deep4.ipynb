{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/EGsHEryK20n6Blz7js2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35d12a56ccec4822903f72dff0571599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a916deee231c4c57b0635e21d1f8507c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89a9296cefae48a98bc9768e270195aa",
              "IPY_MODEL_aaaa69a1d442436e80cdda3fba401dca",
              "IPY_MODEL_6b42aa59977849499907f96c649cb9e0"
            ]
          }
        },
        "a916deee231c4c57b0635e21d1f8507c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89a9296cefae48a98bc9768e270195aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fcffc91e3a3f4e1cb933dc5e29730c74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77b448c438cb4c06ad7ebd4ca0cde425"
          }
        },
        "aaaa69a1d442436e80cdda3fba401dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90a7b50114df40d7861a46d03f7d4ba0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c950c1164ba4ff1b4f9107aff5f5380"
          }
        },
        "6b42aa59977849499907f96c649cb9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58a0ff3af3c14d7f8eab283b850570b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:00&lt;00:00, 50640353.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2b48322b9924fef9125423cfa05a922"
          }
        },
        "fcffc91e3a3f4e1cb933dc5e29730c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77b448c438cb4c06ad7ebd4ca0cde425": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90a7b50114df40d7861a46d03f7d4ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c950c1164ba4ff1b4f9107aff5f5380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58a0ff3af3c14d7f8eab283b850570b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2b48322b9924fef9125423cfa05a922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ffa83ab75dd14882912de6ce776546f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c80d2d4de164d5a97eba77ec186aa41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8118c40155ed421b84085843399384a9",
              "IPY_MODEL_e3359a8a5be745fab13746dfce768dd3",
              "IPY_MODEL_646de78a4b53418fbc03eccb575d93e9"
            ]
          }
        },
        "1c80d2d4de164d5a97eba77ec186aa41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8118c40155ed421b84085843399384a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66279bcef4594e6daccc89700f68165c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7f57dca86f34ba682b06f072d5d6e65"
          }
        },
        "e3359a8a5be745fab13746dfce768dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8067758e0b6a416b9b8fb92c46871311",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b13d507ef05746ac9b1786765b11101a"
          }
        },
        "646de78a4b53418fbc03eccb575d93e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d950919c97e4d46ad50eb23944429db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 804404.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0234ef83a0c5417a8444428a8aa08758"
          }
        },
        "66279bcef4594e6daccc89700f68165c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7f57dca86f34ba682b06f072d5d6e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8067758e0b6a416b9b8fb92c46871311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b13d507ef05746ac9b1786765b11101a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d950919c97e4d46ad50eb23944429db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0234ef83a0c5417a8444428a8aa08758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "293e0e381a6348bca4fcc05c32c04372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_513431a79038426faa8cd8d9dd2cfe77",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_243aca994f0340e99876b38ca7e500d5",
              "IPY_MODEL_919cfe3cfc2744f7956461423c2b5b4d",
              "IPY_MODEL_8133668263d7402cbc201dee8d909bf8"
            ]
          }
        },
        "513431a79038426faa8cd8d9dd2cfe77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "243aca994f0340e99876b38ca7e500d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d19638d95d58404b9e2f10fa197f5a5e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dafd6d4b49064a8e891417935545c180"
          }
        },
        "919cfe3cfc2744f7956461423c2b5b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6432e99bb0b41a18f81dc0999f00e3b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f56eeda3db7c451e8c04738722d306aa"
          }
        },
        "8133668263d7402cbc201dee8d909bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_180d759228bc464084922b048a07fff1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:00&lt;00:00, 17304599.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e8de30ba40443789e4cf5ef2912cf4d"
          }
        },
        "d19638d95d58404b9e2f10fa197f5a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dafd6d4b49064a8e891417935545c180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6432e99bb0b41a18f81dc0999f00e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f56eeda3db7c451e8c04738722d306aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "180d759228bc464084922b048a07fff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e8de30ba40443789e4cf5ef2912cf4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "587ac2edcca94c1ab5d9b6abb02ce30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_40e9e002f0f94cfc83cbc74f95fc7615",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a237f6f27d0b4ea788ac7e0c97f2af06",
              "IPY_MODEL_2828a0125bd94c2890e6dc42f2409440",
              "IPY_MODEL_82714d7e76bd463cb60177e281a7f57c"
            ]
          }
        },
        "40e9e002f0f94cfc83cbc74f95fc7615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a237f6f27d0b4ea788ac7e0c97f2af06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5e2e1695ed984ce0b6bd014da5876a60",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b27772fd78549918a605669b71f9bce"
          }
        },
        "2828a0125bd94c2890e6dc42f2409440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d47169914be471e98dc28ce9ae5cf2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_189fbf8d70264bbab1ee320c3634de6e"
          }
        },
        "82714d7e76bd463cb60177e281a7f57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e10e057eb2964c1191908117b94f22da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 100400.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_921502bfd1f449188c375420af64da9e"
          }
        },
        "5e2e1695ed984ce0b6bd014da5876a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b27772fd78549918a605669b71f9bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d47169914be471e98dc28ce9ae5cf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "189fbf8d70264bbab1ee320c3634de6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e10e057eb2964c1191908117b94f22da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "921502bfd1f449188c375420af64da9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/deep4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAGlOw93uS_t",
        "outputId": "70c46d4f-50db-4ac3-c0f9-95290050046a"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4l8KXJlvmWd",
        "outputId": "dfdfb64f-02d2-49e9-e0bb-b39c87dfd359"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "    self.x_data=from_numpy(xy[:, 0:-1])\n",
        "    self.y_data=from_numpy(xy[:, [-1]])\n",
        "    self.len=xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset_=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset_, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader,0):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    print(f'Epoch{epoch+1} | Batch: {i+1} Loss: {loss.item(): .4f}')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Epoch1 | Batch: 1 Loss:  0.7114\n",
            "Epoch1 | Batch: 2 Loss:  0.7166\n",
            "Epoch1 | Batch: 3 Loss:  0.7251\n",
            "Epoch1 | Batch: 4 Loss:  0.6984\n",
            "Epoch1 | Batch: 5 Loss:  0.7136\n",
            "Epoch1 | Batch: 6 Loss:  0.6986\n",
            "Epoch1 | Batch: 7 Loss:  0.6995\n",
            "Epoch1 | Batch: 8 Loss:  0.6937\n",
            "Epoch1 | Batch: 9 Loss:  0.6958\n",
            "Epoch1 | Batch: 10 Loss:  0.6963\n",
            "Epoch1 | Batch: 11 Loss:  0.6923\n",
            "Epoch1 | Batch: 12 Loss:  0.6891\n",
            "Epoch1 | Batch: 13 Loss:  0.6824\n",
            "Epoch1 | Batch: 14 Loss:  0.6886\n",
            "Epoch1 | Batch: 15 Loss:  0.6819\n",
            "Epoch1 | Batch: 16 Loss:  0.6701\n",
            "Epoch1 | Batch: 17 Loss:  0.6805\n",
            "Epoch1 | Batch: 18 Loss:  0.6837\n",
            "Epoch1 | Batch: 19 Loss:  0.6707\n",
            "Epoch1 | Batch: 20 Loss:  0.6735\n",
            "Epoch1 | Batch: 21 Loss:  0.6711\n",
            "Epoch1 | Batch: 22 Loss:  0.6701\n",
            "Epoch1 | Batch: 23 Loss:  0.6629\n",
            "Epoch1 | Batch: 24 Loss:  0.6678\n",
            "Epoch2 | Batch: 1 Loss:  0.6466\n",
            "Epoch2 | Batch: 2 Loss:  0.6924\n",
            "Epoch2 | Batch: 3 Loss:  0.6857\n",
            "Epoch2 | Batch: 4 Loss:  0.6429\n",
            "Epoch2 | Batch: 5 Loss:  0.6857\n",
            "Epoch2 | Batch: 6 Loss:  0.6546\n",
            "Epoch2 | Batch: 7 Loss:  0.6611\n",
            "Epoch2 | Batch: 8 Loss:  0.6852\n",
            "Epoch2 | Batch: 9 Loss:  0.6685\n",
            "Epoch2 | Batch: 10 Loss:  0.6423\n",
            "Epoch2 | Batch: 11 Loss:  0.6765\n",
            "Epoch2 | Batch: 12 Loss:  0.6488\n",
            "Epoch2 | Batch: 13 Loss:  0.6572\n",
            "Epoch2 | Batch: 14 Loss:  0.6274\n",
            "Epoch2 | Batch: 15 Loss:  0.6861\n",
            "Epoch2 | Batch: 16 Loss:  0.6550\n",
            "Epoch2 | Batch: 17 Loss:  0.6443\n",
            "Epoch2 | Batch: 18 Loss:  0.6107\n",
            "Epoch2 | Batch: 19 Loss:  0.6642\n",
            "Epoch2 | Batch: 20 Loss:  0.6640\n",
            "Epoch2 | Batch: 21 Loss:  0.6754\n",
            "Epoch2 | Batch: 22 Loss:  0.6521\n",
            "Epoch2 | Batch: 23 Loss:  0.6042\n",
            "Epoch2 | Batch: 24 Loss:  0.6696\n",
            "Epoch3 | Batch: 1 Loss:  0.6131\n",
            "Epoch3 | Batch: 2 Loss:  0.6496\n",
            "Epoch3 | Batch: 3 Loss:  0.6361\n",
            "Epoch3 | Batch: 4 Loss:  0.6623\n",
            "Epoch3 | Batch: 5 Loss:  0.7566\n",
            "Epoch3 | Batch: 6 Loss:  0.6757\n",
            "Epoch3 | Batch: 7 Loss:  0.6500\n",
            "Epoch3 | Batch: 8 Loss:  0.7146\n",
            "Epoch3 | Batch: 9 Loss:  0.5995\n",
            "Epoch3 | Batch: 10 Loss:  0.6492\n",
            "Epoch3 | Batch: 11 Loss:  0.6624\n",
            "Epoch3 | Batch: 12 Loss:  0.6354\n",
            "Epoch3 | Batch: 13 Loss:  0.6210\n",
            "Epoch3 | Batch: 14 Loss:  0.6761\n",
            "Epoch3 | Batch: 15 Loss:  0.6340\n",
            "Epoch3 | Batch: 16 Loss:  0.6333\n",
            "Epoch3 | Batch: 17 Loss:  0.6473\n",
            "Epoch3 | Batch: 18 Loss:  0.6033\n",
            "Epoch3 | Batch: 19 Loss:  0.6315\n",
            "Epoch3 | Batch: 20 Loss:  0.6310\n",
            "Epoch3 | Batch: 21 Loss:  0.5995\n",
            "Epoch3 | Batch: 22 Loss:  0.7253\n",
            "Epoch3 | Batch: 23 Loss:  0.6616\n",
            "Epoch3 | Batch: 24 Loss:  0.6048\n",
            "Epoch4 | Batch: 1 Loss:  0.6775\n",
            "Epoch4 | Batch: 2 Loss:  0.5983\n",
            "Epoch4 | Batch: 3 Loss:  0.6778\n",
            "Epoch4 | Batch: 4 Loss:  0.5972\n",
            "Epoch4 | Batch: 5 Loss:  0.7441\n",
            "Epoch4 | Batch: 6 Loss:  0.5816\n",
            "Epoch4 | Batch: 7 Loss:  0.6783\n",
            "Epoch4 | Batch: 8 Loss:  0.6125\n",
            "Epoch4 | Batch: 9 Loss:  0.6617\n",
            "Epoch4 | Batch: 10 Loss:  0.6283\n",
            "Epoch4 | Batch: 11 Loss:  0.6616\n",
            "Epoch4 | Batch: 12 Loss:  0.6617\n",
            "Epoch4 | Batch: 13 Loss:  0.6114\n",
            "Epoch4 | Batch: 14 Loss:  0.6446\n",
            "Epoch4 | Batch: 15 Loss:  0.6960\n",
            "Epoch4 | Batch: 16 Loss:  0.6110\n",
            "Epoch4 | Batch: 17 Loss:  0.6790\n",
            "Epoch4 | Batch: 18 Loss:  0.6617\n",
            "Epoch4 | Batch: 19 Loss:  0.6618\n",
            "Epoch4 | Batch: 20 Loss:  0.6447\n",
            "Epoch4 | Batch: 21 Loss:  0.6275\n",
            "Epoch4 | Batch: 22 Loss:  0.6445\n",
            "Epoch4 | Batch: 23 Loss:  0.5925\n",
            "Epoch4 | Batch: 24 Loss:  0.6710\n",
            "Epoch5 | Batch: 1 Loss:  0.6619\n",
            "Epoch5 | Batch: 2 Loss:  0.6794\n",
            "Epoch5 | Batch: 3 Loss:  0.6270\n",
            "Epoch5 | Batch: 4 Loss:  0.6444\n",
            "Epoch5 | Batch: 5 Loss:  0.6972\n",
            "Epoch5 | Batch: 6 Loss:  0.6445\n",
            "Epoch5 | Batch: 7 Loss:  0.5920\n",
            "Epoch5 | Batch: 8 Loss:  0.6977\n",
            "Epoch5 | Batch: 9 Loss:  0.6266\n",
            "Epoch5 | Batch: 10 Loss:  0.6266\n",
            "Epoch5 | Batch: 11 Loss:  0.5903\n",
            "Epoch5 | Batch: 12 Loss:  0.7170\n",
            "Epoch5 | Batch: 13 Loss:  0.6262\n",
            "Epoch5 | Batch: 14 Loss:  0.6258\n",
            "Epoch5 | Batch: 15 Loss:  0.6804\n",
            "Epoch5 | Batch: 16 Loss:  0.6442\n",
            "Epoch5 | Batch: 17 Loss:  0.6621\n",
            "Epoch5 | Batch: 18 Loss:  0.6440\n",
            "Epoch5 | Batch: 19 Loss:  0.5715\n",
            "Epoch5 | Batch: 20 Loss:  0.7924\n",
            "Epoch5 | Batch: 21 Loss:  0.5730\n",
            "Epoch5 | Batch: 22 Loss:  0.6257\n",
            "Epoch5 | Batch: 23 Loss:  0.6254\n",
            "Epoch5 | Batch: 24 Loss:  0.6204\n",
            "Epoch6 | Batch: 1 Loss:  0.6063\n",
            "Epoch6 | Batch: 2 Loss:  0.6815\n",
            "Epoch6 | Batch: 3 Loss:  0.6812\n",
            "Epoch6 | Batch: 4 Loss:  0.6251\n",
            "Epoch6 | Batch: 5 Loss:  0.5689\n",
            "Epoch6 | Batch: 6 Loss:  0.6821\n",
            "Epoch6 | Batch: 7 Loss:  0.6625\n",
            "Epoch6 | Batch: 8 Loss:  0.6436\n",
            "Epoch6 | Batch: 9 Loss:  0.6248\n",
            "Epoch6 | Batch: 10 Loss:  0.6243\n",
            "Epoch6 | Batch: 11 Loss:  0.6435\n",
            "Epoch6 | Batch: 12 Loss:  0.5667\n",
            "Epoch6 | Batch: 13 Loss:  0.6435\n",
            "Epoch6 | Batch: 14 Loss:  0.7420\n",
            "Epoch6 | Batch: 15 Loss:  0.6436\n",
            "Epoch6 | Batch: 16 Loss:  0.6627\n",
            "Epoch6 | Batch: 17 Loss:  0.6632\n",
            "Epoch6 | Batch: 18 Loss:  0.6437\n",
            "Epoch6 | Batch: 19 Loss:  0.6437\n",
            "Epoch6 | Batch: 20 Loss:  0.6247\n",
            "Epoch6 | Batch: 21 Loss:  0.5859\n",
            "Epoch6 | Batch: 22 Loss:  0.6828\n",
            "Epoch6 | Batch: 23 Loss:  0.7017\n",
            "Epoch6 | Batch: 24 Loss:  0.6461\n",
            "Epoch7 | Batch: 1 Loss:  0.7010\n",
            "Epoch7 | Batch: 2 Loss:  0.5685\n",
            "Epoch7 | Batch: 3 Loss:  0.7012\n",
            "Epoch7 | Batch: 4 Loss:  0.6437\n",
            "Epoch7 | Batch: 5 Loss:  0.6436\n",
            "Epoch7 | Batch: 6 Loss:  0.6818\n",
            "Epoch7 | Batch: 7 Loss:  0.6625\n",
            "Epoch7 | Batch: 8 Loss:  0.6625\n",
            "Epoch7 | Batch: 9 Loss:  0.6251\n",
            "Epoch7 | Batch: 10 Loss:  0.5494\n",
            "Epoch7 | Batch: 11 Loss:  0.6241\n",
            "Epoch7 | Batch: 12 Loss:  0.6045\n",
            "Epoch7 | Batch: 13 Loss:  0.6038\n",
            "Epoch7 | Batch: 14 Loss:  0.6635\n",
            "Epoch7 | Batch: 15 Loss:  0.6234\n",
            "Epoch7 | Batch: 16 Loss:  0.6836\n",
            "Epoch7 | Batch: 17 Loss:  0.6237\n",
            "Epoch7 | Batch: 18 Loss:  0.7031\n",
            "Epoch7 | Batch: 19 Loss:  0.6827\n",
            "Epoch7 | Batch: 20 Loss:  0.5661\n",
            "Epoch7 | Batch: 21 Loss:  0.6039\n",
            "Epoch7 | Batch: 22 Loss:  0.6839\n",
            "Epoch7 | Batch: 23 Loss:  0.6238\n",
            "Epoch7 | Batch: 24 Loss:  0.8125\n",
            "Epoch8 | Batch: 1 Loss:  0.6054\n",
            "Epoch8 | Batch: 2 Loss:  0.6243\n",
            "Epoch8 | Batch: 3 Loss:  0.5465\n",
            "Epoch8 | Batch: 4 Loss:  0.6634\n",
            "Epoch8 | Batch: 5 Loss:  0.6038\n",
            "Epoch8 | Batch: 6 Loss:  0.5633\n",
            "Epoch8 | Batch: 7 Loss:  0.6436\n",
            "Epoch8 | Batch: 8 Loss:  0.7662\n",
            "Epoch8 | Batch: 9 Loss:  0.5642\n",
            "Epoch8 | Batch: 10 Loss:  0.6435\n",
            "Epoch8 | Batch: 11 Loss:  0.6638\n",
            "Epoch8 | Batch: 12 Loss:  0.6837\n",
            "Epoch8 | Batch: 13 Loss:  0.6234\n",
            "Epoch8 | Batch: 14 Loss:  0.7037\n",
            "Epoch8 | Batch: 15 Loss:  0.6238\n",
            "Epoch8 | Batch: 16 Loss:  0.6636\n",
            "Epoch8 | Batch: 17 Loss:  0.6238\n",
            "Epoch8 | Batch: 18 Loss:  0.6037\n",
            "Epoch8 | Batch: 19 Loss:  0.6235\n",
            "Epoch8 | Batch: 20 Loss:  0.6636\n",
            "Epoch8 | Batch: 21 Loss:  0.6835\n",
            "Epoch8 | Batch: 22 Loss:  0.6634\n",
            "Epoch8 | Batch: 23 Loss:  0.7624\n",
            "Epoch8 | Batch: 24 Loss:  0.6995\n",
            "Epoch9 | Batch: 1 Loss:  0.6246\n",
            "Epoch9 | Batch: 2 Loss:  0.6817\n",
            "Epoch9 | Batch: 3 Loss:  0.5303\n",
            "Epoch9 | Batch: 4 Loss:  0.7020\n",
            "Epoch9 | Batch: 5 Loss:  0.6436\n",
            "Epoch9 | Batch: 6 Loss:  0.5856\n",
            "Epoch9 | Batch: 7 Loss:  0.7026\n",
            "Epoch9 | Batch: 8 Loss:  0.6822\n",
            "Epoch9 | Batch: 9 Loss:  0.7012\n",
            "Epoch9 | Batch: 10 Loss:  0.7004\n",
            "Epoch9 | Batch: 11 Loss:  0.5880\n",
            "Epoch9 | Batch: 12 Loss:  0.6248\n",
            "Epoch9 | Batch: 13 Loss:  0.6435\n",
            "Epoch9 | Batch: 14 Loss:  0.6816\n",
            "Epoch9 | Batch: 15 Loss:  0.5869\n",
            "Epoch9 | Batch: 16 Loss:  0.6822\n",
            "Epoch9 | Batch: 17 Loss:  0.6627\n",
            "Epoch9 | Batch: 18 Loss:  0.5867\n",
            "Epoch9 | Batch: 19 Loss:  0.5858\n",
            "Epoch9 | Batch: 20 Loss:  0.6041\n",
            "Epoch9 | Batch: 21 Loss:  0.7032\n",
            "Epoch9 | Batch: 22 Loss:  0.6828\n",
            "Epoch9 | Batch: 23 Loss:  0.6823\n",
            "Epoch9 | Batch: 24 Loss:  0.6193\n",
            "Epoch10 | Batch: 1 Loss:  0.6629\n",
            "Epoch10 | Batch: 2 Loss:  0.5281\n",
            "Epoch10 | Batch: 3 Loss:  0.6238\n",
            "Epoch10 | Batch: 4 Loss:  0.7634\n",
            "Epoch10 | Batch: 5 Loss:  0.6242\n",
            "Epoch10 | Batch: 6 Loss:  0.6240\n",
            "Epoch10 | Batch: 7 Loss:  0.6631\n",
            "Epoch10 | Batch: 8 Loss:  0.6826\n",
            "Epoch10 | Batch: 9 Loss:  0.6825\n",
            "Epoch10 | Batch: 10 Loss:  0.6436\n",
            "Epoch10 | Batch: 11 Loss:  0.6628\n",
            "Epoch10 | Batch: 12 Loss:  0.6436\n",
            "Epoch10 | Batch: 13 Loss:  0.5863\n",
            "Epoch10 | Batch: 14 Loss:  0.5656\n",
            "Epoch10 | Batch: 15 Loss:  0.6832\n",
            "Epoch10 | Batch: 16 Loss:  0.6632\n",
            "Epoch10 | Batch: 17 Loss:  0.6044\n",
            "Epoch10 | Batch: 18 Loss:  0.7030\n",
            "Epoch10 | Batch: 19 Loss:  0.6629\n",
            "Epoch10 | Batch: 20 Loss:  0.6241\n",
            "Epoch10 | Batch: 21 Loss:  0.6436\n",
            "Epoch10 | Batch: 22 Loss:  0.6240\n",
            "Epoch10 | Batch: 23 Loss:  0.7026\n",
            "Epoch10 | Batch: 24 Loss:  0.6190\n",
            "Epoch11 | Batch: 1 Loss:  0.6239\n",
            "Epoch11 | Batch: 2 Loss:  0.6433\n",
            "Epoch11 | Batch: 3 Loss:  0.6043\n",
            "Epoch11 | Batch: 4 Loss:  0.6437\n",
            "Epoch11 | Batch: 5 Loss:  0.6433\n",
            "Epoch11 | Batch: 6 Loss:  0.5838\n",
            "Epoch11 | Batch: 7 Loss:  0.6637\n",
            "Epoch11 | Batch: 8 Loss:  0.6434\n",
            "Epoch11 | Batch: 9 Loss:  0.6638\n",
            "Epoch11 | Batch: 10 Loss:  0.6638\n",
            "Epoch11 | Batch: 11 Loss:  0.6434\n",
            "Epoch11 | Batch: 12 Loss:  0.7034\n",
            "Epoch11 | Batch: 13 Loss:  0.6239\n",
            "Epoch11 | Batch: 14 Loss:  0.6635\n",
            "Epoch11 | Batch: 15 Loss:  0.6631\n",
            "Epoch11 | Batch: 16 Loss:  0.6631\n",
            "Epoch11 | Batch: 17 Loss:  0.6825\n",
            "Epoch11 | Batch: 18 Loss:  0.6628\n",
            "Epoch11 | Batch: 19 Loss:  0.5858\n",
            "Epoch11 | Batch: 20 Loss:  0.6632\n",
            "Epoch11 | Batch: 21 Loss:  0.6435\n",
            "Epoch11 | Batch: 22 Loss:  0.6824\n",
            "Epoch11 | Batch: 23 Loss:  0.6629\n",
            "Epoch11 | Batch: 24 Loss:  0.5390\n",
            "Epoch12 | Batch: 1 Loss:  0.5840\n",
            "Epoch12 | Batch: 2 Loss:  0.6234\n",
            "Epoch12 | Batch: 3 Loss:  0.6840\n",
            "Epoch12 | Batch: 4 Loss:  0.6034\n",
            "Epoch12 | Batch: 5 Loss:  0.6636\n",
            "Epoch12 | Batch: 6 Loss:  0.8045\n",
            "Epoch12 | Batch: 7 Loss:  0.6438\n",
            "Epoch12 | Batch: 8 Loss:  0.6435\n",
            "Epoch12 | Batch: 9 Loss:  0.6823\n",
            "Epoch12 | Batch: 10 Loss:  0.5474\n",
            "Epoch12 | Batch: 11 Loss:  0.6435\n",
            "Epoch12 | Batch: 12 Loss:  0.5845\n",
            "Epoch12 | Batch: 13 Loss:  0.7038\n",
            "Epoch12 | Batch: 14 Loss:  0.6831\n",
            "Epoch12 | Batch: 15 Loss:  0.6629\n",
            "Epoch12 | Batch: 16 Loss:  0.6630\n",
            "Epoch12 | Batch: 17 Loss:  0.6435\n",
            "Epoch12 | Batch: 18 Loss:  0.6823\n",
            "Epoch12 | Batch: 19 Loss:  0.6435\n",
            "Epoch12 | Batch: 20 Loss:  0.5666\n",
            "Epoch12 | Batch: 21 Loss:  0.6241\n",
            "Epoch12 | Batch: 22 Loss:  0.6830\n",
            "Epoch12 | Batch: 23 Loss:  0.6239\n",
            "Epoch12 | Batch: 24 Loss:  0.5916\n",
            "Epoch13 | Batch: 1 Loss:  0.7633\n",
            "Epoch13 | Batch: 2 Loss:  0.7015\n",
            "Epoch13 | Batch: 3 Loss:  0.6054\n",
            "Epoch13 | Batch: 4 Loss:  0.7210\n",
            "Epoch13 | Batch: 5 Loss:  0.6627\n",
            "Epoch13 | Batch: 6 Loss:  0.7193\n",
            "Epoch13 | Batch: 7 Loss:  0.5698\n",
            "Epoch13 | Batch: 8 Loss:  0.6437\n",
            "Epoch13 | Batch: 9 Loss:  0.6435\n",
            "Epoch13 | Batch: 10 Loss:  0.6628\n",
            "Epoch13 | Batch: 11 Loss:  0.6436\n",
            "Epoch13 | Batch: 12 Loss:  0.5867\n",
            "Epoch13 | Batch: 13 Loss:  0.5472\n",
            "Epoch13 | Batch: 14 Loss:  0.5840\n",
            "Epoch13 | Batch: 15 Loss:  0.6839\n",
            "Epoch13 | Batch: 16 Loss:  0.6832\n",
            "Epoch13 | Batch: 17 Loss:  0.5844\n",
            "Epoch13 | Batch: 18 Loss:  0.6634\n",
            "Epoch13 | Batch: 19 Loss:  0.6430\n",
            "Epoch13 | Batch: 20 Loss:  0.6237\n",
            "Epoch13 | Batch: 21 Loss:  0.6437\n",
            "Epoch13 | Batch: 22 Loss:  0.6435\n",
            "Epoch13 | Batch: 23 Loss:  0.5632\n",
            "Epoch13 | Batch: 24 Loss:  0.7317\n",
            "Epoch14 | Batch: 1 Loss:  0.6835\n",
            "Epoch14 | Batch: 2 Loss:  0.6038\n",
            "Epoch14 | Batch: 3 Loss:  0.6632\n",
            "Epoch14 | Batch: 4 Loss:  0.6235\n",
            "Epoch14 | Batch: 5 Loss:  0.6034\n",
            "Epoch14 | Batch: 6 Loss:  0.7042\n",
            "Epoch14 | Batch: 7 Loss:  0.6435\n",
            "Epoch14 | Batch: 8 Loss:  0.5838\n",
            "Epoch14 | Batch: 9 Loss:  0.6232\n",
            "Epoch14 | Batch: 10 Loss:  0.6639\n",
            "Epoch14 | Batch: 11 Loss:  0.5624\n",
            "Epoch14 | Batch: 12 Loss:  0.6022\n",
            "Epoch14 | Batch: 13 Loss:  0.6851\n",
            "Epoch14 | Batch: 14 Loss:  0.6226\n",
            "Epoch14 | Batch: 15 Loss:  0.7264\n",
            "Epoch14 | Batch: 16 Loss:  0.6643\n",
            "Epoch14 | Batch: 17 Loss:  0.6838\n",
            "Epoch14 | Batch: 18 Loss:  0.6836\n",
            "Epoch14 | Batch: 19 Loss:  0.6043\n",
            "Epoch14 | Batch: 20 Loss:  0.6434\n",
            "Epoch14 | Batch: 21 Loss:  0.6635\n",
            "Epoch14 | Batch: 22 Loss:  0.6634\n",
            "Epoch14 | Batch: 23 Loss:  0.7228\n",
            "Epoch14 | Batch: 24 Loss:  0.5376\n",
            "Epoch15 | Batch: 1 Loss:  0.5835\n",
            "Epoch15 | Batch: 2 Loss:  0.6435\n",
            "Epoch15 | Batch: 3 Loss:  0.6638\n",
            "Epoch15 | Batch: 4 Loss:  0.5629\n",
            "Epoch15 | Batch: 5 Loss:  0.6024\n",
            "Epoch15 | Batch: 6 Loss:  0.6227\n",
            "Epoch15 | Batch: 7 Loss:  0.5808\n",
            "Epoch15 | Batch: 8 Loss:  0.6222\n",
            "Epoch15 | Batch: 9 Loss:  0.6440\n",
            "Epoch15 | Batch: 10 Loss:  0.5591\n",
            "Epoch15 | Batch: 11 Loss:  0.7305\n",
            "Epoch15 | Batch: 12 Loss:  0.6857\n",
            "Epoch15 | Batch: 13 Loss:  0.7062\n",
            "Epoch15 | Batch: 14 Loss:  0.6637\n",
            "Epoch15 | Batch: 15 Loss:  0.6431\n",
            "Epoch15 | Batch: 16 Loss:  0.5614\n",
            "Epoch15 | Batch: 17 Loss:  0.7478\n",
            "Epoch15 | Batch: 18 Loss:  0.7049\n",
            "Epoch15 | Batch: 19 Loss:  0.6233\n",
            "Epoch15 | Batch: 20 Loss:  0.7041\n",
            "Epoch15 | Batch: 21 Loss:  0.7031\n",
            "Epoch15 | Batch: 22 Loss:  0.6042\n",
            "Epoch15 | Batch: 23 Loss:  0.6832\n",
            "Epoch15 | Batch: 24 Loss:  0.6463\n",
            "Epoch16 | Batch: 1 Loss:  0.7221\n",
            "Epoch16 | Batch: 2 Loss:  0.6628\n",
            "Epoch16 | Batch: 3 Loss:  0.5861\n",
            "Epoch16 | Batch: 4 Loss:  0.5462\n",
            "Epoch16 | Batch: 5 Loss:  0.5835\n",
            "Epoch16 | Batch: 6 Loss:  0.5421\n",
            "Epoch16 | Batch: 7 Loss:  0.6645\n",
            "Epoch16 | Batch: 8 Loss:  0.5606\n",
            "Epoch16 | Batch: 9 Loss:  0.6648\n",
            "Epoch16 | Batch: 10 Loss:  0.6644\n",
            "Epoch16 | Batch: 11 Loss:  0.6436\n",
            "Epoch16 | Batch: 12 Loss:  0.7059\n",
            "Epoch16 | Batch: 13 Loss:  0.6842\n",
            "Epoch16 | Batch: 14 Loss:  0.5421\n",
            "Epoch16 | Batch: 15 Loss:  0.6645\n",
            "Epoch16 | Batch: 16 Loss:  0.7882\n",
            "Epoch16 | Batch: 17 Loss:  0.5635\n",
            "Epoch16 | Batch: 18 Loss:  0.6433\n",
            "Epoch16 | Batch: 19 Loss:  0.6231\n",
            "Epoch16 | Batch: 20 Loss:  0.7253\n",
            "Epoch16 | Batch: 21 Loss:  0.6833\n",
            "Epoch16 | Batch: 22 Loss:  0.7234\n",
            "Epoch16 | Batch: 23 Loss:  0.6434\n",
            "Epoch16 | Batch: 24 Loss:  0.6731\n",
            "Epoch17 | Batch: 1 Loss:  0.5854\n",
            "Epoch17 | Batch: 2 Loss:  0.6830\n",
            "Epoch17 | Batch: 3 Loss:  0.5850\n",
            "Epoch17 | Batch: 4 Loss:  0.6633\n",
            "Epoch17 | Batch: 5 Loss:  0.5644\n",
            "Epoch17 | Batch: 6 Loss:  0.6839\n",
            "Epoch17 | Batch: 7 Loss:  0.7037\n",
            "Epoch17 | Batch: 8 Loss:  0.6237\n",
            "Epoch17 | Batch: 9 Loss:  0.6432\n",
            "Epoch17 | Batch: 10 Loss:  0.5844\n",
            "Epoch17 | Batch: 11 Loss:  0.7035\n",
            "Epoch17 | Batch: 12 Loss:  0.6039\n",
            "Epoch17 | Batch: 13 Loss:  0.6433\n",
            "Epoch17 | Batch: 14 Loss:  0.7035\n",
            "Epoch17 | Batch: 15 Loss:  0.6239\n",
            "Epoch17 | Batch: 16 Loss:  0.6631\n",
            "Epoch17 | Batch: 17 Loss:  0.7028\n",
            "Epoch17 | Batch: 18 Loss:  0.6627\n",
            "Epoch17 | Batch: 19 Loss:  0.7016\n",
            "Epoch17 | Batch: 20 Loss:  0.7201\n",
            "Epoch17 | Batch: 21 Loss:  0.6436\n",
            "Epoch17 | Batch: 22 Loss:  0.5499\n",
            "Epoch17 | Batch: 23 Loss:  0.5662\n",
            "Epoch17 | Batch: 24 Loss:  0.7017\n",
            "Epoch18 | Batch: 1 Loss:  0.6627\n",
            "Epoch18 | Batch: 2 Loss:  0.6051\n",
            "Epoch18 | Batch: 3 Loss:  0.7221\n",
            "Epoch18 | Batch: 4 Loss:  0.6627\n",
            "Epoch18 | Batch: 5 Loss:  0.6243\n",
            "Epoch18 | Batch: 6 Loss:  0.7201\n",
            "Epoch18 | Batch: 7 Loss:  0.5683\n",
            "Epoch18 | Batch: 8 Loss:  0.7402\n",
            "Epoch18 | Batch: 9 Loss:  0.5682\n",
            "Epoch18 | Batch: 10 Loss:  0.5285\n",
            "Epoch18 | Batch: 11 Loss:  0.6829\n",
            "Epoch18 | Batch: 12 Loss:  0.6827\n",
            "Epoch18 | Batch: 13 Loss:  0.6824\n",
            "Epoch18 | Batch: 14 Loss:  0.5856\n",
            "Epoch18 | Batch: 15 Loss:  0.6432\n",
            "Epoch18 | Batch: 16 Loss:  0.6040\n",
            "Epoch18 | Batch: 17 Loss:  0.6634\n",
            "Epoch18 | Batch: 18 Loss:  0.7026\n",
            "Epoch18 | Batch: 19 Loss:  0.6239\n",
            "Epoch18 | Batch: 20 Loss:  0.6241\n",
            "Epoch18 | Batch: 21 Loss:  0.6238\n",
            "Epoch18 | Batch: 22 Loss:  0.6834\n",
            "Epoch18 | Batch: 23 Loss:  0.6634\n",
            "Epoch18 | Batch: 24 Loss:  0.6192\n",
            "Epoch19 | Batch: 1 Loss:  0.6043\n",
            "Epoch19 | Batch: 2 Loss:  0.5836\n",
            "Epoch19 | Batch: 3 Loss:  0.5626\n",
            "Epoch19 | Batch: 4 Loss:  0.6231\n",
            "Epoch19 | Batch: 5 Loss:  0.6640\n",
            "Epoch19 | Batch: 6 Loss:  0.6026\n",
            "Epoch19 | Batch: 7 Loss:  0.6641\n",
            "Epoch19 | Batch: 8 Loss:  0.6641\n",
            "Epoch19 | Batch: 9 Loss:  0.6640\n",
            "Epoch19 | Batch: 10 Loss:  0.6230\n",
            "Epoch19 | Batch: 11 Loss:  0.6842\n",
            "Epoch19 | Batch: 12 Loss:  0.6434\n",
            "Epoch19 | Batch: 13 Loss:  0.7249\n",
            "Epoch19 | Batch: 14 Loss:  0.6833\n",
            "Epoch19 | Batch: 15 Loss:  0.6240\n",
            "Epoch19 | Batch: 16 Loss:  0.6035\n",
            "Epoch19 | Batch: 17 Loss:  0.6436\n",
            "Epoch19 | Batch: 18 Loss:  0.6836\n",
            "Epoch19 | Batch: 19 Loss:  0.6040\n",
            "Epoch19 | Batch: 20 Loss:  0.6432\n",
            "Epoch19 | Batch: 21 Loss:  0.6234\n",
            "Epoch19 | Batch: 22 Loss:  0.7037\n",
            "Epoch19 | Batch: 23 Loss:  0.7032\n",
            "Epoch19 | Batch: 24 Loss:  0.6733\n",
            "Epoch20 | Batch: 1 Loss:  0.6046\n",
            "Epoch20 | Batch: 2 Loss:  0.5649\n",
            "Epoch20 | Batch: 3 Loss:  0.6839\n",
            "Epoch20 | Batch: 4 Loss:  0.7032\n",
            "Epoch20 | Batch: 5 Loss:  0.6043\n",
            "Epoch20 | Batch: 6 Loss:  0.6036\n",
            "Epoch20 | Batch: 7 Loss:  0.6828\n",
            "Epoch20 | Batch: 8 Loss:  0.6637\n",
            "Epoch20 | Batch: 9 Loss:  0.6044\n",
            "Epoch20 | Batch: 10 Loss:  0.6634\n",
            "Epoch20 | Batch: 11 Loss:  0.6630\n",
            "Epoch20 | Batch: 12 Loss:  0.6825\n",
            "Epoch20 | Batch: 13 Loss:  0.6239\n",
            "Epoch20 | Batch: 14 Loss:  0.6628\n",
            "Epoch20 | Batch: 15 Loss:  0.5453\n",
            "Epoch20 | Batch: 16 Loss:  0.7442\n",
            "Epoch20 | Batch: 17 Loss:  0.6040\n",
            "Epoch20 | Batch: 18 Loss:  0.6240\n",
            "Epoch20 | Batch: 19 Loss:  0.7431\n",
            "Epoch20 | Batch: 20 Loss:  0.6830\n",
            "Epoch20 | Batch: 21 Loss:  0.6244\n",
            "Epoch20 | Batch: 22 Loss:  0.6434\n",
            "Epoch20 | Batch: 23 Loss:  0.6047\n",
            "Epoch20 | Batch: 24 Loss:  0.6733\n",
            "Epoch21 | Batch: 1 Loss:  0.6434\n",
            "Epoch21 | Batch: 2 Loss:  0.5851\n",
            "Epoch21 | Batch: 3 Loss:  0.6239\n",
            "Epoch21 | Batch: 4 Loss:  0.6436\n",
            "Epoch21 | Batch: 5 Loss:  0.6834\n",
            "Epoch21 | Batch: 6 Loss:  0.7035\n",
            "Epoch21 | Batch: 7 Loss:  0.6627\n",
            "Epoch21 | Batch: 8 Loss:  0.5854\n",
            "Epoch21 | Batch: 9 Loss:  0.6437\n",
            "Epoch21 | Batch: 10 Loss:  0.6433\n",
            "Epoch21 | Batch: 11 Loss:  0.6828\n",
            "Epoch21 | Batch: 12 Loss:  0.7213\n",
            "Epoch21 | Batch: 13 Loss:  0.6242\n",
            "Epoch21 | Batch: 14 Loss:  0.7403\n",
            "Epoch21 | Batch: 15 Loss:  0.6060\n",
            "Epoch21 | Batch: 16 Loss:  0.5863\n",
            "Epoch21 | Batch: 17 Loss:  0.5661\n",
            "Epoch21 | Batch: 18 Loss:  0.6038\n",
            "Epoch21 | Batch: 19 Loss:  0.6437\n",
            "Epoch21 | Batch: 20 Loss:  0.7035\n",
            "Epoch21 | Batch: 21 Loss:  0.5251\n",
            "Epoch21 | Batch: 22 Loss:  0.6630\n",
            "Epoch21 | Batch: 23 Loss:  0.7041\n",
            "Epoch21 | Batch: 24 Loss:  0.7288\n",
            "Epoch22 | Batch: 1 Loss:  0.6243\n",
            "Epoch22 | Batch: 2 Loss:  0.6435\n",
            "Epoch22 | Batch: 3 Loss:  0.6234\n",
            "Epoch22 | Batch: 4 Loss:  0.6435\n",
            "Epoch22 | Batch: 5 Loss:  0.6633\n",
            "Epoch22 | Batch: 6 Loss:  0.5847\n",
            "Epoch22 | Batch: 7 Loss:  0.6437\n",
            "Epoch22 | Batch: 8 Loss:  0.6636\n",
            "Epoch22 | Batch: 9 Loss:  0.7030\n",
            "Epoch22 | Batch: 10 Loss:  0.7215\n",
            "Epoch22 | Batch: 11 Loss:  0.6051\n",
            "Epoch22 | Batch: 12 Loss:  0.6441\n",
            "Epoch22 | Batch: 13 Loss:  0.5848\n",
            "Epoch22 | Batch: 14 Loss:  0.6435\n",
            "Epoch22 | Batch: 15 Loss:  0.6041\n",
            "Epoch22 | Batch: 16 Loss:  0.5838\n",
            "Epoch22 | Batch: 17 Loss:  0.6838\n",
            "Epoch22 | Batch: 18 Loss:  0.7634\n",
            "Epoch22 | Batch: 19 Loss:  0.6622\n",
            "Epoch22 | Batch: 20 Loss:  0.6051\n",
            "Epoch22 | Batch: 21 Loss:  0.6429\n",
            "Epoch22 | Batch: 22 Loss:  0.6826\n",
            "Epoch22 | Batch: 23 Loss:  0.6041\n",
            "Epoch22 | Batch: 24 Loss:  0.6737\n",
            "Epoch23 | Batch: 1 Loss:  0.6819\n",
            "Epoch23 | Batch: 2 Loss:  0.5463\n",
            "Epoch23 | Batch: 3 Loss:  0.6234\n",
            "Epoch23 | Batch: 4 Loss:  0.6637\n",
            "Epoch23 | Batch: 5 Loss:  0.7032\n",
            "Epoch23 | Batch: 6 Loss:  0.6238\n",
            "Epoch23 | Batch: 7 Loss:  0.6043\n",
            "Epoch23 | Batch: 8 Loss:  0.7826\n",
            "Epoch23 | Batch: 9 Loss:  0.6434\n",
            "Epoch23 | Batch: 10 Loss:  0.6049\n",
            "Epoch23 | Batch: 11 Loss:  0.6826\n",
            "Epoch23 | Batch: 12 Loss:  0.5854\n",
            "Epoch23 | Batch: 13 Loss:  0.6622\n",
            "Epoch23 | Batch: 14 Loss:  0.7019\n",
            "Epoch23 | Batch: 15 Loss:  0.6052\n",
            "Epoch23 | Batch: 16 Loss:  0.7218\n",
            "Epoch23 | Batch: 17 Loss:  0.6241\n",
            "Epoch23 | Batch: 18 Loss:  0.6436\n",
            "Epoch23 | Batch: 19 Loss:  0.6636\n",
            "Epoch23 | Batch: 20 Loss:  0.6634\n",
            "Epoch23 | Batch: 21 Loss:  0.5668\n",
            "Epoch23 | Batch: 22 Loss:  0.6628\n",
            "Epoch23 | Batch: 23 Loss:  0.5661\n",
            "Epoch23 | Batch: 24 Loss:  0.6734\n",
            "Epoch24 | Batch: 1 Loss:  0.5845\n",
            "Epoch24 | Batch: 2 Loss:  0.6632\n",
            "Epoch24 | Batch: 3 Loss:  0.7834\n",
            "Epoch24 | Batch: 4 Loss:  0.7195\n",
            "Epoch24 | Batch: 5 Loss:  0.7378\n",
            "Epoch24 | Batch: 6 Loss:  0.5881\n",
            "Epoch24 | Batch: 7 Loss:  0.6246\n",
            "Epoch24 | Batch: 8 Loss:  0.6242\n",
            "Epoch24 | Batch: 9 Loss:  0.6432\n",
            "Epoch24 | Batch: 10 Loss:  0.5479\n",
            "Epoch24 | Batch: 11 Loss:  0.5853\n",
            "Epoch24 | Batch: 12 Loss:  0.6440\n",
            "Epoch24 | Batch: 13 Loss:  0.6830\n",
            "Epoch24 | Batch: 14 Loss:  0.6234\n",
            "Epoch24 | Batch: 15 Loss:  0.7236\n",
            "Epoch24 | Batch: 16 Loss:  0.6238\n",
            "Epoch24 | Batch: 17 Loss:  0.6242\n",
            "Epoch24 | Batch: 18 Loss:  0.6432\n",
            "Epoch24 | Batch: 19 Loss:  0.6627\n",
            "Epoch24 | Batch: 20 Loss:  0.5653\n",
            "Epoch24 | Batch: 21 Loss:  0.6035\n",
            "Epoch24 | Batch: 22 Loss:  0.6025\n",
            "Epoch24 | Batch: 23 Loss:  0.7249\n",
            "Epoch24 | Batch: 24 Loss:  0.6745\n",
            "Epoch25 | Batch: 1 Loss:  0.7033\n",
            "Epoch25 | Batch: 2 Loss:  0.6038\n",
            "Epoch25 | Batch: 3 Loss:  0.6039\n",
            "Epoch25 | Batch: 4 Loss:  0.6640\n",
            "Epoch25 | Batch: 5 Loss:  0.5830\n",
            "Epoch25 | Batch: 6 Loss:  0.7238\n",
            "Epoch25 | Batch: 7 Loss:  0.7022\n",
            "Epoch25 | Batch: 8 Loss:  0.5848\n",
            "Epoch25 | Batch: 9 Loss:  0.6233\n",
            "Epoch25 | Batch: 10 Loss:  0.6233\n",
            "Epoch25 | Batch: 11 Loss:  0.6830\n",
            "Epoch25 | Batch: 12 Loss:  0.7025\n",
            "Epoch25 | Batch: 13 Loss:  0.6046\n",
            "Epoch25 | Batch: 14 Loss:  0.6438\n",
            "Epoch25 | Batch: 15 Loss:  0.6037\n",
            "Epoch25 | Batch: 16 Loss:  0.7026\n",
            "Epoch25 | Batch: 17 Loss:  0.6832\n",
            "Epoch25 | Batch: 18 Loss:  0.6435\n",
            "Epoch25 | Batch: 19 Loss:  0.6632\n",
            "Epoch25 | Batch: 20 Loss:  0.6431\n",
            "Epoch25 | Batch: 21 Loss:  0.6239\n",
            "Epoch25 | Batch: 22 Loss:  0.6246\n",
            "Epoch25 | Batch: 23 Loss:  0.6632\n",
            "Epoch25 | Batch: 24 Loss:  0.5637\n",
            "Epoch26 | Batch: 1 Loss:  0.5840\n",
            "Epoch26 | Batch: 2 Loss:  0.6631\n",
            "Epoch26 | Batch: 3 Loss:  0.6428\n",
            "Epoch26 | Batch: 4 Loss:  0.6629\n",
            "Epoch26 | Batch: 5 Loss:  0.6635\n",
            "Epoch26 | Batch: 6 Loss:  0.7034\n",
            "Epoch26 | Batch: 7 Loss:  0.6427\n",
            "Epoch26 | Batch: 8 Loss:  0.6824\n",
            "Epoch26 | Batch: 9 Loss:  0.7028\n",
            "Epoch26 | Batch: 10 Loss:  0.5669\n",
            "Epoch26 | Batch: 11 Loss:  0.5841\n",
            "Epoch26 | Batch: 12 Loss:  0.5839\n",
            "Epoch26 | Batch: 13 Loss:  0.6629\n",
            "Epoch26 | Batch: 14 Loss:  0.7039\n",
            "Epoch26 | Batch: 15 Loss:  0.6232\n",
            "Epoch26 | Batch: 16 Loss:  0.6433\n",
            "Epoch26 | Batch: 17 Loss:  0.6235\n",
            "Epoch26 | Batch: 18 Loss:  0.6433\n",
            "Epoch26 | Batch: 19 Loss:  0.6635\n",
            "Epoch26 | Batch: 20 Loss:  0.7226\n",
            "Epoch26 | Batch: 21 Loss:  0.6632\n",
            "Epoch26 | Batch: 22 Loss:  0.6039\n",
            "Epoch26 | Batch: 23 Loss:  0.6034\n",
            "Epoch26 | Batch: 24 Loss:  0.6466\n",
            "Epoch27 | Batch: 1 Loss:  0.6436\n",
            "Epoch27 | Batch: 2 Loss:  0.6429\n",
            "Epoch27 | Batch: 3 Loss:  0.6229\n",
            "Epoch27 | Batch: 4 Loss:  0.6630\n",
            "Epoch27 | Batch: 5 Loss:  0.5846\n",
            "Epoch27 | Batch: 6 Loss:  0.6636\n",
            "Epoch27 | Batch: 7 Loss:  0.7035\n",
            "Epoch27 | Batch: 8 Loss:  0.6233\n",
            "Epoch27 | Batch: 9 Loss:  0.6827\n",
            "Epoch27 | Batch: 10 Loss:  0.6038\n",
            "Epoch27 | Batch: 11 Loss:  0.6435\n",
            "Epoch27 | Batch: 12 Loss:  0.5826\n",
            "Epoch27 | Batch: 13 Loss:  0.6428\n",
            "Epoch27 | Batch: 14 Loss:  0.6436\n",
            "Epoch27 | Batch: 15 Loss:  0.5815\n",
            "Epoch27 | Batch: 16 Loss:  0.6429\n",
            "Epoch27 | Batch: 17 Loss:  0.7675\n",
            "Epoch27 | Batch: 18 Loss:  0.6837\n",
            "Epoch27 | Batch: 19 Loss:  0.6636\n",
            "Epoch27 | Batch: 20 Loss:  0.5835\n",
            "Epoch27 | Batch: 21 Loss:  0.6227\n",
            "Epoch27 | Batch: 22 Loss:  0.6228\n",
            "Epoch27 | Batch: 23 Loss:  0.6627\n",
            "Epoch27 | Batch: 24 Loss:  0.7310\n",
            "Epoch28 | Batch: 1 Loss:  0.6235\n",
            "Epoch28 | Batch: 2 Loss:  0.6434\n",
            "Epoch28 | Batch: 3 Loss:  0.6830\n",
            "Epoch28 | Batch: 4 Loss:  0.6826\n",
            "Epoch28 | Batch: 5 Loss:  0.6240\n",
            "Epoch28 | Batch: 6 Loss:  0.7206\n",
            "Epoch28 | Batch: 7 Loss:  0.6627\n",
            "Epoch28 | Batch: 8 Loss:  0.6623\n",
            "Epoch28 | Batch: 9 Loss:  0.6802\n",
            "Epoch28 | Batch: 10 Loss:  0.7003\n",
            "Epoch28 | Batch: 11 Loss:  0.6058\n",
            "Epoch28 | Batch: 12 Loss:  0.5871\n",
            "Epoch28 | Batch: 13 Loss:  0.5669\n",
            "Epoch28 | Batch: 14 Loss:  0.6423\n",
            "Epoch28 | Batch: 15 Loss:  0.7023\n",
            "Epoch28 | Batch: 16 Loss:  0.6823\n",
            "Epoch28 | Batch: 17 Loss:  0.5853\n",
            "Epoch28 | Batch: 18 Loss:  0.6631\n",
            "Epoch28 | Batch: 19 Loss:  0.7011\n",
            "Epoch28 | Batch: 20 Loss:  0.6054\n",
            "Epoch28 | Batch: 21 Loss:  0.6247\n",
            "Epoch28 | Batch: 22 Loss:  0.5451\n",
            "Epoch28 | Batch: 23 Loss:  0.6831\n",
            "Epoch28 | Batch: 24 Loss:  0.5905\n",
            "Epoch29 | Batch: 1 Loss:  0.6429\n",
            "Epoch29 | Batch: 2 Loss:  0.7042\n",
            "Epoch29 | Batch: 3 Loss:  0.6425\n",
            "Epoch29 | Batch: 4 Loss:  0.6222\n",
            "Epoch29 | Batch: 5 Loss:  0.7031\n",
            "Epoch29 | Batch: 6 Loss:  0.6434\n",
            "Epoch29 | Batch: 7 Loss:  0.6425\n",
            "Epoch29 | Batch: 8 Loss:  0.6036\n",
            "Epoch29 | Batch: 9 Loss:  0.7819\n",
            "Epoch29 | Batch: 10 Loss:  0.6434\n",
            "Epoch29 | Batch: 11 Loss:  0.6636\n",
            "Epoch29 | Batch: 12 Loss:  0.6427\n",
            "Epoch29 | Batch: 13 Loss:  0.6244\n",
            "Epoch29 | Batch: 14 Loss:  0.6417\n",
            "Epoch29 | Batch: 15 Loss:  0.6434\n",
            "Epoch29 | Batch: 16 Loss:  0.5855\n",
            "Epoch29 | Batch: 17 Loss:  0.6239\n",
            "Epoch29 | Batch: 18 Loss:  0.6234\n",
            "Epoch29 | Batch: 19 Loss:  0.6423\n",
            "Epoch29 | Batch: 20 Loss:  0.6239\n",
            "Epoch29 | Batch: 21 Loss:  0.6429\n",
            "Epoch29 | Batch: 22 Loss:  0.6032\n",
            "Epoch29 | Batch: 23 Loss:  0.6425\n",
            "Epoch29 | Batch: 24 Loss:  0.6451\n",
            "Epoch30 | Batch: 1 Loss:  0.6225\n",
            "Epoch30 | Batch: 2 Loss:  0.6017\n",
            "Epoch30 | Batch: 3 Loss:  0.7049\n",
            "Epoch30 | Batch: 4 Loss:  0.6429\n",
            "Epoch30 | Batch: 5 Loss:  0.7832\n",
            "Epoch30 | Batch: 6 Loss:  0.6821\n",
            "Epoch30 | Batch: 7 Loss:  0.6634\n",
            "Epoch30 | Batch: 8 Loss:  0.6235\n",
            "Epoch30 | Batch: 9 Loss:  0.6622\n",
            "Epoch30 | Batch: 10 Loss:  0.6810\n",
            "Epoch30 | Batch: 11 Loss:  0.6808\n",
            "Epoch30 | Batch: 12 Loss:  0.5673\n",
            "Epoch30 | Batch: 13 Loss:  0.5481\n",
            "Epoch30 | Batch: 14 Loss:  0.6402\n",
            "Epoch30 | Batch: 15 Loss:  0.5837\n",
            "Epoch30 | Batch: 16 Loss:  0.7051\n",
            "Epoch30 | Batch: 17 Loss:  0.6232\n",
            "Epoch30 | Batch: 18 Loss:  0.6428\n",
            "Epoch30 | Batch: 19 Loss:  0.6223\n",
            "Epoch30 | Batch: 20 Loss:  0.6034\n",
            "Epoch30 | Batch: 21 Loss:  0.5817\n",
            "Epoch30 | Batch: 22 Loss:  0.6430\n",
            "Epoch30 | Batch: 23 Loss:  0.6651\n",
            "Epoch30 | Batch: 24 Loss:  0.7282\n",
            "Epoch31 | Batch: 1 Loss:  0.6432\n",
            "Epoch31 | Batch: 2 Loss:  0.5408\n",
            "Epoch31 | Batch: 3 Loss:  0.6833\n",
            "Epoch31 | Batch: 4 Loss:  0.6636\n",
            "Epoch31 | Batch: 5 Loss:  0.6207\n",
            "Epoch31 | Batch: 6 Loss:  0.6209\n",
            "Epoch31 | Batch: 7 Loss:  0.7042\n",
            "Epoch31 | Batch: 8 Loss:  0.5820\n",
            "Epoch31 | Batch: 9 Loss:  0.7044\n",
            "Epoch31 | Batch: 10 Loss:  0.5817\n",
            "Epoch31 | Batch: 11 Loss:  0.6026\n",
            "Epoch31 | Batch: 12 Loss:  0.6434\n",
            "Epoch31 | Batch: 13 Loss:  0.6435\n",
            "Epoch31 | Batch: 14 Loss:  0.6627\n",
            "Epoch31 | Batch: 15 Loss:  0.6437\n",
            "Epoch31 | Batch: 16 Loss:  0.6632\n",
            "Epoch31 | Batch: 17 Loss:  0.6850\n",
            "Epoch31 | Batch: 18 Loss:  0.7034\n",
            "Epoch31 | Batch: 19 Loss:  0.7228\n",
            "Epoch31 | Batch: 20 Loss:  0.6430\n",
            "Epoch31 | Batch: 21 Loss:  0.6820\n",
            "Epoch31 | Batch: 22 Loss:  0.6034\n",
            "Epoch31 | Batch: 23 Loss:  0.6041\n",
            "Epoch31 | Batch: 24 Loss:  0.6172\n",
            "Epoch32 | Batch: 1 Loss:  0.6221\n",
            "Epoch32 | Batch: 2 Loss:  0.5622\n",
            "Epoch32 | Batch: 3 Loss:  0.6832\n",
            "Epoch32 | Batch: 4 Loss:  0.7627\n",
            "Epoch32 | Batch: 5 Loss:  0.6615\n",
            "Epoch32 | Batch: 6 Loss:  0.6615\n",
            "Epoch32 | Batch: 7 Loss:  0.6814\n",
            "Epoch32 | Batch: 8 Loss:  0.5856\n",
            "Epoch32 | Batch: 9 Loss:  0.6439\n",
            "Epoch32 | Batch: 10 Loss:  0.6026\n",
            "Epoch32 | Batch: 11 Loss:  0.7431\n",
            "Epoch32 | Batch: 12 Loss:  0.5839\n",
            "Epoch32 | Batch: 13 Loss:  0.5634\n",
            "Epoch32 | Batch: 14 Loss:  0.6233\n",
            "Epoch32 | Batch: 15 Loss:  0.6210\n",
            "Epoch32 | Batch: 16 Loss:  0.6842\n",
            "Epoch32 | Batch: 17 Loss:  0.6839\n",
            "Epoch32 | Batch: 18 Loss:  0.6218\n",
            "Epoch32 | Batch: 19 Loss:  0.6218\n",
            "Epoch32 | Batch: 20 Loss:  0.6207\n",
            "Epoch32 | Batch: 21 Loss:  0.7245\n",
            "Epoch32 | Batch: 22 Loss:  0.6640\n",
            "Epoch32 | Batch: 23 Loss:  0.5820\n",
            "Epoch32 | Batch: 24 Loss:  0.6747\n",
            "Epoch33 | Batch: 1 Loss:  0.6028\n",
            "Epoch33 | Batch: 2 Loss:  0.6628\n",
            "Epoch33 | Batch: 3 Loss:  0.7621\n",
            "Epoch33 | Batch: 4 Loss:  0.5459\n",
            "Epoch33 | Batch: 5 Loss:  0.5824\n",
            "Epoch33 | Batch: 6 Loss:  0.6000\n",
            "Epoch33 | Batch: 7 Loss:  0.6408\n",
            "Epoch33 | Batch: 8 Loss:  0.7036\n",
            "Epoch33 | Batch: 9 Loss:  0.6028\n",
            "Epoch33 | Batch: 10 Loss:  0.7040\n",
            "Epoch33 | Batch: 11 Loss:  0.6443\n",
            "Epoch33 | Batch: 12 Loss:  0.6019\n",
            "Epoch33 | Batch: 13 Loss:  0.6412\n",
            "Epoch33 | Batch: 14 Loss:  0.6231\n",
            "Epoch33 | Batch: 15 Loss:  0.6826\n",
            "Epoch33 | Batch: 16 Loss:  0.6223\n",
            "Epoch33 | Batch: 17 Loss:  0.8257\n",
            "Epoch33 | Batch: 18 Loss:  0.5259\n",
            "Epoch33 | Batch: 19 Loss:  0.5215\n",
            "Epoch33 | Batch: 20 Loss:  0.6638\n",
            "Epoch33 | Batch: 21 Loss:  0.6825\n",
            "Epoch33 | Batch: 22 Loss:  0.6420\n",
            "Epoch33 | Batch: 23 Loss:  0.6418\n",
            "Epoch33 | Batch: 24 Loss:  0.7845\n",
            "Epoch34 | Batch: 1 Loss:  0.6411\n",
            "Epoch34 | Batch: 2 Loss:  0.7420\n",
            "Epoch34 | Batch: 3 Loss:  0.6611\n",
            "Epoch34 | Batch: 4 Loss:  0.6806\n",
            "Epoch34 | Batch: 5 Loss:  0.6213\n",
            "Epoch34 | Batch: 6 Loss:  0.6037\n",
            "Epoch34 | Batch: 7 Loss:  0.5856\n",
            "Epoch34 | Batch: 8 Loss:  0.6042\n",
            "Epoch34 | Batch: 9 Loss:  0.6428\n",
            "Epoch34 | Batch: 10 Loss:  0.6234\n",
            "Epoch34 | Batch: 11 Loss:  0.7396\n",
            "Epoch34 | Batch: 12 Loss:  0.6223\n",
            "Epoch34 | Batch: 13 Loss:  0.6416\n",
            "Epoch34 | Batch: 14 Loss:  0.6217\n",
            "Epoch34 | Batch: 15 Loss:  0.6436\n",
            "Epoch34 | Batch: 16 Loss:  0.6626\n",
            "Epoch34 | Batch: 17 Loss:  0.6235\n",
            "Epoch34 | Batch: 18 Loss:  0.7018\n",
            "Epoch34 | Batch: 19 Loss:  0.6226\n",
            "Epoch34 | Batch: 20 Loss:  0.6798\n",
            "Epoch34 | Batch: 21 Loss:  0.5645\n",
            "Epoch34 | Batch: 22 Loss:  0.6422\n",
            "Epoch34 | Batch: 23 Loss:  0.5986\n",
            "Epoch34 | Batch: 24 Loss:  0.6990\n",
            "Epoch35 | Batch: 1 Loss:  0.6219\n",
            "Epoch35 | Batch: 2 Loss:  0.6406\n",
            "Epoch35 | Batch: 3 Loss:  0.6800\n",
            "Epoch35 | Batch: 4 Loss:  0.6257\n",
            "Epoch35 | Batch: 5 Loss:  0.5999\n",
            "Epoch35 | Batch: 6 Loss:  0.6805\n",
            "Epoch35 | Batch: 7 Loss:  0.6005\n",
            "Epoch35 | Batch: 8 Loss:  0.6009\n",
            "Epoch35 | Batch: 9 Loss:  0.6178\n",
            "Epoch35 | Batch: 10 Loss:  0.5396\n",
            "Epoch35 | Batch: 11 Loss:  0.5996\n",
            "Epoch35 | Batch: 12 Loss:  0.7256\n",
            "Epoch35 | Batch: 13 Loss:  0.6648\n",
            "Epoch35 | Batch: 14 Loss:  0.6609\n",
            "Epoch35 | Batch: 15 Loss:  0.6216\n",
            "Epoch35 | Batch: 16 Loss:  0.7253\n",
            "Epoch35 | Batch: 17 Loss:  0.6031\n",
            "Epoch35 | Batch: 18 Loss:  0.7012\n",
            "Epoch35 | Batch: 19 Loss:  0.6420\n",
            "Epoch35 | Batch: 20 Loss:  0.7251\n",
            "Epoch35 | Batch: 21 Loss:  0.5848\n",
            "Epoch35 | Batch: 22 Loss:  0.6212\n",
            "Epoch35 | Batch: 23 Loss:  0.5784\n",
            "Epoch35 | Batch: 24 Loss:  0.8405\n",
            "Epoch36 | Batch: 1 Loss:  0.5816\n",
            "Epoch36 | Batch: 2 Loss:  0.6392\n",
            "Epoch36 | Batch: 3 Loss:  0.6430\n",
            "Epoch36 | Batch: 4 Loss:  0.6417\n",
            "Epoch36 | Batch: 5 Loss:  0.6207\n",
            "Epoch36 | Batch: 6 Loss:  0.6190\n",
            "Epoch36 | Batch: 7 Loss:  0.6003\n",
            "Epoch36 | Batch: 8 Loss:  0.7839\n",
            "Epoch36 | Batch: 9 Loss:  0.7011\n",
            "Epoch36 | Batch: 10 Loss:  0.6793\n",
            "Epoch36 | Batch: 11 Loss:  0.6582\n",
            "Epoch36 | Batch: 12 Loss:  0.6051\n",
            "Epoch36 | Batch: 13 Loss:  0.6771\n",
            "Epoch36 | Batch: 14 Loss:  0.5824\n",
            "Epoch36 | Batch: 15 Loss:  0.7201\n",
            "Epoch36 | Batch: 16 Loss:  0.6032\n",
            "Epoch36 | Batch: 17 Loss:  0.5826\n",
            "Epoch36 | Batch: 18 Loss:  0.6432\n",
            "Epoch36 | Batch: 19 Loss:  0.6994\n",
            "Epoch36 | Batch: 20 Loss:  0.6776\n",
            "Epoch36 | Batch: 21 Loss:  0.6806\n",
            "Epoch36 | Batch: 22 Loss:  0.6016\n",
            "Epoch36 | Batch: 23 Loss:  0.5454\n",
            "Epoch36 | Batch: 24 Loss:  0.6472\n",
            "Epoch37 | Batch: 1 Loss:  0.6625\n",
            "Epoch37 | Batch: 2 Loss:  0.6756\n",
            "Epoch37 | Batch: 3 Loss:  0.6417\n",
            "Epoch37 | Batch: 4 Loss:  0.6195\n",
            "Epoch37 | Batch: 5 Loss:  0.6416\n",
            "Epoch37 | Batch: 6 Loss:  0.7166\n",
            "Epoch37 | Batch: 7 Loss:  0.5310\n",
            "Epoch37 | Batch: 8 Loss:  0.6182\n",
            "Epoch37 | Batch: 9 Loss:  0.5998\n",
            "Epoch37 | Batch: 10 Loss:  0.6969\n",
            "Epoch37 | Batch: 11 Loss:  0.6650\n",
            "Epoch37 | Batch: 12 Loss:  0.5767\n",
            "Epoch37 | Batch: 13 Loss:  0.6837\n",
            "Epoch37 | Batch: 14 Loss:  0.6741\n",
            "Epoch37 | Batch: 15 Loss:  0.6601\n",
            "Epoch37 | Batch: 16 Loss:  0.6598\n",
            "Epoch37 | Batch: 17 Loss:  0.7227\n",
            "Epoch37 | Batch: 18 Loss:  0.6954\n",
            "Epoch37 | Batch: 19 Loss:  0.6178\n",
            "Epoch37 | Batch: 20 Loss:  0.6014\n",
            "Epoch37 | Batch: 21 Loss:  0.6771\n",
            "Epoch37 | Batch: 22 Loss:  0.6223\n",
            "Epoch37 | Batch: 23 Loss:  0.5471\n",
            "Epoch37 | Batch: 24 Loss:  0.5871\n",
            "Epoch38 | Batch: 1 Loss:  0.6576\n",
            "Epoch38 | Batch: 2 Loss:  0.6426\n",
            "Epoch38 | Batch: 3 Loss:  0.6369\n",
            "Epoch38 | Batch: 4 Loss:  0.5968\n",
            "Epoch38 | Batch: 5 Loss:  0.6986\n",
            "Epoch38 | Batch: 6 Loss:  0.6583\n",
            "Epoch38 | Batch: 7 Loss:  0.6765\n",
            "Epoch38 | Batch: 8 Loss:  0.6053\n",
            "Epoch38 | Batch: 9 Loss:  0.5982\n",
            "Epoch38 | Batch: 10 Loss:  0.6186\n",
            "Epoch38 | Batch: 11 Loss:  0.7032\n",
            "Epoch38 | Batch: 12 Loss:  0.5974\n",
            "Epoch38 | Batch: 13 Loss:  0.6561\n",
            "Epoch38 | Batch: 14 Loss:  0.6449\n",
            "Epoch38 | Batch: 15 Loss:  0.6318\n",
            "Epoch38 | Batch: 16 Loss:  0.6163\n",
            "Epoch38 | Batch: 17 Loss:  0.5832\n",
            "Epoch38 | Batch: 18 Loss:  0.7545\n",
            "Epoch38 | Batch: 19 Loss:  0.6525\n",
            "Epoch38 | Batch: 20 Loss:  0.6737\n",
            "Epoch38 | Batch: 21 Loss:  0.5879\n",
            "Epoch38 | Batch: 22 Loss:  0.6389\n",
            "Epoch38 | Batch: 23 Loss:  0.6347\n",
            "Epoch38 | Batch: 24 Loss:  0.5862\n",
            "Epoch39 | Batch: 1 Loss:  0.6986\n",
            "Epoch39 | Batch: 2 Loss:  0.7163\n",
            "Epoch39 | Batch: 3 Loss:  0.6635\n",
            "Epoch39 | Batch: 4 Loss:  0.6560\n",
            "Epoch39 | Batch: 5 Loss:  0.6209\n",
            "Epoch39 | Batch: 6 Loss:  0.6194\n",
            "Epoch39 | Batch: 7 Loss:  0.6015\n",
            "Epoch39 | Batch: 8 Loss:  0.7375\n",
            "Epoch39 | Batch: 9 Loss:  0.5635\n",
            "Epoch39 | Batch: 10 Loss:  0.6338\n",
            "Epoch39 | Batch: 11 Loss:  0.5460\n",
            "Epoch39 | Batch: 12 Loss:  0.6911\n",
            "Epoch39 | Batch: 13 Loss:  0.5744\n",
            "Epoch39 | Batch: 14 Loss:  0.5790\n",
            "Epoch39 | Batch: 15 Loss:  0.7165\n",
            "Epoch39 | Batch: 16 Loss:  0.7702\n",
            "Epoch39 | Batch: 17 Loss:  0.6491\n",
            "Epoch39 | Batch: 18 Loss:  0.6861\n",
            "Epoch39 | Batch: 19 Loss:  0.6386\n",
            "Epoch39 | Batch: 20 Loss:  0.5920\n",
            "Epoch39 | Batch: 21 Loss:  0.5582\n",
            "Epoch39 | Batch: 22 Loss:  0.5516\n",
            "Epoch39 | Batch: 23 Loss:  0.5899\n",
            "Epoch39 | Batch: 24 Loss:  0.6600\n",
            "Epoch40 | Batch: 1 Loss:  0.6252\n",
            "Epoch40 | Batch: 2 Loss:  0.7085\n",
            "Epoch40 | Batch: 3 Loss:  0.5716\n",
            "Epoch40 | Batch: 4 Loss:  0.6702\n",
            "Epoch40 | Batch: 5 Loss:  0.6759\n",
            "Epoch40 | Batch: 6 Loss:  0.5151\n",
            "Epoch40 | Batch: 7 Loss:  0.7387\n",
            "Epoch40 | Batch: 8 Loss:  0.6126\n",
            "Epoch40 | Batch: 9 Loss:  0.6592\n",
            "Epoch40 | Batch: 10 Loss:  0.5738\n",
            "Epoch40 | Batch: 11 Loss:  0.6366\n",
            "Epoch40 | Batch: 12 Loss:  0.6547\n",
            "Epoch40 | Batch: 13 Loss:  0.6003\n",
            "Epoch40 | Batch: 14 Loss:  0.6830\n",
            "Epoch40 | Batch: 15 Loss:  0.6277\n",
            "Epoch40 | Batch: 16 Loss:  0.5919\n",
            "Epoch40 | Batch: 17 Loss:  0.6525\n",
            "Epoch40 | Batch: 18 Loss:  0.6628\n",
            "Epoch40 | Batch: 19 Loss:  0.6059\n",
            "Epoch40 | Batch: 20 Loss:  0.6534\n",
            "Epoch40 | Batch: 21 Loss:  0.6176\n",
            "Epoch40 | Batch: 22 Loss:  0.5805\n",
            "Epoch40 | Batch: 23 Loss:  0.5930\n",
            "Epoch40 | Batch: 24 Loss:  0.7413\n",
            "Epoch41 | Batch: 1 Loss:  0.5927\n",
            "Epoch41 | Batch: 2 Loss:  0.6153\n",
            "Epoch41 | Batch: 3 Loss:  0.5944\n",
            "Epoch41 | Batch: 4 Loss:  0.7008\n",
            "Epoch41 | Batch: 5 Loss:  0.5235\n",
            "Epoch41 | Batch: 6 Loss:  0.6697\n",
            "Epoch41 | Batch: 7 Loss:  0.6347\n",
            "Epoch41 | Batch: 8 Loss:  0.6260\n",
            "Epoch41 | Batch: 9 Loss:  0.6180\n",
            "Epoch41 | Batch: 10 Loss:  0.6584\n",
            "Epoch41 | Batch: 11 Loss:  0.6439\n",
            "Epoch41 | Batch: 12 Loss:  0.5611\n",
            "Epoch41 | Batch: 13 Loss:  0.7559\n",
            "Epoch41 | Batch: 14 Loss:  0.6880\n",
            "Epoch41 | Batch: 15 Loss:  0.6215\n",
            "Epoch41 | Batch: 16 Loss:  0.6710\n",
            "Epoch41 | Batch: 17 Loss:  0.6707\n",
            "Epoch41 | Batch: 18 Loss:  0.6219\n",
            "Epoch41 | Batch: 19 Loss:  0.5544\n",
            "Epoch41 | Batch: 20 Loss:  0.6831\n",
            "Epoch41 | Batch: 21 Loss:  0.6252\n",
            "Epoch41 | Batch: 22 Loss:  0.6246\n",
            "Epoch41 | Batch: 23 Loss:  0.5341\n",
            "Epoch41 | Batch: 24 Loss:  0.6268\n",
            "Epoch42 | Batch: 1 Loss:  0.5917\n",
            "Epoch42 | Batch: 2 Loss:  0.6518\n",
            "Epoch42 | Batch: 3 Loss:  0.6470\n",
            "Epoch42 | Batch: 4 Loss:  0.6558\n",
            "Epoch42 | Batch: 5 Loss:  0.5969\n",
            "Epoch42 | Batch: 6 Loss:  0.5972\n",
            "Epoch42 | Batch: 7 Loss:  0.6009\n",
            "Epoch42 | Batch: 8 Loss:  0.6124\n",
            "Epoch42 | Batch: 9 Loss:  0.5659\n",
            "Epoch42 | Batch: 10 Loss:  0.5377\n",
            "Epoch42 | Batch: 11 Loss:  0.6552\n",
            "Epoch42 | Batch: 12 Loss:  0.6424\n",
            "Epoch42 | Batch: 13 Loss:  0.6805\n",
            "Epoch42 | Batch: 14 Loss:  0.6713\n",
            "Epoch42 | Batch: 15 Loss:  0.6199\n",
            "Epoch42 | Batch: 16 Loss:  0.6065\n",
            "Epoch42 | Batch: 17 Loss:  0.7134\n",
            "Epoch42 | Batch: 18 Loss:  0.6219\n",
            "Epoch42 | Batch: 19 Loss:  0.5998\n",
            "Epoch42 | Batch: 20 Loss:  0.6523\n",
            "Epoch42 | Batch: 21 Loss:  0.6511\n",
            "Epoch42 | Batch: 22 Loss:  0.6150\n",
            "Epoch42 | Batch: 23 Loss:  0.6107\n",
            "Epoch42 | Batch: 24 Loss:  0.5476\n",
            "Epoch43 | Batch: 1 Loss:  0.5929\n",
            "Epoch43 | Batch: 2 Loss:  0.6552\n",
            "Epoch43 | Batch: 3 Loss:  0.5826\n",
            "Epoch43 | Batch: 4 Loss:  0.7074\n",
            "Epoch43 | Batch: 5 Loss:  0.5338\n",
            "Epoch43 | Batch: 6 Loss:  0.6043\n",
            "Epoch43 | Batch: 7 Loss:  0.6199\n",
            "Epoch43 | Batch: 8 Loss:  0.5579\n",
            "Epoch43 | Batch: 9 Loss:  0.5401\n",
            "Epoch43 | Batch: 10 Loss:  0.6275\n",
            "Epoch43 | Batch: 11 Loss:  0.6410\n",
            "Epoch43 | Batch: 12 Loss:  0.6517\n",
            "Epoch43 | Batch: 13 Loss:  0.6202\n",
            "Epoch43 | Batch: 14 Loss:  0.5604\n",
            "Epoch43 | Batch: 15 Loss:  0.6304\n",
            "Epoch43 | Batch: 16 Loss:  0.6576\n",
            "Epoch43 | Batch: 17 Loss:  0.5900\n",
            "Epoch43 | Batch: 18 Loss:  0.6570\n",
            "Epoch43 | Batch: 19 Loss:  0.6016\n",
            "Epoch43 | Batch: 20 Loss:  0.6370\n",
            "Epoch43 | Batch: 21 Loss:  0.5581\n",
            "Epoch43 | Batch: 22 Loss:  0.6623\n",
            "Epoch43 | Batch: 23 Loss:  0.7059\n",
            "Epoch43 | Batch: 24 Loss:  0.5637\n",
            "Epoch44 | Batch: 1 Loss:  0.5156\n",
            "Epoch44 | Batch: 2 Loss:  0.6791\n",
            "Epoch44 | Batch: 3 Loss:  0.6301\n",
            "Epoch44 | Batch: 4 Loss:  0.6765\n",
            "Epoch44 | Batch: 5 Loss:  0.6328\n",
            "Epoch44 | Batch: 6 Loss:  0.5988\n",
            "Epoch44 | Batch: 7 Loss:  0.6103\n",
            "Epoch44 | Batch: 8 Loss:  0.5129\n",
            "Epoch44 | Batch: 9 Loss:  0.6323\n",
            "Epoch44 | Batch: 10 Loss:  0.5603\n",
            "Epoch44 | Batch: 11 Loss:  0.6161\n",
            "Epoch44 | Batch: 12 Loss:  0.6299\n",
            "Epoch44 | Batch: 13 Loss:  0.5277\n",
            "Epoch44 | Batch: 14 Loss:  0.5458\n",
            "Epoch44 | Batch: 15 Loss:  0.4749\n",
            "Epoch44 | Batch: 16 Loss:  0.6378\n",
            "Epoch44 | Batch: 17 Loss:  0.5504\n",
            "Epoch44 | Batch: 18 Loss:  0.6559\n",
            "Epoch44 | Batch: 19 Loss:  0.5739\n",
            "Epoch44 | Batch: 20 Loss:  0.6934\n",
            "Epoch44 | Batch: 21 Loss:  0.6895\n",
            "Epoch44 | Batch: 22 Loss:  0.6062\n",
            "Epoch44 | Batch: 23 Loss:  0.6626\n",
            "Epoch44 | Batch: 24 Loss:  0.6034\n",
            "Epoch45 | Batch: 1 Loss:  0.5146\n",
            "Epoch45 | Batch: 2 Loss:  0.5674\n",
            "Epoch45 | Batch: 3 Loss:  0.6374\n",
            "Epoch45 | Batch: 4 Loss:  0.5611\n",
            "Epoch45 | Batch: 5 Loss:  0.5620\n",
            "Epoch45 | Batch: 6 Loss:  0.6681\n",
            "Epoch45 | Batch: 7 Loss:  0.6249\n",
            "Epoch45 | Batch: 8 Loss:  0.6365\n",
            "Epoch45 | Batch: 9 Loss:  0.5854\n",
            "Epoch45 | Batch: 10 Loss:  0.5715\n",
            "Epoch45 | Batch: 11 Loss:  0.6085\n",
            "Epoch45 | Batch: 12 Loss:  0.6099\n",
            "Epoch45 | Batch: 13 Loss:  0.6140\n",
            "Epoch45 | Batch: 14 Loss:  0.5645\n",
            "Epoch45 | Batch: 15 Loss:  0.5278\n",
            "Epoch45 | Batch: 16 Loss:  0.5611\n",
            "Epoch45 | Batch: 17 Loss:  0.6384\n",
            "Epoch45 | Batch: 18 Loss:  0.6289\n",
            "Epoch45 | Batch: 19 Loss:  0.6197\n",
            "Epoch45 | Batch: 20 Loss:  0.5038\n",
            "Epoch45 | Batch: 21 Loss:  0.5054\n",
            "Epoch45 | Batch: 22 Loss:  0.5902\n",
            "Epoch45 | Batch: 23 Loss:  0.6834\n",
            "Epoch45 | Batch: 24 Loss:  0.5738\n",
            "Epoch46 | Batch: 1 Loss:  0.6080\n",
            "Epoch46 | Batch: 2 Loss:  0.5933\n",
            "Epoch46 | Batch: 3 Loss:  0.6063\n",
            "Epoch46 | Batch: 4 Loss:  0.5706\n",
            "Epoch46 | Batch: 5 Loss:  0.5280\n",
            "Epoch46 | Batch: 6 Loss:  0.5600\n",
            "Epoch46 | Batch: 7 Loss:  0.6893\n",
            "Epoch46 | Batch: 8 Loss:  0.5056\n",
            "Epoch46 | Batch: 9 Loss:  0.6491\n",
            "Epoch46 | Batch: 10 Loss:  0.5574\n",
            "Epoch46 | Batch: 11 Loss:  0.5411\n",
            "Epoch46 | Batch: 12 Loss:  0.4904\n",
            "Epoch46 | Batch: 13 Loss:  0.5390\n",
            "Epoch46 | Batch: 14 Loss:  0.6580\n",
            "Epoch46 | Batch: 15 Loss:  0.5778\n",
            "Epoch46 | Batch: 16 Loss:  0.6408\n",
            "Epoch46 | Batch: 17 Loss:  0.5049\n",
            "Epoch46 | Batch: 18 Loss:  0.5456\n",
            "Epoch46 | Batch: 19 Loss:  0.5056\n",
            "Epoch46 | Batch: 20 Loss:  0.4973\n",
            "Epoch46 | Batch: 21 Loss:  0.6058\n",
            "Epoch46 | Batch: 22 Loss:  0.5273\n",
            "Epoch46 | Batch: 23 Loss:  0.5548\n",
            "Epoch46 | Batch: 24 Loss:  0.6841\n",
            "Epoch47 | Batch: 1 Loss:  0.6250\n",
            "Epoch47 | Batch: 2 Loss:  0.5239\n",
            "Epoch47 | Batch: 3 Loss:  0.5679\n",
            "Epoch47 | Batch: 4 Loss:  0.4027\n",
            "Epoch47 | Batch: 5 Loss:  0.5110\n",
            "Epoch47 | Batch: 6 Loss:  0.5347\n",
            "Epoch47 | Batch: 7 Loss:  0.6242\n",
            "Epoch47 | Batch: 8 Loss:  0.5728\n",
            "Epoch47 | Batch: 9 Loss:  0.5246\n",
            "Epoch47 | Batch: 10 Loss:  0.5349\n",
            "Epoch47 | Batch: 11 Loss:  0.6181\n",
            "Epoch47 | Batch: 12 Loss:  0.5976\n",
            "Epoch47 | Batch: 13 Loss:  0.4738\n",
            "Epoch47 | Batch: 14 Loss:  0.5047\n",
            "Epoch47 | Batch: 15 Loss:  0.5258\n",
            "Epoch47 | Batch: 16 Loss:  0.6732\n",
            "Epoch47 | Batch: 17 Loss:  0.5349\n",
            "Epoch47 | Batch: 18 Loss:  0.6229\n",
            "Epoch47 | Batch: 19 Loss:  0.5387\n",
            "Epoch47 | Batch: 20 Loss:  0.5882\n",
            "Epoch47 | Batch: 21 Loss:  0.5079\n",
            "Epoch47 | Batch: 22 Loss:  0.6090\n",
            "Epoch47 | Batch: 23 Loss:  0.5113\n",
            "Epoch47 | Batch: 24 Loss:  0.4870\n",
            "Epoch48 | Batch: 1 Loss:  0.5003\n",
            "Epoch48 | Batch: 2 Loss:  0.6012\n",
            "Epoch48 | Batch: 3 Loss:  0.4995\n",
            "Epoch48 | Batch: 4 Loss:  0.5663\n",
            "Epoch48 | Batch: 5 Loss:  0.4378\n",
            "Epoch48 | Batch: 6 Loss:  0.5617\n",
            "Epoch48 | Batch: 7 Loss:  0.5813\n",
            "Epoch48 | Batch: 8 Loss:  0.5465\n",
            "Epoch48 | Batch: 9 Loss:  0.5012\n",
            "Epoch48 | Batch: 10 Loss:  0.5187\n",
            "Epoch48 | Batch: 11 Loss:  0.5266\n",
            "Epoch48 | Batch: 12 Loss:  0.5646\n",
            "Epoch48 | Batch: 13 Loss:  0.5175\n",
            "Epoch48 | Batch: 14 Loss:  0.4766\n",
            "Epoch48 | Batch: 15 Loss:  0.5309\n",
            "Epoch48 | Batch: 16 Loss:  0.4421\n",
            "Epoch48 | Batch: 17 Loss:  0.5750\n",
            "Epoch48 | Batch: 18 Loss:  0.5675\n",
            "Epoch48 | Batch: 19 Loss:  0.5356\n",
            "Epoch48 | Batch: 20 Loss:  0.4682\n",
            "Epoch48 | Batch: 21 Loss:  0.5324\n",
            "Epoch48 | Batch: 22 Loss:  0.4967\n",
            "Epoch48 | Batch: 23 Loss:  0.5875\n",
            "Epoch48 | Batch: 24 Loss:  0.6052\n",
            "Epoch49 | Batch: 1 Loss:  0.4120\n",
            "Epoch49 | Batch: 2 Loss:  0.3728\n",
            "Epoch49 | Batch: 3 Loss:  0.4801\n",
            "Epoch49 | Batch: 4 Loss:  0.5256\n",
            "Epoch49 | Batch: 5 Loss:  0.6274\n",
            "Epoch49 | Batch: 6 Loss:  0.5338\n",
            "Epoch49 | Batch: 7 Loss:  0.6212\n",
            "Epoch49 | Batch: 8 Loss:  0.5773\n",
            "Epoch49 | Batch: 9 Loss:  0.5128\n",
            "Epoch49 | Batch: 10 Loss:  0.5493\n",
            "Epoch49 | Batch: 11 Loss:  0.4896\n",
            "Epoch49 | Batch: 12 Loss:  0.3996\n",
            "Epoch49 | Batch: 13 Loss:  0.7491\n",
            "Epoch49 | Batch: 14 Loss:  0.4555\n",
            "Epoch49 | Batch: 15 Loss:  0.4116\n",
            "Epoch49 | Batch: 16 Loss:  0.4916\n",
            "Epoch49 | Batch: 17 Loss:  0.6417\n",
            "Epoch49 | Batch: 18 Loss:  0.5798\n",
            "Epoch49 | Batch: 19 Loss:  0.4073\n",
            "Epoch49 | Batch: 20 Loss:  0.4672\n",
            "Epoch49 | Batch: 21 Loss:  0.5777\n",
            "Epoch49 | Batch: 22 Loss:  0.5355\n",
            "Epoch49 | Batch: 23 Loss:  0.4167\n",
            "Epoch49 | Batch: 24 Loss:  0.5316\n",
            "Epoch50 | Batch: 1 Loss:  0.4161\n",
            "Epoch50 | Batch: 2 Loss:  0.4508\n",
            "Epoch50 | Batch: 3 Loss:  0.5340\n",
            "Epoch50 | Batch: 4 Loss:  0.4856\n",
            "Epoch50 | Batch: 5 Loss:  0.4204\n",
            "Epoch50 | Batch: 6 Loss:  0.5052\n",
            "Epoch50 | Batch: 7 Loss:  0.4943\n",
            "Epoch50 | Batch: 8 Loss:  0.4478\n",
            "Epoch50 | Batch: 9 Loss:  0.4586\n",
            "Epoch50 | Batch: 10 Loss:  0.6305\n",
            "Epoch50 | Batch: 11 Loss:  0.4345\n",
            "Epoch50 | Batch: 12 Loss:  0.6363\n",
            "Epoch50 | Batch: 13 Loss:  0.5849\n",
            "Epoch50 | Batch: 14 Loss:  0.5024\n",
            "Epoch50 | Batch: 15 Loss:  0.4403\n",
            "Epoch50 | Batch: 16 Loss:  0.4595\n",
            "Epoch50 | Batch: 17 Loss:  0.4276\n",
            "Epoch50 | Batch: 18 Loss:  0.4097\n",
            "Epoch50 | Batch: 19 Loss:  0.5465\n",
            "Epoch50 | Batch: 20 Loss:  0.4456\n",
            "Epoch50 | Batch: 21 Loss:  0.6723\n",
            "Epoch50 | Batch: 22 Loss:  0.5125\n",
            "Epoch50 | Batch: 23 Loss:  0.4234\n",
            "Epoch50 | Batch: 24 Loss:  0.7165\n",
            "Epoch51 | Batch: 1 Loss:  0.5648\n",
            "Epoch51 | Batch: 2 Loss:  0.5916\n",
            "Epoch51 | Batch: 3 Loss:  0.5120\n",
            "Epoch51 | Batch: 4 Loss:  0.4330\n",
            "Epoch51 | Batch: 5 Loss:  0.5271\n",
            "Epoch51 | Batch: 6 Loss:  0.5801\n",
            "Epoch51 | Batch: 7 Loss:  0.5629\n",
            "Epoch51 | Batch: 8 Loss:  0.3986\n",
            "Epoch51 | Batch: 9 Loss:  0.4115\n",
            "Epoch51 | Batch: 10 Loss:  0.4990\n",
            "Epoch51 | Batch: 11 Loss:  0.4298\n",
            "Epoch51 | Batch: 12 Loss:  0.4017\n",
            "Epoch51 | Batch: 13 Loss:  0.3903\n",
            "Epoch51 | Batch: 14 Loss:  0.3888\n",
            "Epoch51 | Batch: 15 Loss:  0.4779\n",
            "Epoch51 | Batch: 16 Loss:  0.6281\n",
            "Epoch51 | Batch: 17 Loss:  0.5194\n",
            "Epoch51 | Batch: 18 Loss:  0.4465\n",
            "Epoch51 | Batch: 19 Loss:  0.4647\n",
            "Epoch51 | Batch: 20 Loss:  0.3756\n",
            "Epoch51 | Batch: 21 Loss:  0.6099\n",
            "Epoch51 | Batch: 22 Loss:  0.5378\n",
            "Epoch51 | Batch: 23 Loss:  0.5060\n",
            "Epoch51 | Batch: 24 Loss:  0.4950\n",
            "Epoch52 | Batch: 1 Loss:  0.5305\n",
            "Epoch52 | Batch: 2 Loss:  0.4539\n",
            "Epoch52 | Batch: 3 Loss:  0.3848\n",
            "Epoch52 | Batch: 4 Loss:  0.5379\n",
            "Epoch52 | Batch: 5 Loss:  0.5071\n",
            "Epoch52 | Batch: 6 Loss:  0.4990\n",
            "Epoch52 | Batch: 7 Loss:  0.6640\n",
            "Epoch52 | Batch: 8 Loss:  0.4000\n",
            "Epoch52 | Batch: 9 Loss:  0.4559\n",
            "Epoch52 | Batch: 10 Loss:  0.5644\n",
            "Epoch52 | Batch: 11 Loss:  0.4518\n",
            "Epoch52 | Batch: 12 Loss:  0.4508\n",
            "Epoch52 | Batch: 13 Loss:  0.4230\n",
            "Epoch52 | Batch: 14 Loss:  0.4444\n",
            "Epoch52 | Batch: 15 Loss:  0.4767\n",
            "Epoch52 | Batch: 16 Loss:  0.5501\n",
            "Epoch52 | Batch: 17 Loss:  0.4047\n",
            "Epoch52 | Batch: 18 Loss:  0.5711\n",
            "Epoch52 | Batch: 19 Loss:  0.6348\n",
            "Epoch52 | Batch: 20 Loss:  0.3961\n",
            "Epoch52 | Batch: 21 Loss:  0.4089\n",
            "Epoch52 | Batch: 22 Loss:  0.4517\n",
            "Epoch52 | Batch: 23 Loss:  0.5277\n",
            "Epoch52 | Batch: 24 Loss:  0.4026\n",
            "Epoch53 | Batch: 1 Loss:  0.4039\n",
            "Epoch53 | Batch: 2 Loss:  0.4245\n",
            "Epoch53 | Batch: 3 Loss:  0.5322\n",
            "Epoch53 | Batch: 4 Loss:  0.4935\n",
            "Epoch53 | Batch: 5 Loss:  0.5083\n",
            "Epoch53 | Batch: 6 Loss:  0.5220\n",
            "Epoch53 | Batch: 7 Loss:  0.4022\n",
            "Epoch53 | Batch: 8 Loss:  0.6035\n",
            "Epoch53 | Batch: 9 Loss:  0.5298\n",
            "Epoch53 | Batch: 10 Loss:  0.4930\n",
            "Epoch53 | Batch: 11 Loss:  0.4600\n",
            "Epoch53 | Batch: 12 Loss:  0.4556\n",
            "Epoch53 | Batch: 13 Loss:  0.5820\n",
            "Epoch53 | Batch: 14 Loss:  0.5178\n",
            "Epoch53 | Batch: 15 Loss:  0.6577\n",
            "Epoch53 | Batch: 16 Loss:  0.4925\n",
            "Epoch53 | Batch: 17 Loss:  0.4724\n",
            "Epoch53 | Batch: 18 Loss:  0.4666\n",
            "Epoch53 | Batch: 19 Loss:  0.4585\n",
            "Epoch53 | Batch: 20 Loss:  0.3054\n",
            "Epoch53 | Batch: 21 Loss:  0.5308\n",
            "Epoch53 | Batch: 22 Loss:  0.4345\n",
            "Epoch53 | Batch: 23 Loss:  0.2908\n",
            "Epoch53 | Batch: 24 Loss:  0.4617\n",
            "Epoch54 | Batch: 1 Loss:  0.4402\n",
            "Epoch54 | Batch: 2 Loss:  0.4180\n",
            "Epoch54 | Batch: 3 Loss:  0.5650\n",
            "Epoch54 | Batch: 4 Loss:  0.5427\n",
            "Epoch54 | Batch: 5 Loss:  0.6035\n",
            "Epoch54 | Batch: 6 Loss:  0.4864\n",
            "Epoch54 | Batch: 7 Loss:  0.5524\n",
            "Epoch54 | Batch: 8 Loss:  0.3322\n",
            "Epoch54 | Batch: 9 Loss:  0.4863\n",
            "Epoch54 | Batch: 10 Loss:  0.5832\n",
            "Epoch54 | Batch: 11 Loss:  0.4698\n",
            "Epoch54 | Batch: 12 Loss:  0.4148\n",
            "Epoch54 | Batch: 13 Loss:  0.4838\n",
            "Epoch54 | Batch: 14 Loss:  0.3679\n",
            "Epoch54 | Batch: 15 Loss:  0.5105\n",
            "Epoch54 | Batch: 16 Loss:  0.4104\n",
            "Epoch54 | Batch: 17 Loss:  0.3553\n",
            "Epoch54 | Batch: 18 Loss:  0.3451\n",
            "Epoch54 | Batch: 19 Loss:  0.5241\n",
            "Epoch54 | Batch: 20 Loss:  0.3668\n",
            "Epoch54 | Batch: 21 Loss:  0.4113\n",
            "Epoch54 | Batch: 22 Loss:  0.6214\n",
            "Epoch54 | Batch: 23 Loss:  0.5451\n",
            "Epoch54 | Batch: 24 Loss:  0.6854\n",
            "Epoch55 | Batch: 1 Loss:  0.4459\n",
            "Epoch55 | Batch: 2 Loss:  0.4334\n",
            "Epoch55 | Batch: 3 Loss:  0.3934\n",
            "Epoch55 | Batch: 4 Loss:  0.4074\n",
            "Epoch55 | Batch: 5 Loss:  0.4259\n",
            "Epoch55 | Batch: 6 Loss:  0.4547\n",
            "Epoch55 | Batch: 7 Loss:  0.5336\n",
            "Epoch55 | Batch: 8 Loss:  0.5022\n",
            "Epoch55 | Batch: 9 Loss:  0.5191\n",
            "Epoch55 | Batch: 10 Loss:  0.4248\n",
            "Epoch55 | Batch: 11 Loss:  0.4570\n",
            "Epoch55 | Batch: 12 Loss:  0.4982\n",
            "Epoch55 | Batch: 13 Loss:  0.5553\n",
            "Epoch55 | Batch: 14 Loss:  0.4248\n",
            "Epoch55 | Batch: 15 Loss:  0.3735\n",
            "Epoch55 | Batch: 16 Loss:  0.3936\n",
            "Epoch55 | Batch: 17 Loss:  0.5702\n",
            "Epoch55 | Batch: 18 Loss:  0.4333\n",
            "Epoch55 | Batch: 19 Loss:  0.5614\n",
            "Epoch55 | Batch: 20 Loss:  0.6610\n",
            "Epoch55 | Batch: 21 Loss:  0.4643\n",
            "Epoch55 | Batch: 22 Loss:  0.3827\n",
            "Epoch55 | Batch: 23 Loss:  0.5247\n",
            "Epoch55 | Batch: 24 Loss:  0.5881\n",
            "Epoch56 | Batch: 1 Loss:  0.3588\n",
            "Epoch56 | Batch: 2 Loss:  0.3787\n",
            "Epoch56 | Batch: 3 Loss:  0.5046\n",
            "Epoch56 | Batch: 4 Loss:  0.4100\n",
            "Epoch56 | Batch: 5 Loss:  0.4694\n",
            "Epoch56 | Batch: 6 Loss:  0.4469\n",
            "Epoch56 | Batch: 7 Loss:  0.3427\n",
            "Epoch56 | Batch: 8 Loss:  0.3532\n",
            "Epoch56 | Batch: 9 Loss:  0.6244\n",
            "Epoch56 | Batch: 10 Loss:  0.4493\n",
            "Epoch56 | Batch: 11 Loss:  0.6932\n",
            "Epoch56 | Batch: 12 Loss:  0.4926\n",
            "Epoch56 | Batch: 13 Loss:  0.6069\n",
            "Epoch56 | Batch: 14 Loss:  0.3953\n",
            "Epoch56 | Batch: 15 Loss:  0.4468\n",
            "Epoch56 | Batch: 16 Loss:  0.7303\n",
            "Epoch56 | Batch: 17 Loss:  0.3683\n",
            "Epoch56 | Batch: 18 Loss:  0.5698\n",
            "Epoch56 | Batch: 19 Loss:  0.5733\n",
            "Epoch56 | Batch: 20 Loss:  0.4543\n",
            "Epoch56 | Batch: 21 Loss:  0.4076\n",
            "Epoch56 | Batch: 22 Loss:  0.3452\n",
            "Epoch56 | Batch: 23 Loss:  0.5016\n",
            "Epoch56 | Batch: 24 Loss:  0.3061\n",
            "Epoch57 | Batch: 1 Loss:  0.5969\n",
            "Epoch57 | Batch: 2 Loss:  0.4531\n",
            "Epoch57 | Batch: 3 Loss:  0.4421\n",
            "Epoch57 | Batch: 4 Loss:  0.5336\n",
            "Epoch57 | Batch: 5 Loss:  0.4159\n",
            "Epoch57 | Batch: 6 Loss:  0.4774\n",
            "Epoch57 | Batch: 7 Loss:  0.6182\n",
            "Epoch57 | Batch: 8 Loss:  0.4154\n",
            "Epoch57 | Batch: 9 Loss:  0.4793\n",
            "Epoch57 | Batch: 10 Loss:  0.3696\n",
            "Epoch57 | Batch: 11 Loss:  0.5023\n",
            "Epoch57 | Batch: 12 Loss:  0.4927\n",
            "Epoch57 | Batch: 13 Loss:  0.5479\n",
            "Epoch57 | Batch: 14 Loss:  0.4000\n",
            "Epoch57 | Batch: 15 Loss:  0.4147\n",
            "Epoch57 | Batch: 16 Loss:  0.5557\n",
            "Epoch57 | Batch: 17 Loss:  0.4227\n",
            "Epoch57 | Batch: 18 Loss:  0.4176\n",
            "Epoch57 | Batch: 19 Loss:  0.5758\n",
            "Epoch57 | Batch: 20 Loss:  0.3828\n",
            "Epoch57 | Batch: 21 Loss:  0.4715\n",
            "Epoch57 | Batch: 22 Loss:  0.4486\n",
            "Epoch57 | Batch: 23 Loss:  0.5214\n",
            "Epoch57 | Batch: 24 Loss:  0.3985\n",
            "Epoch58 | Batch: 1 Loss:  0.6291\n",
            "Epoch58 | Batch: 2 Loss:  0.6011\n",
            "Epoch58 | Batch: 3 Loss:  0.5050\n",
            "Epoch58 | Batch: 4 Loss:  0.4328\n",
            "Epoch58 | Batch: 5 Loss:  0.4670\n",
            "Epoch58 | Batch: 6 Loss:  0.3427\n",
            "Epoch58 | Batch: 7 Loss:  0.3423\n",
            "Epoch58 | Batch: 8 Loss:  0.3166\n",
            "Epoch58 | Batch: 9 Loss:  0.3139\n",
            "Epoch58 | Batch: 10 Loss:  0.4702\n",
            "Epoch58 | Batch: 11 Loss:  0.6349\n",
            "Epoch58 | Batch: 12 Loss:  0.5322\n",
            "Epoch58 | Batch: 13 Loss:  0.4875\n",
            "Epoch58 | Batch: 14 Loss:  0.5569\n",
            "Epoch58 | Batch: 15 Loss:  0.3913\n",
            "Epoch58 | Batch: 16 Loss:  0.4124\n",
            "Epoch58 | Batch: 17 Loss:  0.5614\n",
            "Epoch58 | Batch: 18 Loss:  0.4392\n",
            "Epoch58 | Batch: 19 Loss:  0.3912\n",
            "Epoch58 | Batch: 20 Loss:  0.4847\n",
            "Epoch58 | Batch: 21 Loss:  0.3971\n",
            "Epoch58 | Batch: 22 Loss:  0.3566\n",
            "Epoch58 | Batch: 23 Loss:  0.6447\n",
            "Epoch58 | Batch: 24 Loss:  0.6087\n",
            "Epoch59 | Batch: 1 Loss:  0.4543\n",
            "Epoch59 | Batch: 2 Loss:  0.6341\n",
            "Epoch59 | Batch: 3 Loss:  0.5073\n",
            "Epoch59 | Batch: 4 Loss:  0.4299\n",
            "Epoch59 | Batch: 5 Loss:  0.3771\n",
            "Epoch59 | Batch: 6 Loss:  0.3591\n",
            "Epoch59 | Batch: 7 Loss:  0.2734\n",
            "Epoch59 | Batch: 8 Loss:  0.4696\n",
            "Epoch59 | Batch: 9 Loss:  0.4415\n",
            "Epoch59 | Batch: 10 Loss:  0.6027\n",
            "Epoch59 | Batch: 11 Loss:  0.5025\n",
            "Epoch59 | Batch: 12 Loss:  0.4884\n",
            "Epoch59 | Batch: 13 Loss:  0.4237\n",
            "Epoch59 | Batch: 14 Loss:  0.4609\n",
            "Epoch59 | Batch: 15 Loss:  0.4185\n",
            "Epoch59 | Batch: 16 Loss:  0.3876\n",
            "Epoch59 | Batch: 17 Loss:  0.4515\n",
            "Epoch59 | Batch: 18 Loss:  0.6062\n",
            "Epoch59 | Batch: 19 Loss:  0.5137\n",
            "Epoch59 | Batch: 20 Loss:  0.6213\n",
            "Epoch59 | Batch: 21 Loss:  0.4819\n",
            "Epoch59 | Batch: 22 Loss:  0.5591\n",
            "Epoch59 | Batch: 23 Loss:  0.3183\n",
            "Epoch59 | Batch: 24 Loss:  0.5568\n",
            "Epoch60 | Batch: 1 Loss:  0.4135\n",
            "Epoch60 | Batch: 2 Loss:  0.5091\n",
            "Epoch60 | Batch: 3 Loss:  0.3581\n",
            "Epoch60 | Batch: 4 Loss:  0.5828\n",
            "Epoch60 | Batch: 5 Loss:  0.4630\n",
            "Epoch60 | Batch: 6 Loss:  0.4844\n",
            "Epoch60 | Batch: 7 Loss:  0.4249\n",
            "Epoch60 | Batch: 8 Loss:  0.5180\n",
            "Epoch60 | Batch: 9 Loss:  0.4795\n",
            "Epoch60 | Batch: 10 Loss:  0.4465\n",
            "Epoch60 | Batch: 11 Loss:  0.5342\n",
            "Epoch60 | Batch: 12 Loss:  0.4648\n",
            "Epoch60 | Batch: 13 Loss:  0.5029\n",
            "Epoch60 | Batch: 14 Loss:  0.4472\n",
            "Epoch60 | Batch: 15 Loss:  0.5526\n",
            "Epoch60 | Batch: 16 Loss:  0.4734\n",
            "Epoch60 | Batch: 17 Loss:  0.3222\n",
            "Epoch60 | Batch: 18 Loss:  0.3616\n",
            "Epoch60 | Batch: 19 Loss:  0.5465\n",
            "Epoch60 | Batch: 20 Loss:  0.4310\n",
            "Epoch60 | Batch: 21 Loss:  0.5300\n",
            "Epoch60 | Batch: 22 Loss:  0.5248\n",
            "Epoch60 | Batch: 23 Loss:  0.5813\n",
            "Epoch60 | Batch: 24 Loss:  0.2614\n",
            "Epoch61 | Batch: 1 Loss:  0.5194\n",
            "Epoch61 | Batch: 2 Loss:  0.3151\n",
            "Epoch61 | Batch: 3 Loss:  0.7110\n",
            "Epoch61 | Batch: 4 Loss:  0.5642\n",
            "Epoch61 | Batch: 5 Loss:  0.5766\n",
            "Epoch61 | Batch: 6 Loss:  0.4298\n",
            "Epoch61 | Batch: 7 Loss:  0.5764\n",
            "Epoch61 | Batch: 8 Loss:  0.6131\n",
            "Epoch61 | Batch: 9 Loss:  0.4345\n",
            "Epoch61 | Batch: 10 Loss:  0.3982\n",
            "Epoch61 | Batch: 11 Loss:  0.4339\n",
            "Epoch61 | Batch: 12 Loss:  0.3293\n",
            "Epoch61 | Batch: 13 Loss:  0.3663\n",
            "Epoch61 | Batch: 14 Loss:  0.6976\n",
            "Epoch61 | Batch: 15 Loss:  0.4048\n",
            "Epoch61 | Batch: 16 Loss:  0.4218\n",
            "Epoch61 | Batch: 17 Loss:  0.3524\n",
            "Epoch61 | Batch: 18 Loss:  0.4612\n",
            "Epoch61 | Batch: 19 Loss:  0.3275\n",
            "Epoch61 | Batch: 20 Loss:  0.5919\n",
            "Epoch61 | Batch: 21 Loss:  0.5596\n",
            "Epoch61 | Batch: 22 Loss:  0.3629\n",
            "Epoch61 | Batch: 23 Loss:  0.4235\n",
            "Epoch61 | Batch: 24 Loss:  0.5588\n",
            "Epoch62 | Batch: 1 Loss:  0.4730\n",
            "Epoch62 | Batch: 2 Loss:  0.5117\n",
            "Epoch62 | Batch: 3 Loss:  0.4447\n",
            "Epoch62 | Batch: 4 Loss:  0.5218\n",
            "Epoch62 | Batch: 5 Loss:  0.4634\n",
            "Epoch62 | Batch: 6 Loss:  0.5635\n",
            "Epoch62 | Batch: 7 Loss:  0.5367\n",
            "Epoch62 | Batch: 8 Loss:  0.2598\n",
            "Epoch62 | Batch: 9 Loss:  0.5657\n",
            "Epoch62 | Batch: 10 Loss:  0.3324\n",
            "Epoch62 | Batch: 11 Loss:  0.4464\n",
            "Epoch62 | Batch: 12 Loss:  0.5903\n",
            "Epoch62 | Batch: 13 Loss:  0.5177\n",
            "Epoch62 | Batch: 14 Loss:  0.4795\n",
            "Epoch62 | Batch: 15 Loss:  0.4925\n",
            "Epoch62 | Batch: 16 Loss:  0.3576\n",
            "Epoch62 | Batch: 17 Loss:  0.3626\n",
            "Epoch62 | Batch: 18 Loss:  0.3998\n",
            "Epoch62 | Batch: 19 Loss:  0.4352\n",
            "Epoch62 | Batch: 20 Loss:  0.5225\n",
            "Epoch62 | Batch: 21 Loss:  0.5003\n",
            "Epoch62 | Batch: 22 Loss:  0.4584\n",
            "Epoch62 | Batch: 23 Loss:  0.4517\n",
            "Epoch62 | Batch: 24 Loss:  0.5782\n",
            "Epoch63 | Batch: 1 Loss:  0.5188\n",
            "Epoch63 | Batch: 2 Loss:  0.3996\n",
            "Epoch63 | Batch: 3 Loss:  0.4086\n",
            "Epoch63 | Batch: 4 Loss:  0.6146\n",
            "Epoch63 | Batch: 5 Loss:  0.5552\n",
            "Epoch63 | Batch: 6 Loss:  0.4069\n",
            "Epoch63 | Batch: 7 Loss:  0.4405\n",
            "Epoch63 | Batch: 8 Loss:  0.5621\n",
            "Epoch63 | Batch: 9 Loss:  0.3907\n",
            "Epoch63 | Batch: 10 Loss:  0.6023\n",
            "Epoch63 | Batch: 11 Loss:  0.4274\n",
            "Epoch63 | Batch: 12 Loss:  0.4768\n",
            "Epoch63 | Batch: 13 Loss:  0.5606\n",
            "Epoch63 | Batch: 14 Loss:  0.3534\n",
            "Epoch63 | Batch: 15 Loss:  0.3196\n",
            "Epoch63 | Batch: 16 Loss:  0.5558\n",
            "Epoch63 | Batch: 17 Loss:  0.3428\n",
            "Epoch63 | Batch: 18 Loss:  0.4071\n",
            "Epoch63 | Batch: 19 Loss:  0.2919\n",
            "Epoch63 | Batch: 20 Loss:  0.2865\n",
            "Epoch63 | Batch: 21 Loss:  0.6801\n",
            "Epoch63 | Batch: 22 Loss:  0.5699\n",
            "Epoch63 | Batch: 23 Loss:  0.5696\n",
            "Epoch63 | Batch: 24 Loss:  0.4927\n",
            "Epoch64 | Batch: 1 Loss:  0.4705\n",
            "Epoch64 | Batch: 2 Loss:  0.5460\n",
            "Epoch64 | Batch: 3 Loss:  0.3070\n",
            "Epoch64 | Batch: 4 Loss:  0.5975\n",
            "Epoch64 | Batch: 5 Loss:  0.5674\n",
            "Epoch64 | Batch: 6 Loss:  0.4448\n",
            "Epoch64 | Batch: 7 Loss:  0.4615\n",
            "Epoch64 | Batch: 8 Loss:  0.5023\n",
            "Epoch64 | Batch: 9 Loss:  0.4462\n",
            "Epoch64 | Batch: 10 Loss:  0.3967\n",
            "Epoch64 | Batch: 11 Loss:  0.4537\n",
            "Epoch64 | Batch: 12 Loss:  0.3196\n",
            "Epoch64 | Batch: 13 Loss:  0.4742\n",
            "Epoch64 | Batch: 14 Loss:  0.4916\n",
            "Epoch64 | Batch: 15 Loss:  0.7142\n",
            "Epoch64 | Batch: 16 Loss:  0.3677\n",
            "Epoch64 | Batch: 17 Loss:  0.4926\n",
            "Epoch64 | Batch: 18 Loss:  0.4405\n",
            "Epoch64 | Batch: 19 Loss:  0.6221\n",
            "Epoch64 | Batch: 20 Loss:  0.3264\n",
            "Epoch64 | Batch: 21 Loss:  0.4628\n",
            "Epoch64 | Batch: 22 Loss:  0.4386\n",
            "Epoch64 | Batch: 23 Loss:  0.4131\n",
            "Epoch64 | Batch: 24 Loss:  0.5052\n",
            "Epoch65 | Batch: 1 Loss:  0.5074\n",
            "Epoch65 | Batch: 2 Loss:  0.4081\n",
            "Epoch65 | Batch: 3 Loss:  0.4739\n",
            "Epoch65 | Batch: 4 Loss:  0.4460\n",
            "Epoch65 | Batch: 5 Loss:  0.5277\n",
            "Epoch65 | Batch: 6 Loss:  0.4829\n",
            "Epoch65 | Batch: 7 Loss:  0.5602\n",
            "Epoch65 | Batch: 8 Loss:  0.3856\n",
            "Epoch65 | Batch: 9 Loss:  0.6233\n",
            "Epoch65 | Batch: 10 Loss:  0.4295\n",
            "Epoch65 | Batch: 11 Loss:  0.3249\n",
            "Epoch65 | Batch: 12 Loss:  0.5605\n",
            "Epoch65 | Batch: 13 Loss:  0.4261\n",
            "Epoch65 | Batch: 14 Loss:  0.5678\n",
            "Epoch65 | Batch: 15 Loss:  0.4373\n",
            "Epoch65 | Batch: 16 Loss:  0.4315\n",
            "Epoch65 | Batch: 17 Loss:  0.5533\n",
            "Epoch65 | Batch: 18 Loss:  0.3878\n",
            "Epoch65 | Batch: 19 Loss:  0.4286\n",
            "Epoch65 | Batch: 20 Loss:  0.4320\n",
            "Epoch65 | Batch: 21 Loss:  0.4200\n",
            "Epoch65 | Batch: 22 Loss:  0.4765\n",
            "Epoch65 | Batch: 23 Loss:  0.5276\n",
            "Epoch65 | Batch: 24 Loss:  0.4507\n",
            "Epoch66 | Batch: 1 Loss:  0.3936\n",
            "Epoch66 | Batch: 2 Loss:  0.4342\n",
            "Epoch66 | Batch: 3 Loss:  0.5218\n",
            "Epoch66 | Batch: 4 Loss:  0.4394\n",
            "Epoch66 | Batch: 5 Loss:  0.3520\n",
            "Epoch66 | Batch: 6 Loss:  0.4592\n",
            "Epoch66 | Batch: 7 Loss:  0.4695\n",
            "Epoch66 | Batch: 8 Loss:  0.4708\n",
            "Epoch66 | Batch: 9 Loss:  0.3828\n",
            "Epoch66 | Batch: 10 Loss:  0.3567\n",
            "Epoch66 | Batch: 11 Loss:  0.4265\n",
            "Epoch66 | Batch: 12 Loss:  0.4970\n",
            "Epoch66 | Batch: 13 Loss:  0.6072\n",
            "Epoch66 | Batch: 14 Loss:  0.4811\n",
            "Epoch66 | Batch: 15 Loss:  0.5846\n",
            "Epoch66 | Batch: 16 Loss:  0.4842\n",
            "Epoch66 | Batch: 17 Loss:  0.4792\n",
            "Epoch66 | Batch: 18 Loss:  0.5200\n",
            "Epoch66 | Batch: 19 Loss:  0.4568\n",
            "Epoch66 | Batch: 20 Loss:  0.5399\n",
            "Epoch66 | Batch: 21 Loss:  0.4349\n",
            "Epoch66 | Batch: 22 Loss:  0.5212\n",
            "Epoch66 | Batch: 23 Loss:  0.5064\n",
            "Epoch66 | Batch: 24 Loss:  0.4761\n",
            "Epoch67 | Batch: 1 Loss:  0.5608\n",
            "Epoch67 | Batch: 2 Loss:  0.5723\n",
            "Epoch67 | Batch: 3 Loss:  0.4883\n",
            "Epoch67 | Batch: 4 Loss:  0.3078\n",
            "Epoch67 | Batch: 5 Loss:  0.6574\n",
            "Epoch67 | Batch: 6 Loss:  0.3615\n",
            "Epoch67 | Batch: 7 Loss:  0.4879\n",
            "Epoch67 | Batch: 8 Loss:  0.3042\n",
            "Epoch67 | Batch: 9 Loss:  0.5707\n",
            "Epoch67 | Batch: 10 Loss:  0.6060\n",
            "Epoch67 | Batch: 11 Loss:  0.4457\n",
            "Epoch67 | Batch: 12 Loss:  0.4013\n",
            "Epoch67 | Batch: 13 Loss:  0.3965\n",
            "Epoch67 | Batch: 14 Loss:  0.5098\n",
            "Epoch67 | Batch: 15 Loss:  0.5813\n",
            "Epoch67 | Batch: 16 Loss:  0.5312\n",
            "Epoch67 | Batch: 17 Loss:  0.5278\n",
            "Epoch67 | Batch: 18 Loss:  0.3942\n",
            "Epoch67 | Batch: 19 Loss:  0.4758\n",
            "Epoch67 | Batch: 20 Loss:  0.4083\n",
            "Epoch67 | Batch: 21 Loss:  0.4135\n",
            "Epoch67 | Batch: 22 Loss:  0.2861\n",
            "Epoch67 | Batch: 23 Loss:  0.5318\n",
            "Epoch67 | Batch: 24 Loss:  0.3791\n",
            "Epoch68 | Batch: 1 Loss:  0.5570\n",
            "Epoch68 | Batch: 2 Loss:  0.4170\n",
            "Epoch68 | Batch: 3 Loss:  0.3959\n",
            "Epoch68 | Batch: 4 Loss:  0.5928\n",
            "Epoch68 | Batch: 5 Loss:  0.4339\n",
            "Epoch68 | Batch: 6 Loss:  0.3842\n",
            "Epoch68 | Batch: 7 Loss:  0.4131\n",
            "Epoch68 | Batch: 8 Loss:  0.3956\n",
            "Epoch68 | Batch: 9 Loss:  0.5374\n",
            "Epoch68 | Batch: 10 Loss:  0.4452\n",
            "Epoch68 | Batch: 11 Loss:  0.4548\n",
            "Epoch68 | Batch: 12 Loss:  0.4664\n",
            "Epoch68 | Batch: 13 Loss:  0.3970\n",
            "Epoch68 | Batch: 14 Loss:  0.3822\n",
            "Epoch68 | Batch: 15 Loss:  0.6290\n",
            "Epoch68 | Batch: 16 Loss:  0.3972\n",
            "Epoch68 | Batch: 17 Loss:  0.5442\n",
            "Epoch68 | Batch: 18 Loss:  0.5045\n",
            "Epoch68 | Batch: 19 Loss:  0.4425\n",
            "Epoch68 | Batch: 20 Loss:  0.4690\n",
            "Epoch68 | Batch: 21 Loss:  0.4702\n",
            "Epoch68 | Batch: 22 Loss:  0.4529\n",
            "Epoch68 | Batch: 23 Loss:  0.6372\n",
            "Epoch68 | Batch: 24 Loss:  0.3758\n",
            "Epoch69 | Batch: 1 Loss:  0.4592\n",
            "Epoch69 | Batch: 2 Loss:  0.5137\n",
            "Epoch69 | Batch: 3 Loss:  0.4299\n",
            "Epoch69 | Batch: 4 Loss:  0.4534\n",
            "Epoch69 | Batch: 5 Loss:  0.4655\n",
            "Epoch69 | Batch: 6 Loss:  0.3672\n",
            "Epoch69 | Batch: 7 Loss:  0.3125\n",
            "Epoch69 | Batch: 8 Loss:  0.5289\n",
            "Epoch69 | Batch: 9 Loss:  0.3582\n",
            "Epoch69 | Batch: 10 Loss:  0.5141\n",
            "Epoch69 | Batch: 11 Loss:  0.4749\n",
            "Epoch69 | Batch: 12 Loss:  0.4674\n",
            "Epoch69 | Batch: 13 Loss:  0.5291\n",
            "Epoch69 | Batch: 14 Loss:  0.5482\n",
            "Epoch69 | Batch: 15 Loss:  0.5365\n",
            "Epoch69 | Batch: 16 Loss:  0.3202\n",
            "Epoch69 | Batch: 17 Loss:  0.7044\n",
            "Epoch69 | Batch: 18 Loss:  0.5856\n",
            "Epoch69 | Batch: 19 Loss:  0.3296\n",
            "Epoch69 | Batch: 20 Loss:  0.5291\n",
            "Epoch69 | Batch: 21 Loss:  0.4310\n",
            "Epoch69 | Batch: 22 Loss:  0.4180\n",
            "Epoch69 | Batch: 23 Loss:  0.4536\n",
            "Epoch69 | Batch: 24 Loss:  0.4087\n",
            "Epoch70 | Batch: 1 Loss:  0.5377\n",
            "Epoch70 | Batch: 2 Loss:  0.6342\n",
            "Epoch70 | Batch: 3 Loss:  0.5167\n",
            "Epoch70 | Batch: 4 Loss:  0.4482\n",
            "Epoch70 | Batch: 5 Loss:  0.4935\n",
            "Epoch70 | Batch: 6 Loss:  0.5400\n",
            "Epoch70 | Batch: 7 Loss:  0.4502\n",
            "Epoch70 | Batch: 8 Loss:  0.3659\n",
            "Epoch70 | Batch: 9 Loss:  0.3737\n",
            "Epoch70 | Batch: 10 Loss:  0.4664\n",
            "Epoch70 | Batch: 11 Loss:  0.4315\n",
            "Epoch70 | Batch: 12 Loss:  0.4996\n",
            "Epoch70 | Batch: 13 Loss:  0.5018\n",
            "Epoch70 | Batch: 14 Loss:  0.3977\n",
            "Epoch70 | Batch: 15 Loss:  0.4285\n",
            "Epoch70 | Batch: 16 Loss:  0.4561\n",
            "Epoch70 | Batch: 17 Loss:  0.4755\n",
            "Epoch70 | Batch: 18 Loss:  0.4300\n",
            "Epoch70 | Batch: 19 Loss:  0.5132\n",
            "Epoch70 | Batch: 20 Loss:  0.6226\n",
            "Epoch70 | Batch: 21 Loss:  0.5088\n",
            "Epoch70 | Batch: 22 Loss:  0.4673\n",
            "Epoch70 | Batch: 23 Loss:  0.3374\n",
            "Epoch70 | Batch: 24 Loss:  0.1838\n",
            "Epoch71 | Batch: 1 Loss:  0.4245\n",
            "Epoch71 | Batch: 2 Loss:  0.4100\n",
            "Epoch71 | Batch: 3 Loss:  0.3672\n",
            "Epoch71 | Batch: 4 Loss:  0.5498\n",
            "Epoch71 | Batch: 5 Loss:  0.4816\n",
            "Epoch71 | Batch: 6 Loss:  0.4641\n",
            "Epoch71 | Batch: 7 Loss:  0.5578\n",
            "Epoch71 | Batch: 8 Loss:  0.5889\n",
            "Epoch71 | Batch: 9 Loss:  0.3064\n",
            "Epoch71 | Batch: 10 Loss:  0.3511\n",
            "Epoch71 | Batch: 11 Loss:  0.5000\n",
            "Epoch71 | Batch: 12 Loss:  0.6157\n",
            "Epoch71 | Batch: 13 Loss:  0.4065\n",
            "Epoch71 | Batch: 14 Loss:  0.4086\n",
            "Epoch71 | Batch: 15 Loss:  0.4616\n",
            "Epoch71 | Batch: 16 Loss:  0.4963\n",
            "Epoch71 | Batch: 17 Loss:  0.5209\n",
            "Epoch71 | Batch: 18 Loss:  0.4465\n",
            "Epoch71 | Batch: 19 Loss:  0.4177\n",
            "Epoch71 | Batch: 20 Loss:  0.4763\n",
            "Epoch71 | Batch: 21 Loss:  0.4576\n",
            "Epoch71 | Batch: 22 Loss:  0.6329\n",
            "Epoch71 | Batch: 23 Loss:  0.5256\n",
            "Epoch71 | Batch: 24 Loss:  0.5506\n",
            "Epoch72 | Batch: 1 Loss:  0.4721\n",
            "Epoch72 | Batch: 2 Loss:  0.4859\n",
            "Epoch72 | Batch: 3 Loss:  0.3626\n",
            "Epoch72 | Batch: 4 Loss:  0.4687\n",
            "Epoch72 | Batch: 5 Loss:  0.3749\n",
            "Epoch72 | Batch: 6 Loss:  0.6044\n",
            "Epoch72 | Batch: 7 Loss:  0.5978\n",
            "Epoch72 | Batch: 8 Loss:  0.5231\n",
            "Epoch72 | Batch: 9 Loss:  0.4821\n",
            "Epoch72 | Batch: 10 Loss:  0.5261\n",
            "Epoch72 | Batch: 11 Loss:  0.6223\n",
            "Epoch72 | Batch: 12 Loss:  0.4254\n",
            "Epoch72 | Batch: 13 Loss:  0.3684\n",
            "Epoch72 | Batch: 14 Loss:  0.5181\n",
            "Epoch72 | Batch: 15 Loss:  0.4981\n",
            "Epoch72 | Batch: 16 Loss:  0.4837\n",
            "Epoch72 | Batch: 17 Loss:  0.5999\n",
            "Epoch72 | Batch: 18 Loss:  0.3554\n",
            "Epoch72 | Batch: 19 Loss:  0.3060\n",
            "Epoch72 | Batch: 20 Loss:  0.5248\n",
            "Epoch72 | Batch: 21 Loss:  0.3150\n",
            "Epoch72 | Batch: 22 Loss:  0.3904\n",
            "Epoch72 | Batch: 23 Loss:  0.4634\n",
            "Epoch72 | Batch: 24 Loss:  0.4649\n",
            "Epoch73 | Batch: 1 Loss:  0.4432\n",
            "Epoch73 | Batch: 2 Loss:  0.5800\n",
            "Epoch73 | Batch: 3 Loss:  0.2658\n",
            "Epoch73 | Batch: 4 Loss:  0.3298\n",
            "Epoch73 | Batch: 5 Loss:  0.5610\n",
            "Epoch73 | Batch: 6 Loss:  0.6470\n",
            "Epoch73 | Batch: 7 Loss:  0.5697\n",
            "Epoch73 | Batch: 8 Loss:  0.4484\n",
            "Epoch73 | Batch: 9 Loss:  0.5325\n",
            "Epoch73 | Batch: 10 Loss:  0.4674\n",
            "Epoch73 | Batch: 11 Loss:  0.3806\n",
            "Epoch73 | Batch: 12 Loss:  0.5712\n",
            "Epoch73 | Batch: 13 Loss:  0.4583\n",
            "Epoch73 | Batch: 14 Loss:  0.4413\n",
            "Epoch73 | Batch: 15 Loss:  0.5765\n",
            "Epoch73 | Batch: 16 Loss:  0.4291\n",
            "Epoch73 | Batch: 17 Loss:  0.3537\n",
            "Epoch73 | Batch: 18 Loss:  0.4095\n",
            "Epoch73 | Batch: 19 Loss:  0.3999\n",
            "Epoch73 | Batch: 20 Loss:  0.7291\n",
            "Epoch73 | Batch: 21 Loss:  0.3912\n",
            "Epoch73 | Batch: 22 Loss:  0.4316\n",
            "Epoch73 | Batch: 23 Loss:  0.4994\n",
            "Epoch73 | Batch: 24 Loss:  0.4908\n",
            "Epoch74 | Batch: 1 Loss:  0.4977\n",
            "Epoch74 | Batch: 2 Loss:  0.6019\n",
            "Epoch74 | Batch: 3 Loss:  0.4921\n",
            "Epoch74 | Batch: 4 Loss:  0.3750\n",
            "Epoch74 | Batch: 5 Loss:  0.4583\n",
            "Epoch74 | Batch: 6 Loss:  0.4935\n",
            "Epoch74 | Batch: 7 Loss:  0.3409\n",
            "Epoch74 | Batch: 8 Loss:  0.5175\n",
            "Epoch74 | Batch: 9 Loss:  0.5455\n",
            "Epoch74 | Batch: 10 Loss:  0.3165\n",
            "Epoch74 | Batch: 11 Loss:  0.4850\n",
            "Epoch74 | Batch: 12 Loss:  0.5149\n",
            "Epoch74 | Batch: 13 Loss:  0.3384\n",
            "Epoch74 | Batch: 14 Loss:  0.4361\n",
            "Epoch74 | Batch: 15 Loss:  0.4680\n",
            "Epoch74 | Batch: 16 Loss:  0.3744\n",
            "Epoch74 | Batch: 17 Loss:  0.5141\n",
            "Epoch74 | Batch: 18 Loss:  0.4093\n",
            "Epoch74 | Batch: 19 Loss:  0.5005\n",
            "Epoch74 | Batch: 20 Loss:  0.3190\n",
            "Epoch74 | Batch: 21 Loss:  0.6116\n",
            "Epoch74 | Batch: 22 Loss:  0.4257\n",
            "Epoch74 | Batch: 23 Loss:  0.4954\n",
            "Epoch74 | Batch: 24 Loss:  0.6583\n",
            "Epoch75 | Batch: 1 Loss:  0.6180\n",
            "Epoch75 | Batch: 2 Loss:  0.4703\n",
            "Epoch75 | Batch: 3 Loss:  0.4289\n",
            "Epoch75 | Batch: 4 Loss:  0.5514\n",
            "Epoch75 | Batch: 5 Loss:  0.4206\n",
            "Epoch75 | Batch: 6 Loss:  0.3875\n",
            "Epoch75 | Batch: 7 Loss:  0.3534\n",
            "Epoch75 | Batch: 8 Loss:  0.4005\n",
            "Epoch75 | Batch: 9 Loss:  0.3740\n",
            "Epoch75 | Batch: 10 Loss:  0.5348\n",
            "Epoch75 | Batch: 11 Loss:  0.3650\n",
            "Epoch75 | Batch: 12 Loss:  0.4127\n",
            "Epoch75 | Batch: 13 Loss:  0.4896\n",
            "Epoch75 | Batch: 14 Loss:  0.4016\n",
            "Epoch75 | Batch: 15 Loss:  0.4028\n",
            "Epoch75 | Batch: 16 Loss:  0.3321\n",
            "Epoch75 | Batch: 17 Loss:  0.5374\n",
            "Epoch75 | Batch: 18 Loss:  0.5184\n",
            "Epoch75 | Batch: 19 Loss:  0.5205\n",
            "Epoch75 | Batch: 20 Loss:  0.5369\n",
            "Epoch75 | Batch: 21 Loss:  0.6053\n",
            "Epoch75 | Batch: 22 Loss:  0.4403\n",
            "Epoch75 | Batch: 23 Loss:  0.5505\n",
            "Epoch75 | Batch: 24 Loss:  0.6227\n",
            "Epoch76 | Batch: 1 Loss:  0.5260\n",
            "Epoch76 | Batch: 2 Loss:  0.3924\n",
            "Epoch76 | Batch: 3 Loss:  0.5882\n",
            "Epoch76 | Batch: 4 Loss:  0.4256\n",
            "Epoch76 | Batch: 5 Loss:  0.5860\n",
            "Epoch76 | Batch: 6 Loss:  0.5393\n",
            "Epoch76 | Batch: 7 Loss:  0.4638\n",
            "Epoch76 | Batch: 8 Loss:  0.5434\n",
            "Epoch76 | Batch: 9 Loss:  0.4656\n",
            "Epoch76 | Batch: 10 Loss:  0.4955\n",
            "Epoch76 | Batch: 11 Loss:  0.4319\n",
            "Epoch76 | Batch: 12 Loss:  0.4140\n",
            "Epoch76 | Batch: 13 Loss:  0.3163\n",
            "Epoch76 | Batch: 14 Loss:  0.7425\n",
            "Epoch76 | Batch: 15 Loss:  0.4051\n",
            "Epoch76 | Batch: 16 Loss:  0.3267\n",
            "Epoch76 | Batch: 17 Loss:  0.5903\n",
            "Epoch76 | Batch: 18 Loss:  0.3409\n",
            "Epoch76 | Batch: 19 Loss:  0.3530\n",
            "Epoch76 | Batch: 20 Loss:  0.5839\n",
            "Epoch76 | Batch: 21 Loss:  0.3455\n",
            "Epoch76 | Batch: 22 Loss:  0.3987\n",
            "Epoch76 | Batch: 23 Loss:  0.3314\n",
            "Epoch76 | Batch: 24 Loss:  0.5964\n",
            "Epoch77 | Batch: 1 Loss:  0.6028\n",
            "Epoch77 | Batch: 2 Loss:  0.3789\n",
            "Epoch77 | Batch: 3 Loss:  0.5179\n",
            "Epoch77 | Batch: 4 Loss:  0.3888\n",
            "Epoch77 | Batch: 5 Loss:  0.4337\n",
            "Epoch77 | Batch: 6 Loss:  0.4223\n",
            "Epoch77 | Batch: 7 Loss:  0.6937\n",
            "Epoch77 | Batch: 8 Loss:  0.4121\n",
            "Epoch77 | Batch: 9 Loss:  0.5058\n",
            "Epoch77 | Batch: 10 Loss:  0.4581\n",
            "Epoch77 | Batch: 11 Loss:  0.5208\n",
            "Epoch77 | Batch: 12 Loss:  0.4847\n",
            "Epoch77 | Batch: 13 Loss:  0.3808\n",
            "Epoch77 | Batch: 14 Loss:  0.4993\n",
            "Epoch77 | Batch: 15 Loss:  0.3709\n",
            "Epoch77 | Batch: 16 Loss:  0.5397\n",
            "Epoch77 | Batch: 17 Loss:  0.4661\n",
            "Epoch77 | Batch: 18 Loss:  0.5695\n",
            "Epoch77 | Batch: 19 Loss:  0.3748\n",
            "Epoch77 | Batch: 20 Loss:  0.3003\n",
            "Epoch77 | Batch: 21 Loss:  0.4428\n",
            "Epoch77 | Batch: 22 Loss:  0.4227\n",
            "Epoch77 | Batch: 23 Loss:  0.4278\n",
            "Epoch77 | Batch: 24 Loss:  0.5261\n",
            "Epoch78 | Batch: 1 Loss:  0.3370\n",
            "Epoch78 | Batch: 2 Loss:  0.5277\n",
            "Epoch78 | Batch: 3 Loss:  0.6034\n",
            "Epoch78 | Batch: 4 Loss:  0.4493\n",
            "Epoch78 | Batch: 5 Loss:  0.4963\n",
            "Epoch78 | Batch: 6 Loss:  0.4963\n",
            "Epoch78 | Batch: 7 Loss:  0.3911\n",
            "Epoch78 | Batch: 8 Loss:  0.5444\n",
            "Epoch78 | Batch: 9 Loss:  0.4292\n",
            "Epoch78 | Batch: 10 Loss:  0.3767\n",
            "Epoch78 | Batch: 11 Loss:  0.4086\n",
            "Epoch78 | Batch: 12 Loss:  0.5034\n",
            "Epoch78 | Batch: 13 Loss:  0.4556\n",
            "Epoch78 | Batch: 14 Loss:  0.5525\n",
            "Epoch78 | Batch: 15 Loss:  0.4258\n",
            "Epoch78 | Batch: 16 Loss:  0.4364\n",
            "Epoch78 | Batch: 17 Loss:  0.4240\n",
            "Epoch78 | Batch: 18 Loss:  0.3756\n",
            "Epoch78 | Batch: 19 Loss:  0.5619\n",
            "Epoch78 | Batch: 20 Loss:  0.4194\n",
            "Epoch78 | Batch: 21 Loss:  0.5663\n",
            "Epoch78 | Batch: 22 Loss:  0.5662\n",
            "Epoch78 | Batch: 23 Loss:  0.4360\n",
            "Epoch78 | Batch: 24 Loss:  0.4539\n",
            "Epoch79 | Batch: 1 Loss:  0.4683\n",
            "Epoch79 | Batch: 2 Loss:  0.3910\n",
            "Epoch79 | Batch: 3 Loss:  0.4811\n",
            "Epoch79 | Batch: 4 Loss:  0.4124\n",
            "Epoch79 | Batch: 5 Loss:  0.4335\n",
            "Epoch79 | Batch: 6 Loss:  0.4556\n",
            "Epoch79 | Batch: 7 Loss:  0.4372\n",
            "Epoch79 | Batch: 8 Loss:  0.4384\n",
            "Epoch79 | Batch: 9 Loss:  0.3943\n",
            "Epoch79 | Batch: 10 Loss:  0.4667\n",
            "Epoch79 | Batch: 11 Loss:  0.4240\n",
            "Epoch79 | Batch: 12 Loss:  0.3736\n",
            "Epoch79 | Batch: 13 Loss:  0.3922\n",
            "Epoch79 | Batch: 14 Loss:  0.3994\n",
            "Epoch79 | Batch: 15 Loss:  0.5704\n",
            "Epoch79 | Batch: 16 Loss:  0.6759\n",
            "Epoch79 | Batch: 17 Loss:  0.4425\n",
            "Epoch79 | Batch: 18 Loss:  0.5899\n",
            "Epoch79 | Batch: 19 Loss:  0.5375\n",
            "Epoch79 | Batch: 20 Loss:  0.3662\n",
            "Epoch79 | Batch: 21 Loss:  0.4688\n",
            "Epoch79 | Batch: 22 Loss:  0.7072\n",
            "Epoch79 | Batch: 23 Loss:  0.3737\n",
            "Epoch79 | Batch: 24 Loss:  0.4361\n",
            "Epoch80 | Batch: 1 Loss:  0.4387\n",
            "Epoch80 | Batch: 2 Loss:  0.5261\n",
            "Epoch80 | Batch: 3 Loss:  0.5870\n",
            "Epoch80 | Batch: 4 Loss:  0.5273\n",
            "Epoch80 | Batch: 5 Loss:  0.3213\n",
            "Epoch80 | Batch: 6 Loss:  0.3414\n",
            "Epoch80 | Batch: 7 Loss:  0.6465\n",
            "Epoch80 | Batch: 8 Loss:  0.6890\n",
            "Epoch80 | Batch: 9 Loss:  0.3746\n",
            "Epoch80 | Batch: 10 Loss:  0.4842\n",
            "Epoch80 | Batch: 11 Loss:  0.3916\n",
            "Epoch80 | Batch: 12 Loss:  0.5276\n",
            "Epoch80 | Batch: 13 Loss:  0.3026\n",
            "Epoch80 | Batch: 14 Loss:  0.3615\n",
            "Epoch80 | Batch: 15 Loss:  0.4789\n",
            "Epoch80 | Batch: 16 Loss:  0.4629\n",
            "Epoch80 | Batch: 17 Loss:  0.3367\n",
            "Epoch80 | Batch: 18 Loss:  0.3482\n",
            "Epoch80 | Batch: 19 Loss:  0.5214\n",
            "Epoch80 | Batch: 20 Loss:  0.4589\n",
            "Epoch80 | Batch: 21 Loss:  0.5517\n",
            "Epoch80 | Batch: 22 Loss:  0.5198\n",
            "Epoch80 | Batch: 23 Loss:  0.4602\n",
            "Epoch80 | Batch: 24 Loss:  0.4222\n",
            "Epoch81 | Batch: 1 Loss:  0.4221\n",
            "Epoch81 | Batch: 2 Loss:  0.4532\n",
            "Epoch81 | Batch: 3 Loss:  0.3554\n",
            "Epoch81 | Batch: 4 Loss:  0.4223\n",
            "Epoch81 | Batch: 5 Loss:  0.4935\n",
            "Epoch81 | Batch: 6 Loss:  0.4322\n",
            "Epoch81 | Batch: 7 Loss:  0.7324\n",
            "Epoch81 | Batch: 8 Loss:  0.3274\n",
            "Epoch81 | Batch: 9 Loss:  0.4142\n",
            "Epoch81 | Batch: 10 Loss:  0.4311\n",
            "Epoch81 | Batch: 11 Loss:  0.5089\n",
            "Epoch81 | Batch: 12 Loss:  0.4816\n",
            "Epoch81 | Batch: 13 Loss:  0.4789\n",
            "Epoch81 | Batch: 14 Loss:  0.5312\n",
            "Epoch81 | Batch: 15 Loss:  0.5518\n",
            "Epoch81 | Batch: 16 Loss:  0.3953\n",
            "Epoch81 | Batch: 17 Loss:  0.5000\n",
            "Epoch81 | Batch: 18 Loss:  0.4584\n",
            "Epoch81 | Batch: 19 Loss:  0.4166\n",
            "Epoch81 | Batch: 20 Loss:  0.5622\n",
            "Epoch81 | Batch: 21 Loss:  0.4035\n",
            "Epoch81 | Batch: 22 Loss:  0.5750\n",
            "Epoch81 | Batch: 23 Loss:  0.3297\n",
            "Epoch81 | Batch: 24 Loss:  0.5113\n",
            "Epoch82 | Batch: 1 Loss:  0.3408\n",
            "Epoch82 | Batch: 2 Loss:  0.4107\n",
            "Epoch82 | Batch: 3 Loss:  0.3497\n",
            "Epoch82 | Batch: 4 Loss:  0.4966\n",
            "Epoch82 | Batch: 5 Loss:  0.4904\n",
            "Epoch82 | Batch: 6 Loss:  0.5395\n",
            "Epoch82 | Batch: 7 Loss:  0.4455\n",
            "Epoch82 | Batch: 8 Loss:  0.4086\n",
            "Epoch82 | Batch: 9 Loss:  0.4770\n",
            "Epoch82 | Batch: 10 Loss:  0.5320\n",
            "Epoch82 | Batch: 11 Loss:  0.5091\n",
            "Epoch82 | Batch: 12 Loss:  0.5187\n",
            "Epoch82 | Batch: 13 Loss:  0.4782\n",
            "Epoch82 | Batch: 14 Loss:  0.4009\n",
            "Epoch82 | Batch: 15 Loss:  0.4937\n",
            "Epoch82 | Batch: 16 Loss:  0.3559\n",
            "Epoch82 | Batch: 17 Loss:  0.6041\n",
            "Epoch82 | Batch: 18 Loss:  0.4540\n",
            "Epoch82 | Batch: 19 Loss:  0.4846\n",
            "Epoch82 | Batch: 20 Loss:  0.3949\n",
            "Epoch82 | Batch: 21 Loss:  0.3926\n",
            "Epoch82 | Batch: 22 Loss:  0.4665\n",
            "Epoch82 | Batch: 23 Loss:  0.6114\n",
            "Epoch82 | Batch: 24 Loss:  0.5950\n",
            "Epoch83 | Batch: 1 Loss:  0.5048\n",
            "Epoch83 | Batch: 2 Loss:  0.3345\n",
            "Epoch83 | Batch: 3 Loss:  0.5676\n",
            "Epoch83 | Batch: 4 Loss:  0.5631\n",
            "Epoch83 | Batch: 5 Loss:  0.5405\n",
            "Epoch83 | Batch: 6 Loss:  0.4800\n",
            "Epoch83 | Batch: 7 Loss:  0.4657\n",
            "Epoch83 | Batch: 8 Loss:  0.4336\n",
            "Epoch83 | Batch: 9 Loss:  0.4841\n",
            "Epoch83 | Batch: 10 Loss:  0.4860\n",
            "Epoch83 | Batch: 11 Loss:  0.5353\n",
            "Epoch83 | Batch: 12 Loss:  0.4767\n",
            "Epoch83 | Batch: 13 Loss:  0.4468\n",
            "Epoch83 | Batch: 14 Loss:  0.4142\n",
            "Epoch83 | Batch: 15 Loss:  0.3518\n",
            "Epoch83 | Batch: 16 Loss:  0.3429\n",
            "Epoch83 | Batch: 17 Loss:  0.5074\n",
            "Epoch83 | Batch: 18 Loss:  0.5030\n",
            "Epoch83 | Batch: 19 Loss:  0.4783\n",
            "Epoch83 | Batch: 20 Loss:  0.3258\n",
            "Epoch83 | Batch: 21 Loss:  0.3968\n",
            "Epoch83 | Batch: 22 Loss:  0.3231\n",
            "Epoch83 | Batch: 23 Loss:  0.5296\n",
            "Epoch83 | Batch: 24 Loss:  0.7428\n",
            "Epoch84 | Batch: 1 Loss:  0.4778\n",
            "Epoch84 | Batch: 2 Loss:  0.5929\n",
            "Epoch84 | Batch: 3 Loss:  0.5352\n",
            "Epoch84 | Batch: 4 Loss:  0.4065\n",
            "Epoch84 | Batch: 5 Loss:  0.5748\n",
            "Epoch84 | Batch: 6 Loss:  0.4139\n",
            "Epoch84 | Batch: 7 Loss:  0.5017\n",
            "Epoch84 | Batch: 8 Loss:  0.3981\n",
            "Epoch84 | Batch: 9 Loss:  0.4055\n",
            "Epoch84 | Batch: 10 Loss:  0.4109\n",
            "Epoch84 | Batch: 11 Loss:  0.4228\n",
            "Epoch84 | Batch: 12 Loss:  0.3854\n",
            "Epoch84 | Batch: 13 Loss:  0.6798\n",
            "Epoch84 | Batch: 14 Loss:  0.4433\n",
            "Epoch84 | Batch: 15 Loss:  0.5018\n",
            "Epoch84 | Batch: 16 Loss:  0.4309\n",
            "Epoch84 | Batch: 17 Loss:  0.4828\n",
            "Epoch84 | Batch: 18 Loss:  0.3663\n",
            "Epoch84 | Batch: 19 Loss:  0.6535\n",
            "Epoch84 | Batch: 20 Loss:  0.4576\n",
            "Epoch84 | Batch: 21 Loss:  0.3758\n",
            "Epoch84 | Batch: 22 Loss:  0.4060\n",
            "Epoch84 | Batch: 23 Loss:  0.4206\n",
            "Epoch84 | Batch: 24 Loss:  0.5296\n",
            "Epoch85 | Batch: 1 Loss:  0.5661\n",
            "Epoch85 | Batch: 2 Loss:  0.5676\n",
            "Epoch85 | Batch: 3 Loss:  0.5721\n",
            "Epoch85 | Batch: 4 Loss:  0.3444\n",
            "Epoch85 | Batch: 5 Loss:  0.3878\n",
            "Epoch85 | Batch: 6 Loss:  0.3473\n",
            "Epoch85 | Batch: 7 Loss:  0.3949\n",
            "Epoch85 | Batch: 8 Loss:  0.4800\n",
            "Epoch85 | Batch: 9 Loss:  0.6250\n",
            "Epoch85 | Batch: 10 Loss:  0.2960\n",
            "Epoch85 | Batch: 11 Loss:  0.6194\n",
            "Epoch85 | Batch: 12 Loss:  0.4894\n",
            "Epoch85 | Batch: 13 Loss:  0.5295\n",
            "Epoch85 | Batch: 14 Loss:  0.4522\n",
            "Epoch85 | Batch: 15 Loss:  0.3415\n",
            "Epoch85 | Batch: 16 Loss:  0.3310\n",
            "Epoch85 | Batch: 17 Loss:  0.5349\n",
            "Epoch85 | Batch: 18 Loss:  0.5090\n",
            "Epoch85 | Batch: 19 Loss:  0.4378\n",
            "Epoch85 | Batch: 20 Loss:  0.6013\n",
            "Epoch85 | Batch: 21 Loss:  0.4768\n",
            "Epoch85 | Batch: 22 Loss:  0.4418\n",
            "Epoch85 | Batch: 23 Loss:  0.4220\n",
            "Epoch85 | Batch: 24 Loss:  0.4681\n",
            "Epoch86 | Batch: 1 Loss:  0.3852\n",
            "Epoch86 | Batch: 2 Loss:  0.6467\n",
            "Epoch86 | Batch: 3 Loss:  0.5397\n",
            "Epoch86 | Batch: 4 Loss:  0.4837\n",
            "Epoch86 | Batch: 5 Loss:  0.3902\n",
            "Epoch86 | Batch: 6 Loss:  0.5092\n",
            "Epoch86 | Batch: 7 Loss:  0.3522\n",
            "Epoch86 | Batch: 8 Loss:  0.3975\n",
            "Epoch86 | Batch: 9 Loss:  0.4298\n",
            "Epoch86 | Batch: 10 Loss:  0.5916\n",
            "Epoch86 | Batch: 11 Loss:  0.4921\n",
            "Epoch86 | Batch: 12 Loss:  0.4107\n",
            "Epoch86 | Batch: 13 Loss:  0.3749\n",
            "Epoch86 | Batch: 14 Loss:  0.4902\n",
            "Epoch86 | Batch: 15 Loss:  0.4773\n",
            "Epoch86 | Batch: 16 Loss:  0.4753\n",
            "Epoch86 | Batch: 17 Loss:  0.5184\n",
            "Epoch86 | Batch: 18 Loss:  0.4448\n",
            "Epoch86 | Batch: 19 Loss:  0.4439\n",
            "Epoch86 | Batch: 20 Loss:  0.4927\n",
            "Epoch86 | Batch: 21 Loss:  0.3934\n",
            "Epoch86 | Batch: 22 Loss:  0.4582\n",
            "Epoch86 | Batch: 23 Loss:  0.5860\n",
            "Epoch86 | Batch: 24 Loss:  0.3519\n",
            "Epoch87 | Batch: 1 Loss:  0.4588\n",
            "Epoch87 | Batch: 2 Loss:  0.3627\n",
            "Epoch87 | Batch: 3 Loss:  0.4734\n",
            "Epoch87 | Batch: 4 Loss:  0.4254\n",
            "Epoch87 | Batch: 5 Loss:  0.4305\n",
            "Epoch87 | Batch: 6 Loss:  0.3579\n",
            "Epoch87 | Batch: 7 Loss:  0.5301\n",
            "Epoch87 | Batch: 8 Loss:  0.5606\n",
            "Epoch87 | Batch: 9 Loss:  0.4411\n",
            "Epoch87 | Batch: 10 Loss:  0.4741\n",
            "Epoch87 | Batch: 11 Loss:  0.4436\n",
            "Epoch87 | Batch: 12 Loss:  0.6210\n",
            "Epoch87 | Batch: 13 Loss:  0.3515\n",
            "Epoch87 | Batch: 14 Loss:  0.4309\n",
            "Epoch87 | Batch: 15 Loss:  0.6582\n",
            "Epoch87 | Batch: 16 Loss:  0.4907\n",
            "Epoch87 | Batch: 17 Loss:  0.5029\n",
            "Epoch87 | Batch: 18 Loss:  0.5724\n",
            "Epoch87 | Batch: 19 Loss:  0.4039\n",
            "Epoch87 | Batch: 20 Loss:  0.4806\n",
            "Epoch87 | Batch: 21 Loss:  0.4917\n",
            "Epoch87 | Batch: 22 Loss:  0.5161\n",
            "Epoch87 | Batch: 23 Loss:  0.3563\n",
            "Epoch87 | Batch: 24 Loss:  0.3537\n",
            "Epoch88 | Batch: 1 Loss:  0.4272\n",
            "Epoch88 | Batch: 2 Loss:  0.4399\n",
            "Epoch88 | Batch: 3 Loss:  0.5947\n",
            "Epoch88 | Batch: 4 Loss:  0.3758\n",
            "Epoch88 | Batch: 5 Loss:  0.4950\n",
            "Epoch88 | Batch: 6 Loss:  0.4487\n",
            "Epoch88 | Batch: 7 Loss:  0.4707\n",
            "Epoch88 | Batch: 8 Loss:  0.5290\n",
            "Epoch88 | Batch: 9 Loss:  0.5155\n",
            "Epoch88 | Batch: 10 Loss:  0.5384\n",
            "Epoch88 | Batch: 11 Loss:  0.4700\n",
            "Epoch88 | Batch: 12 Loss:  0.4255\n",
            "Epoch88 | Batch: 13 Loss:  0.5763\n",
            "Epoch88 | Batch: 14 Loss:  0.4200\n",
            "Epoch88 | Batch: 15 Loss:  0.4047\n",
            "Epoch88 | Batch: 16 Loss:  0.5199\n",
            "Epoch88 | Batch: 17 Loss:  0.4019\n",
            "Epoch88 | Batch: 18 Loss:  0.3497\n",
            "Epoch88 | Batch: 19 Loss:  0.3479\n",
            "Epoch88 | Batch: 20 Loss:  0.4924\n",
            "Epoch88 | Batch: 21 Loss:  0.5931\n",
            "Epoch88 | Batch: 22 Loss:  0.4572\n",
            "Epoch88 | Batch: 23 Loss:  0.3610\n",
            "Epoch88 | Batch: 24 Loss:  0.5437\n",
            "Epoch89 | Batch: 1 Loss:  0.4453\n",
            "Epoch89 | Batch: 2 Loss:  0.5677\n",
            "Epoch89 | Batch: 3 Loss:  0.3974\n",
            "Epoch89 | Batch: 4 Loss:  0.5220\n",
            "Epoch89 | Batch: 5 Loss:  0.4623\n",
            "Epoch89 | Batch: 6 Loss:  0.4421\n",
            "Epoch89 | Batch: 7 Loss:  0.3934\n",
            "Epoch89 | Batch: 8 Loss:  0.4462\n",
            "Epoch89 | Batch: 9 Loss:  0.4148\n",
            "Epoch89 | Batch: 10 Loss:  0.3297\n",
            "Epoch89 | Batch: 11 Loss:  0.5357\n",
            "Epoch89 | Batch: 12 Loss:  0.4773\n",
            "Epoch89 | Batch: 13 Loss:  0.4739\n",
            "Epoch89 | Batch: 14 Loss:  0.4737\n",
            "Epoch89 | Batch: 15 Loss:  0.5211\n",
            "Epoch89 | Batch: 16 Loss:  0.4734\n",
            "Epoch89 | Batch: 17 Loss:  0.3848\n",
            "Epoch89 | Batch: 18 Loss:  0.4454\n",
            "Epoch89 | Batch: 19 Loss:  0.5379\n",
            "Epoch89 | Batch: 20 Loss:  0.4707\n",
            "Epoch89 | Batch: 21 Loss:  0.5234\n",
            "Epoch89 | Batch: 22 Loss:  0.6774\n",
            "Epoch89 | Batch: 23 Loss:  0.4358\n",
            "Epoch89 | Batch: 24 Loss:  0.3062\n",
            "Epoch90 | Batch: 1 Loss:  0.5246\n",
            "Epoch90 | Batch: 2 Loss:  0.5511\n",
            "Epoch90 | Batch: 3 Loss:  0.2592\n",
            "Epoch90 | Batch: 4 Loss:  0.5433\n",
            "Epoch90 | Batch: 5 Loss:  0.4480\n",
            "Epoch90 | Batch: 6 Loss:  0.4768\n",
            "Epoch90 | Batch: 7 Loss:  0.5438\n",
            "Epoch90 | Batch: 8 Loss:  0.4041\n",
            "Epoch90 | Batch: 9 Loss:  0.4011\n",
            "Epoch90 | Batch: 10 Loss:  0.4643\n",
            "Epoch90 | Batch: 11 Loss:  0.4377\n",
            "Epoch90 | Batch: 12 Loss:  0.5173\n",
            "Epoch90 | Batch: 13 Loss:  0.5066\n",
            "Epoch90 | Batch: 14 Loss:  0.4649\n",
            "Epoch90 | Batch: 15 Loss:  0.5526\n",
            "Epoch90 | Batch: 16 Loss:  0.3490\n",
            "Epoch90 | Batch: 17 Loss:  0.4929\n",
            "Epoch90 | Batch: 18 Loss:  0.4711\n",
            "Epoch90 | Batch: 19 Loss:  0.4446\n",
            "Epoch90 | Batch: 20 Loss:  0.4979\n",
            "Epoch90 | Batch: 21 Loss:  0.6455\n",
            "Epoch90 | Batch: 22 Loss:  0.4493\n",
            "Epoch90 | Batch: 23 Loss:  0.3657\n",
            "Epoch90 | Batch: 24 Loss:  0.3813\n",
            "Epoch91 | Batch: 1 Loss:  0.4058\n",
            "Epoch91 | Batch: 2 Loss:  0.3779\n",
            "Epoch91 | Batch: 3 Loss:  0.5758\n",
            "Epoch91 | Batch: 4 Loss:  0.4571\n",
            "Epoch91 | Batch: 5 Loss:  0.2832\n",
            "Epoch91 | Batch: 6 Loss:  0.5609\n",
            "Epoch91 | Batch: 7 Loss:  0.3482\n",
            "Epoch91 | Batch: 8 Loss:  0.4675\n",
            "Epoch91 | Batch: 9 Loss:  0.5447\n",
            "Epoch91 | Batch: 10 Loss:  0.3639\n",
            "Epoch91 | Batch: 11 Loss:  0.5302\n",
            "Epoch91 | Batch: 12 Loss:  0.7404\n",
            "Epoch91 | Batch: 13 Loss:  0.2521\n",
            "Epoch91 | Batch: 14 Loss:  0.6183\n",
            "Epoch91 | Batch: 15 Loss:  0.4769\n",
            "Epoch91 | Batch: 16 Loss:  0.4376\n",
            "Epoch91 | Batch: 17 Loss:  0.4506\n",
            "Epoch91 | Batch: 18 Loss:  0.5071\n",
            "Epoch91 | Batch: 19 Loss:  0.4258\n",
            "Epoch91 | Batch: 20 Loss:  0.4542\n",
            "Epoch91 | Batch: 21 Loss:  0.5588\n",
            "Epoch91 | Batch: 22 Loss:  0.4396\n",
            "Epoch91 | Batch: 23 Loss:  0.5881\n",
            "Epoch91 | Batch: 24 Loss:  0.2999\n",
            "Epoch92 | Batch: 1 Loss:  0.3802\n",
            "Epoch92 | Batch: 2 Loss:  0.4007\n",
            "Epoch92 | Batch: 3 Loss:  0.4630\n",
            "Epoch92 | Batch: 4 Loss:  0.6130\n",
            "Epoch92 | Batch: 5 Loss:  0.3950\n",
            "Epoch92 | Batch: 6 Loss:  0.5513\n",
            "Epoch92 | Batch: 7 Loss:  0.4970\n",
            "Epoch92 | Batch: 8 Loss:  0.4017\n",
            "Epoch92 | Batch: 9 Loss:  0.4657\n",
            "Epoch92 | Batch: 10 Loss:  0.3931\n",
            "Epoch92 | Batch: 11 Loss:  0.4904\n",
            "Epoch92 | Batch: 12 Loss:  0.5365\n",
            "Epoch92 | Batch: 13 Loss:  0.4001\n",
            "Epoch92 | Batch: 14 Loss:  0.4452\n",
            "Epoch92 | Batch: 15 Loss:  0.4032\n",
            "Epoch92 | Batch: 16 Loss:  0.5346\n",
            "Epoch92 | Batch: 17 Loss:  0.3950\n",
            "Epoch92 | Batch: 18 Loss:  0.4980\n",
            "Epoch92 | Batch: 19 Loss:  0.3880\n",
            "Epoch92 | Batch: 20 Loss:  0.3296\n",
            "Epoch92 | Batch: 21 Loss:  0.5996\n",
            "Epoch92 | Batch: 22 Loss:  0.7610\n",
            "Epoch92 | Batch: 23 Loss:  0.4184\n",
            "Epoch92 | Batch: 24 Loss:  0.3959\n",
            "Epoch93 | Batch: 1 Loss:  0.6200\n",
            "Epoch93 | Batch: 2 Loss:  0.4412\n",
            "Epoch93 | Batch: 3 Loss:  0.3716\n",
            "Epoch93 | Batch: 4 Loss:  0.5069\n",
            "Epoch93 | Batch: 5 Loss:  0.3864\n",
            "Epoch93 | Batch: 6 Loss:  0.3868\n",
            "Epoch93 | Batch: 7 Loss:  0.3899\n",
            "Epoch93 | Batch: 8 Loss:  0.2825\n",
            "Epoch93 | Batch: 9 Loss:  0.3358\n",
            "Epoch93 | Batch: 10 Loss:  0.5389\n",
            "Epoch93 | Batch: 11 Loss:  0.4926\n",
            "Epoch93 | Batch: 12 Loss:  0.4228\n",
            "Epoch93 | Batch: 13 Loss:  0.5719\n",
            "Epoch93 | Batch: 14 Loss:  0.3665\n",
            "Epoch93 | Batch: 15 Loss:  0.4045\n",
            "Epoch93 | Batch: 16 Loss:  0.4404\n",
            "Epoch93 | Batch: 17 Loss:  0.5130\n",
            "Epoch93 | Batch: 18 Loss:  0.5198\n",
            "Epoch93 | Batch: 19 Loss:  0.6577\n",
            "Epoch93 | Batch: 20 Loss:  0.3965\n",
            "Epoch93 | Batch: 21 Loss:  0.4332\n",
            "Epoch93 | Batch: 22 Loss:  0.5753\n",
            "Epoch93 | Batch: 23 Loss:  0.6033\n",
            "Epoch93 | Batch: 24 Loss:  0.5279\n",
            "Epoch94 | Batch: 1 Loss:  0.4695\n",
            "Epoch94 | Batch: 2 Loss:  0.4932\n",
            "Epoch94 | Batch: 3 Loss:  0.3605\n",
            "Epoch94 | Batch: 4 Loss:  0.5145\n",
            "Epoch94 | Batch: 5 Loss:  0.4445\n",
            "Epoch94 | Batch: 6 Loss:  0.4488\n",
            "Epoch94 | Batch: 7 Loss:  0.6444\n",
            "Epoch94 | Batch: 8 Loss:  0.4752\n",
            "Epoch94 | Batch: 9 Loss:  0.3967\n",
            "Epoch94 | Batch: 10 Loss:  0.3559\n",
            "Epoch94 | Batch: 11 Loss:  0.2541\n",
            "Epoch94 | Batch: 12 Loss:  0.4996\n",
            "Epoch94 | Batch: 13 Loss:  0.3612\n",
            "Epoch94 | Batch: 14 Loss:  0.3269\n",
            "Epoch94 | Batch: 15 Loss:  0.4906\n",
            "Epoch94 | Batch: 16 Loss:  0.6082\n",
            "Epoch94 | Batch: 17 Loss:  0.3834\n",
            "Epoch94 | Batch: 18 Loss:  0.5064\n",
            "Epoch94 | Batch: 19 Loss:  0.4831\n",
            "Epoch94 | Batch: 20 Loss:  0.6584\n",
            "Epoch94 | Batch: 21 Loss:  0.3575\n",
            "Epoch94 | Batch: 22 Loss:  0.4831\n",
            "Epoch94 | Batch: 23 Loss:  0.6709\n",
            "Epoch94 | Batch: 24 Loss:  0.5164\n",
            "Epoch95 | Batch: 1 Loss:  0.3342\n",
            "Epoch95 | Batch: 2 Loss:  0.4586\n",
            "Epoch95 | Batch: 3 Loss:  0.4233\n",
            "Epoch95 | Batch: 4 Loss:  0.4766\n",
            "Epoch95 | Batch: 5 Loss:  0.5336\n",
            "Epoch95 | Batch: 6 Loss:  0.5174\n",
            "Epoch95 | Batch: 7 Loss:  0.4531\n",
            "Epoch95 | Batch: 8 Loss:  0.4793\n",
            "Epoch95 | Batch: 9 Loss:  0.4433\n",
            "Epoch95 | Batch: 10 Loss:  0.5941\n",
            "Epoch95 | Batch: 11 Loss:  0.4824\n",
            "Epoch95 | Batch: 12 Loss:  0.5599\n",
            "Epoch95 | Batch: 13 Loss:  0.5551\n",
            "Epoch95 | Batch: 14 Loss:  0.3372\n",
            "Epoch95 | Batch: 15 Loss:  0.6083\n",
            "Epoch95 | Batch: 16 Loss:  0.4321\n",
            "Epoch95 | Batch: 17 Loss:  0.4527\n",
            "Epoch95 | Batch: 18 Loss:  0.3835\n",
            "Epoch95 | Batch: 19 Loss:  0.3536\n",
            "Epoch95 | Batch: 20 Loss:  0.3905\n",
            "Epoch95 | Batch: 21 Loss:  0.3932\n",
            "Epoch95 | Batch: 22 Loss:  0.4781\n",
            "Epoch95 | Batch: 23 Loss:  0.5304\n",
            "Epoch95 | Batch: 24 Loss:  0.5059\n",
            "Epoch96 | Batch: 1 Loss:  0.4907\n",
            "Epoch96 | Batch: 2 Loss:  0.4989\n",
            "Epoch96 | Batch: 3 Loss:  0.6098\n",
            "Epoch96 | Batch: 4 Loss:  0.3360\n",
            "Epoch96 | Batch: 5 Loss:  0.3677\n",
            "Epoch96 | Batch: 6 Loss:  0.5075\n",
            "Epoch96 | Batch: 7 Loss:  0.3944\n",
            "Epoch96 | Batch: 8 Loss:  0.3792\n",
            "Epoch96 | Batch: 9 Loss:  0.6065\n",
            "Epoch96 | Batch: 10 Loss:  0.5204\n",
            "Epoch96 | Batch: 11 Loss:  0.4620\n",
            "Epoch96 | Batch: 12 Loss:  0.3208\n",
            "Epoch96 | Batch: 13 Loss:  0.5329\n",
            "Epoch96 | Batch: 14 Loss:  0.4212\n",
            "Epoch96 | Batch: 15 Loss:  0.4581\n",
            "Epoch96 | Batch: 16 Loss:  0.5747\n",
            "Epoch96 | Batch: 17 Loss:  0.4767\n",
            "Epoch96 | Batch: 18 Loss:  0.3516\n",
            "Epoch96 | Batch: 19 Loss:  0.5849\n",
            "Epoch96 | Batch: 20 Loss:  0.4401\n",
            "Epoch96 | Batch: 21 Loss:  0.5188\n",
            "Epoch96 | Batch: 22 Loss:  0.4623\n",
            "Epoch96 | Batch: 23 Loss:  0.3682\n",
            "Epoch96 | Batch: 24 Loss:  0.3756\n",
            "Epoch97 | Batch: 1 Loss:  0.6604\n",
            "Epoch97 | Batch: 2 Loss:  0.3743\n",
            "Epoch97 | Batch: 3 Loss:  0.5113\n",
            "Epoch97 | Batch: 4 Loss:  0.3010\n",
            "Epoch97 | Batch: 5 Loss:  0.4513\n",
            "Epoch97 | Batch: 6 Loss:  0.3996\n",
            "Epoch97 | Batch: 7 Loss:  0.5159\n",
            "Epoch97 | Batch: 8 Loss:  0.4396\n",
            "Epoch97 | Batch: 9 Loss:  0.5256\n",
            "Epoch97 | Batch: 10 Loss:  0.6196\n",
            "Epoch97 | Batch: 11 Loss:  0.3804\n",
            "Epoch97 | Batch: 12 Loss:  0.4086\n",
            "Epoch97 | Batch: 13 Loss:  0.7077\n",
            "Epoch97 | Batch: 14 Loss:  0.3239\n",
            "Epoch97 | Batch: 15 Loss:  0.4993\n",
            "Epoch97 | Batch: 16 Loss:  0.5161\n",
            "Epoch97 | Batch: 17 Loss:  0.3933\n",
            "Epoch97 | Batch: 18 Loss:  0.5981\n",
            "Epoch97 | Batch: 19 Loss:  0.3450\n",
            "Epoch97 | Batch: 20 Loss:  0.4202\n",
            "Epoch97 | Batch: 21 Loss:  0.4277\n",
            "Epoch97 | Batch: 22 Loss:  0.3559\n",
            "Epoch97 | Batch: 23 Loss:  0.5290\n",
            "Epoch97 | Batch: 24 Loss:  0.5129\n",
            "Epoch98 | Batch: 1 Loss:  0.4746\n",
            "Epoch98 | Batch: 2 Loss:  0.4158\n",
            "Epoch98 | Batch: 3 Loss:  0.2816\n",
            "Epoch98 | Batch: 4 Loss:  0.4364\n",
            "Epoch98 | Batch: 5 Loss:  0.7250\n",
            "Epoch98 | Batch: 6 Loss:  0.3473\n",
            "Epoch98 | Batch: 7 Loss:  0.4401\n",
            "Epoch98 | Batch: 8 Loss:  0.5354\n",
            "Epoch98 | Batch: 9 Loss:  0.4524\n",
            "Epoch98 | Batch: 10 Loss:  0.4960\n",
            "Epoch98 | Batch: 11 Loss:  0.3449\n",
            "Epoch98 | Batch: 12 Loss:  0.5569\n",
            "Epoch98 | Batch: 13 Loss:  0.4658\n",
            "Epoch98 | Batch: 14 Loss:  0.5118\n",
            "Epoch98 | Batch: 15 Loss:  0.3441\n",
            "Epoch98 | Batch: 16 Loss:  0.5181\n",
            "Epoch98 | Batch: 17 Loss:  0.4384\n",
            "Epoch98 | Batch: 18 Loss:  0.5087\n",
            "Epoch98 | Batch: 19 Loss:  0.4893\n",
            "Epoch98 | Batch: 20 Loss:  0.5491\n",
            "Epoch98 | Batch: 21 Loss:  0.4958\n",
            "Epoch98 | Batch: 22 Loss:  0.4851\n",
            "Epoch98 | Batch: 23 Loss:  0.4498\n",
            "Epoch98 | Batch: 24 Loss:  0.4714\n",
            "Epoch99 | Batch: 1 Loss:  0.3871\n",
            "Epoch99 | Batch: 2 Loss:  0.3323\n",
            "Epoch99 | Batch: 3 Loss:  0.4926\n",
            "Epoch99 | Batch: 4 Loss:  0.5109\n",
            "Epoch99 | Batch: 5 Loss:  0.4364\n",
            "Epoch99 | Batch: 6 Loss:  0.3147\n",
            "Epoch99 | Batch: 7 Loss:  0.6238\n",
            "Epoch99 | Batch: 8 Loss:  0.5644\n",
            "Epoch99 | Batch: 9 Loss:  0.3078\n",
            "Epoch99 | Batch: 10 Loss:  0.3758\n",
            "Epoch99 | Batch: 11 Loss:  0.3652\n",
            "Epoch99 | Batch: 12 Loss:  0.3867\n",
            "Epoch99 | Batch: 13 Loss:  0.5888\n",
            "Epoch99 | Batch: 14 Loss:  0.5640\n",
            "Epoch99 | Batch: 15 Loss:  0.6043\n",
            "Epoch99 | Batch: 16 Loss:  0.5679\n",
            "Epoch99 | Batch: 17 Loss:  0.4480\n",
            "Epoch99 | Batch: 18 Loss:  0.5925\n",
            "Epoch99 | Batch: 19 Loss:  0.4094\n",
            "Epoch99 | Batch: 20 Loss:  0.3349\n",
            "Epoch99 | Batch: 21 Loss:  0.4390\n",
            "Epoch99 | Batch: 22 Loss:  0.5499\n",
            "Epoch99 | Batch: 23 Loss:  0.4722\n",
            "Epoch99 | Batch: 24 Loss:  0.4180\n",
            "Epoch100 | Batch: 1 Loss:  0.3084\n",
            "Epoch100 | Batch: 2 Loss:  0.3073\n",
            "Epoch100 | Batch: 3 Loss:  0.4288\n",
            "Epoch100 | Batch: 4 Loss:  0.6799\n",
            "Epoch100 | Batch: 5 Loss:  0.5329\n",
            "Epoch100 | Batch: 6 Loss:  0.5033\n",
            "Epoch100 | Batch: 7 Loss:  0.5168\n",
            "Epoch100 | Batch: 8 Loss:  0.5826\n",
            "Epoch100 | Batch: 9 Loss:  0.5933\n",
            "Epoch100 | Batch: 10 Loss:  0.5282\n",
            "Epoch100 | Batch: 11 Loss:  0.4573\n",
            "Epoch100 | Batch: 12 Loss:  0.5558\n",
            "Epoch100 | Batch: 13 Loss:  0.4247\n",
            "Epoch100 | Batch: 14 Loss:  0.3429\n",
            "Epoch100 | Batch: 15 Loss:  0.3781\n",
            "Epoch100 | Batch: 16 Loss:  0.2579\n",
            "Epoch100 | Batch: 17 Loss:  0.4314\n",
            "Epoch100 | Batch: 18 Loss:  0.4994\n",
            "Epoch100 | Batch: 19 Loss:  0.4649\n",
            "Epoch100 | Batch: 20 Loss:  0.4039\n",
            "Epoch100 | Batch: 21 Loss:  0.4156\n",
            "Epoch100 | Batch: 22 Loss:  0.5809\n",
            "Epoch100 | Batch: 23 Loss:  0.4852\n",
            "Epoch100 | Batch: 24 Loss:  0.5459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "35d12a56ccec4822903f72dff0571599",
            "a916deee231c4c57b0635e21d1f8507c",
            "89a9296cefae48a98bc9768e270195aa",
            "aaaa69a1d442436e80cdda3fba401dca",
            "6b42aa59977849499907f96c649cb9e0",
            "fcffc91e3a3f4e1cb933dc5e29730c74",
            "77b448c438cb4c06ad7ebd4ca0cde425",
            "90a7b50114df40d7861a46d03f7d4ba0",
            "0c950c1164ba4ff1b4f9107aff5f5380",
            "58a0ff3af3c14d7f8eab283b850570b9",
            "d2b48322b9924fef9125423cfa05a922",
            "ffa83ab75dd14882912de6ce776546f6",
            "1c80d2d4de164d5a97eba77ec186aa41",
            "8118c40155ed421b84085843399384a9",
            "e3359a8a5be745fab13746dfce768dd3",
            "646de78a4b53418fbc03eccb575d93e9",
            "66279bcef4594e6daccc89700f68165c",
            "b7f57dca86f34ba682b06f072d5d6e65",
            "8067758e0b6a416b9b8fb92c46871311",
            "b13d507ef05746ac9b1786765b11101a",
            "9d950919c97e4d46ad50eb23944429db",
            "0234ef83a0c5417a8444428a8aa08758",
            "293e0e381a6348bca4fcc05c32c04372",
            "513431a79038426faa8cd8d9dd2cfe77",
            "243aca994f0340e99876b38ca7e500d5",
            "919cfe3cfc2744f7956461423c2b5b4d",
            "8133668263d7402cbc201dee8d909bf8",
            "d19638d95d58404b9e2f10fa197f5a5e",
            "dafd6d4b49064a8e891417935545c180",
            "f6432e99bb0b41a18f81dc0999f00e3b",
            "f56eeda3db7c451e8c04738722d306aa",
            "180d759228bc464084922b048a07fff1",
            "2e8de30ba40443789e4cf5ef2912cf4d",
            "587ac2edcca94c1ab5d9b6abb02ce30f",
            "40e9e002f0f94cfc83cbc74f95fc7615",
            "a237f6f27d0b4ea788ac7e0c97f2af06",
            "2828a0125bd94c2890e6dc42f2409440",
            "82714d7e76bd463cb60177e281a7f57c",
            "5e2e1695ed984ce0b6bd014da5876a60",
            "6b27772fd78549918a605669b71f9bce",
            "9d47169914be471e98dc28ce9ae5cf2c",
            "189fbf8d70264bbab1ee320c3634de6e",
            "e10e057eb2964c1191908117b94f22da",
            "921502bfd1f449188c375420af64da9e"
          ]
        },
        "id": "2GoGmZdMzR9K",
        "outputId": "af7eb0a9-8ac8-46b1-ecb1-ac4e8cebba04"
      },
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MNIST Model on cpu\n",
            "============================================\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35d12a56ccec4822903f72dff0571599",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffa83ab75dd14882912de6ce776546f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "293e0e381a6348bca4fcc05c32c04372",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "587ac2edcca94c1ab5d9b6abb02ce30f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.299730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.293741\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.270647\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.227532\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.209363\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.174132\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.094544\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 1.973746\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 1.858489\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 1.596246\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 1.344402\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 1.086075\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 0.929347\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 0.717807\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 0.752362\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 0.572052\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 0.511398\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 0.617343\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 0.589804\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 0.610508\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 0.688527\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 0.368345\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 0.407339\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 0.280099\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 0.436105\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 0.593330\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 0.296441\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 0.443161\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 0.428385\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 0.322009\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 0.243955\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 0.491652\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 0.358524\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 0.589211\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 0.286297\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 0.413491\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 0.263573\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 0.244589\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 0.285619\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 0.270016\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 0.191347\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 0.265825\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 0.275968\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 0.334322\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 0.235343\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 0.405397\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 0.435775\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 0.239560\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 0.188626\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 0.244877\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.197033\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 0.319611\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 0.571059\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 0.259887\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 0.381906\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 0.174250\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 0.213251\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 0.293279\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 0.208315\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 0.487538\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 0.217798\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 0.202942\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 0.354087\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 0.378718\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 0.212096\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 0.142729\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 0.263848\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 0.238241\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 0.218762\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 0.158328\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 0.227883\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 0.140935\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 0.139988\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 0.205374\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 0.146708\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 0.390675\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 0.129213\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 0.277099\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 0.242394\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 0.304059\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 0.132794\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 0.152437\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 0.126368\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 0.327994\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 0.225648\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 0.238717\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 0.198155\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 0.466178\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 0.154108\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 0.155339\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 0.209040\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 0.059066\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 0.392106\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 0.118836\n",
            "Training time: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0027, Accuracy: 9496/10000 (95%)\n",
            "Testing time: 0m 26s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.182956\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 0.058516\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 0.290593\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 0.153244\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 0.152902\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 0.193868\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.074410\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.094652\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.353422\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.154873\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.253987\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.074723\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.289330\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.128275\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.179304\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.265670\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.243445\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.329243\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.092043\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.225940\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.145526\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.179868\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.140981\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.184041\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.226799\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.169447\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.178876\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.086271\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.174111\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.208347\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.312751\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.089117\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.131035\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.271536\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.082668\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.103621\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.145999\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.160489\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.132313\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.191944\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.100359\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.351116\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.121513\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.083846\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.195266\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.157504\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.129249\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.260464\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.302646\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.110094\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.158316\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.096118\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.045450\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.453530\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.120807\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.035705\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.194531\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.332379\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.174494\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.110136\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.233968\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.192726\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.076850\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.180849\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.307331\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.136296\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.105576\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.066890\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.160915\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.208695\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.164187\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.163562\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.127755\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.168206\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.140871\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.098739\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.048245\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.163234\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.121112\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.123391\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.083718\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.132928\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.087908\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.378895\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.040903\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.104741\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.055662\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.057976\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.110823\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.072465\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.074523\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.127362\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.060539\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.259073\n",
            "Training time: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0017, Accuracy: 9689/10000 (97%)\n",
            "Testing time: 0m 26s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.183983\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.120549\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.172337\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.145424\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.151042\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.124266\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.127180\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.097456\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.138897\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.211841\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.182587\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.150373\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.119568\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.171619\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.316464\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.096236\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.227798\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.059529\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.066463\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.120776\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.132086\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.093295\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.087796\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.115583\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.038798\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.211060\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.046622\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.106236\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.173715\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.133039\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.140100\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.047926\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.057736\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.115396\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.173466\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.176431\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.059746\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.038799\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.047334\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.038587\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.042584\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.049585\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.160753\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.086257\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.114641\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.090839\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.118148\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.188954\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.244283\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.146442\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.063422\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.139028\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.119515\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.027313\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.234946\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.045451\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.186433\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.109464\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.287990\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.326380\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.051457\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.131848\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.113544\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.040852\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.068168\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.056994\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.082817\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.153202\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.192245\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.103844\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.044882\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.082645\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.372343\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.017269\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.094940\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.044296\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.160596\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.091395\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.151232\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.062383\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.031588\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.022840\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.170197\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.208697\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.140188\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.029109\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.046795\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.069573\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.039063\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.137119\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.080509\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.150749\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.057892\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.070422\n",
            "Training time: 0m 23s\n",
            "===========================\n",
            "Test set: Average loss: 0.0015, Accuracy: 9717/10000 (97%)\n",
            "Testing time: 0m 26s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.081466\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.085630\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.045368\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.201867\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.102022\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.087046\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.081849\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.060787\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.103541\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.161093\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.114457\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.078086\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.063505\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.093156\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.119392\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.048305\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.095643\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.039483\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.044086\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.124365\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.272414\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.037803\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.060263\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.086092\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.049923\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.158272\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.074615\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.050141\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.174527\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.085669\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.024874\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.105287\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.057285\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.133755\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.037400\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.027385\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.191078\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.129581\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.069787\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.079414\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.113601\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.027440\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.102301\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.183899\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.075192\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.045992\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.200117\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.126818\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.035567\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.283517\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.186900\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.053714\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.045719\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.084823\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.094923\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.043069\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.024164\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.063601\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.195459\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.089067\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.070993\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.084569\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.217093\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.045909\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.124066\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.130841\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.103675\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.124647\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.290761\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.155350\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.161792\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.058803\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.123224\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.057946\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.052030\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.128200\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.049438\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.147740\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.044930\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.041984\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.041254\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.097350\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.056911\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.053003\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.126495\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.099630\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.114592\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.063280\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.039253\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.042191\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.045604\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.055255\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.055775\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.198055\n",
            "Training time: 0m 23s\n",
            "===========================\n",
            "Test set: Average loss: 0.0012, Accuracy: 9776/10000 (98%)\n",
            "Testing time: 0m 25s\n",
            "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.066748\n",
            "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.098993\n",
            "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.100728\n",
            "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.068919\n",
            "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.028049\n",
            "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.081061\n",
            "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.055599\n",
            "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.015609\n",
            "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.022583\n",
            "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.251342\n",
            "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.105062\n",
            "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.146455\n",
            "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.177327\n",
            "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.145871\n",
            "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.099335\n",
            "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.130985\n",
            "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.097263\n",
            "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.045382\n",
            "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.131925\n",
            "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.125298\n",
            "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.145629\n",
            "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.114132\n",
            "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.175160\n",
            "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.115038\n",
            "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.024948\n",
            "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.007215\n",
            "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.020249\n",
            "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.186750\n",
            "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.045700\n",
            "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.041899\n",
            "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.160556\n",
            "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.087855\n",
            "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.011819\n",
            "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.069666\n",
            "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.167362\n",
            "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.050493\n",
            "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.088102\n",
            "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.126809\n",
            "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.066462\n",
            "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.041096\n",
            "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.149249\n",
            "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.096542\n",
            "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.130485\n",
            "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.069991\n",
            "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.126427\n",
            "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.050128\n",
            "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.019409\n",
            "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.063668\n",
            "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.035750\n",
            "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.123510\n",
            "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.049677\n",
            "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.050053\n",
            "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.129221\n",
            "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.058117\n",
            "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.083288\n",
            "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.016135\n",
            "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.029030\n",
            "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.023109\n",
            "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.013205\n",
            "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.013339\n",
            "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.032600\n",
            "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.188169\n",
            "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.070021\n",
            "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.078021\n",
            "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.025780\n",
            "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.161158\n",
            "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.049676\n",
            "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.077851\n",
            "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.029667\n",
            "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.181019\n",
            "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.073909\n",
            "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.133199\n",
            "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.027001\n",
            "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.028400\n",
            "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.090674\n",
            "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.083921\n",
            "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.051176\n",
            "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.056205\n",
            "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.060708\n",
            "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.090311\n",
            "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.047472\n",
            "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.233473\n",
            "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.068884\n",
            "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.199823\n",
            "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.120203\n",
            "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.022542\n",
            "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.045062\n",
            "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.024915\n",
            "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.034751\n",
            "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.110682\n",
            "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.100111\n",
            "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.010507\n",
            "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.183050\n",
            "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.076139\n",
            "Training time: 0m 23s\n",
            "===========================\n",
            "Test set: Average loss: 0.0010, Accuracy: 9786/10000 (98%)\n",
            "Testing time: 0m 25s\n",
            "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.074029\n",
            "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.027594\n",
            "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.014015\n",
            "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.124623\n",
            "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.131046\n",
            "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.025285\n",
            "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.094803\n",
            "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.117839\n",
            "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.024332\n",
            "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.024516\n",
            "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.094269\n",
            "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.044765\n",
            "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.052801\n",
            "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.080416\n",
            "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.021764\n",
            "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.025506\n",
            "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.033291\n",
            "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.134356\n",
            "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.027912\n",
            "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.088000\n",
            "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.053422\n",
            "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.055864\n",
            "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.035473\n",
            "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.141452\n",
            "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.018623\n",
            "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.064534\n",
            "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.133514\n",
            "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.016824\n",
            "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.040415\n",
            "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.126383\n",
            "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.066181\n",
            "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.059194\n",
            "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.036683\n",
            "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.036784\n",
            "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.079265\n",
            "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.026118\n",
            "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.040049\n",
            "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.012291\n",
            "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.027639\n",
            "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.015712\n",
            "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.046941\n",
            "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.035282\n",
            "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.040073\n",
            "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.058779\n",
            "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.018371\n",
            "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.007732\n",
            "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.072712\n",
            "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.085051\n",
            "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.044343\n",
            "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.062093\n",
            "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.148154\n",
            "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.109571\n",
            "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.124761\n",
            "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.021306\n",
            "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.038428\n",
            "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.022712\n",
            "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.034772\n",
            "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.066149\n",
            "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.140052\n",
            "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.044847\n",
            "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.030423\n",
            "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.080112\n",
            "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.042060\n",
            "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.088965\n",
            "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.029694\n",
            "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.037998\n",
            "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.036504\n",
            "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.049055\n",
            "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.155953\n",
            "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.049822\n",
            "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.048585\n",
            "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.062764\n",
            "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.036472\n",
            "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.220691\n",
            "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.140854\n",
            "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.095293\n",
            "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.056201\n",
            "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.097910\n",
            "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.037315\n",
            "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.083048\n",
            "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.067977\n",
            "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.126547\n",
            "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.112572\n",
            "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.064459\n",
            "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.106858\n",
            "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.099762\n",
            "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.052653\n",
            "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.027769\n",
            "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.044858\n",
            "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.078725\n",
            "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.010281\n",
            "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.070918\n",
            "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.131234\n",
            "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.072512\n",
            "Training time: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0010, Accuracy: 9803/10000 (98%)\n",
            "Testing time: 0m 26s\n",
            "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.121150\n",
            "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.070294\n",
            "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.196668\n",
            "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.028666\n",
            "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.020905\n",
            "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.037147\n",
            "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.170758\n",
            "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.030564\n",
            "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.047644\n",
            "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.033457\n",
            "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.055991\n",
            "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.084879\n",
            "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.089650\n",
            "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.035408\n",
            "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.086717\n",
            "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.073409\n",
            "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.047181\n",
            "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.143125\n",
            "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.022832\n",
            "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.035714\n",
            "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.088256\n",
            "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.088716\n",
            "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.065000\n",
            "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.077130\n",
            "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.071256\n",
            "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.060407\n",
            "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.034196\n",
            "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.030443\n",
            "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.029665\n",
            "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.101051\n",
            "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.019767\n",
            "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.047285\n",
            "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.026348\n",
            "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.077482\n",
            "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.046636\n",
            "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.066475\n",
            "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.139513\n",
            "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.039915\n",
            "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.054763\n",
            "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.099468\n",
            "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.113472\n",
            "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.013483\n",
            "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.084544\n",
            "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.040113\n",
            "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.012314\n",
            "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.026209\n",
            "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.022138\n",
            "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.049028\n",
            "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.049610\n",
            "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.046003\n",
            "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.092536\n",
            "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.045462\n",
            "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.058741\n",
            "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.161356\n",
            "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.016557\n",
            "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.153800\n",
            "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.007299\n",
            "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.021201\n",
            "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.094294\n",
            "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.088176\n",
            "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.033780\n",
            "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.019839\n",
            "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.059655\n",
            "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.095354\n",
            "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.038864\n",
            "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.112880\n",
            "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.077054\n",
            "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.033318\n",
            "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.038108\n",
            "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.042614\n",
            "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.007880\n",
            "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.046592\n",
            "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.012248\n",
            "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.126421\n",
            "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.084861\n",
            "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.007202\n",
            "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.025793\n",
            "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.018712\n",
            "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.122341\n",
            "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.068502\n",
            "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.122142\n",
            "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.114228\n",
            "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.074957\n",
            "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.128413\n",
            "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.013443\n",
            "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.054432\n",
            "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.018995\n",
            "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.137111\n",
            "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.061436\n",
            "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.044802\n",
            "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.157331\n",
            "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.250213\n",
            "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.011741\n",
            "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.080223\n",
            "Training time: 0m 23s\n",
            "===========================\n",
            "Test set: Average loss: 0.0010, Accuracy: 9792/10000 (98%)\n",
            "Testing time: 0m 25s\n",
            "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.041633\n",
            "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.092343\n",
            "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.020469\n",
            "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.007179\n",
            "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.227169\n",
            "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.048561\n",
            "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.056251\n",
            "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.051486\n",
            "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.019270\n",
            "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.062619\n",
            "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.008418\n",
            "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.012202\n",
            "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.041538\n",
            "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.157880\n",
            "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.031088\n",
            "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.015584\n",
            "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.074939\n",
            "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.026292\n",
            "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.029180\n",
            "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.086980\n",
            "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.088328\n",
            "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.089070\n",
            "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.028225\n",
            "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.015983\n",
            "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.040831\n",
            "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.054153\n",
            "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.065588\n",
            "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.040338\n",
            "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.006228\n",
            "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.008319\n",
            "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.059360\n",
            "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.006634\n",
            "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.064779\n",
            "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.088677\n",
            "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.033623\n",
            "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.040553\n",
            "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.095335\n",
            "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.071549\n",
            "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.198683\n",
            "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.062342\n",
            "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.060827\n",
            "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.103160\n",
            "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.064972\n",
            "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.015452\n",
            "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.032947\n",
            "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.126797\n",
            "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.019627\n",
            "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.137562\n",
            "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.135635\n",
            "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.017736\n",
            "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.058103\n",
            "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.039906\n",
            "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.063122\n",
            "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.129347\n",
            "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.193639\n",
            "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.124795\n",
            "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.021298\n",
            "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.131236\n",
            "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.075639\n",
            "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.051241\n",
            "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.125538\n",
            "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.128506\n",
            "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.169234\n",
            "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.080513\n",
            "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.025087\n",
            "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.077499\n",
            "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.381808\n",
            "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.105102\n",
            "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.047594\n",
            "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.034614\n",
            "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.018219\n",
            "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.057330\n",
            "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.154504\n",
            "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.175852\n",
            "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.091936\n",
            "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.060117\n",
            "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.036422\n",
            "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.038063\n",
            "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.048977\n",
            "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.012820\n",
            "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.052891\n",
            "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.126952\n",
            "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.009828\n",
            "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.015536\n",
            "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.006281\n",
            "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.034633\n",
            "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.064160\n",
            "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.152737\n",
            "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.067278\n",
            "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.013406\n",
            "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.065690\n",
            "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.057411\n",
            "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.040391\n",
            "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.024784\n",
            "Training time: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0009, Accuracy: 9805/10000 (98%)\n",
            "Testing time: 0m 26s\n",
            "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.047168\n",
            "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.062122\n",
            "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.123153\n",
            "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.020935\n",
            "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.019552\n",
            "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.087127\n",
            "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.023511\n",
            "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.047606\n",
            "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.045237\n",
            "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.031244\n",
            "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.085783\n",
            "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.010768\n",
            "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.014354\n",
            "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.095474\n",
            "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.034138\n",
            "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.013012\n",
            "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.036597\n",
            "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.085388\n",
            "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.041554\n",
            "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.015536\n",
            "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.047348\n",
            "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.084706\n",
            "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.044630\n",
            "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.068845\n",
            "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.037555\n",
            "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.051864\n",
            "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.044944\n",
            "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.083992\n",
            "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.056056\n",
            "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.022525\n",
            "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.071941\n",
            "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.038634\n",
            "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.104170\n",
            "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.025456\n",
            "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.109023\n",
            "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.052275\n",
            "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.007909\n",
            "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.022205\n",
            "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.060253\n",
            "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.092832\n",
            "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.039286\n",
            "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.041328\n",
            "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.017898\n",
            "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.076165\n",
            "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.019635\n",
            "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.032639\n",
            "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.003665\n",
            "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.008363\n",
            "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.027678\n",
            "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.089928\n",
            "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.018165\n",
            "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.211754\n",
            "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.006264\n",
            "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.033116\n",
            "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.054562\n",
            "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.020410\n",
            "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.056306\n",
            "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.027034\n",
            "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.038091\n",
            "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.045421\n",
            "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.058014\n",
            "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.019226\n",
            "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.068386\n",
            "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.196397\n",
            "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.080605\n",
            "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.035685\n",
            "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.129917\n",
            "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.030343\n",
            "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.029072\n",
            "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.181354\n",
            "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.038370\n",
            "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.029289\n",
            "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.056701\n",
            "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.077290\n",
            "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.083355\n",
            "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.046412\n",
            "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.014802\n",
            "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.073531\n",
            "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.032667\n",
            "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.099779\n",
            "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.022010\n",
            "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.016724\n",
            "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.227201\n",
            "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.045341\n",
            "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.024291\n",
            "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.092459\n",
            "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.026174\n",
            "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.006505\n",
            "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.025990\n",
            "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.037867\n",
            "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.100199\n",
            "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.074869\n",
            "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.050744\n",
            "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.077939\n",
            "Training time: 0m 24s\n",
            "===========================\n",
            "Test set: Average loss: 0.0008, Accuracy: 9848/10000 (98%)\n",
            "Testing time: 0m 26s\n",
            "Total Time: 3m 51s\n",
            "Model was trained on cpu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMmFM5r6Vb5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}