{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuPkOhi5MEAg7iEPTxn8p9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/pytorch_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0r4KOL2a_Ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868d4041-17c8-41d2-ef89-052a6da122f4"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing mnist model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "train_dataset = datasets.MNIST(root = './mnist_data?',\n",
        "                                train = True,\n",
        "                                transform=transforms.ToTensor(),\n",
        "                                download=True\n",
        "                                )\n",
        "test_dataset=datasets.MNIST(root = './mnist_data?',\n",
        "                                train = False,\n",
        "                                transform=transforms.ToTensor(),\n",
        "                                )\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1=nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2=nn.Conv2d(10,20,kernel_size=5)\n",
        "    self.mp=nn.MaxPool2d(2)\n",
        "    self.fc=nn.Linear(320,10)\n",
        "  def forward(self,x):\n",
        "    in_size=x.size(0)\n",
        "    x=F.relu(self.mp(self.conv1(x)))\n",
        "    x=F.relu(self.mp(self.conv2(x)))\n",
        "    x=x.view(in_size,-1)\n",
        "    x=self.fc(x)\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "model=Net()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traing mnist model on cpu\n",
            "============================================\n",
            "train epoch: 1 | batch staus: 0/60000 ( 0%) | Loss:  2.321494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch: 1 | batch staus: 640/60000 ( 1%) | Loss:  2.309114\n",
            "train epoch: 1 | batch staus: 1280/60000 ( 2%) | Loss:  2.308627\n",
            "train epoch: 1 | batch staus: 1920/60000 ( 3%) | Loss:  2.283464\n",
            "train epoch: 1 | batch staus: 2560/60000 ( 4%) | Loss:  2.284811\n",
            "train epoch: 1 | batch staus: 3200/60000 ( 5%) | Loss:  2.270162\n",
            "train epoch: 1 | batch staus: 3840/60000 ( 6%) | Loss:  2.252997\n",
            "train epoch: 1 | batch staus: 4480/60000 ( 7%) | Loss:  2.240324\n",
            "train epoch: 1 | batch staus: 5120/60000 ( 9%) | Loss:  2.221005\n",
            "train epoch: 1 | batch staus: 5760/60000 ( 10%) | Loss:  2.191262\n",
            "train epoch: 1 | batch staus: 6400/60000 ( 11%) | Loss:  2.129405\n",
            "train epoch: 1 | batch staus: 7040/60000 ( 12%) | Loss:  2.095781\n",
            "train epoch: 1 | batch staus: 7680/60000 ( 13%) | Loss:  2.029686\n",
            "train epoch: 1 | batch staus: 8320/60000 ( 14%) | Loss:  1.843876\n",
            "train epoch: 1 | batch staus: 8960/60000 ( 15%) | Loss:  1.767946\n",
            "train epoch: 1 | batch staus: 9600/60000 ( 16%) | Loss:  1.374155\n",
            "train epoch: 1 | batch staus: 10240/60000 ( 17%) | Loss:  1.077036\n",
            "train epoch: 1 | batch staus: 10880/60000 ( 18%) | Loss:  0.960165\n",
            "train epoch: 1 | batch staus: 11520/60000 ( 19%) | Loss:  0.793860\n",
            "train epoch: 1 | batch staus: 12160/60000 ( 20%) | Loss:  0.632297\n",
            "train epoch: 1 | batch staus: 12800/60000 ( 21%) | Loss:  0.670483\n",
            "train epoch: 1 | batch staus: 13440/60000 ( 22%) | Loss:  0.772626\n",
            "train epoch: 1 | batch staus: 14080/60000 ( 23%) | Loss:  0.575803\n",
            "train epoch: 1 | batch staus: 14720/60000 ( 25%) | Loss:  0.566178\n",
            "train epoch: 1 | batch staus: 15360/60000 ( 26%) | Loss:  0.352134\n",
            "train epoch: 1 | batch staus: 16000/60000 ( 27%) | Loss:  0.506521\n",
            "train epoch: 1 | batch staus: 16640/60000 ( 28%) | Loss:  0.386770\n",
            "train epoch: 1 | batch staus: 17280/60000 ( 29%) | Loss:  0.366090\n",
            "train epoch: 1 | batch staus: 17920/60000 ( 30%) | Loss:  0.648337\n",
            "train epoch: 1 | batch staus: 18560/60000 ( 31%) | Loss:  0.592699\n",
            "train epoch: 1 | batch staus: 19200/60000 ( 32%) | Loss:  0.319327\n",
            "train epoch: 1 | batch staus: 19840/60000 ( 33%) | Loss:  0.515408\n",
            "train epoch: 1 | batch staus: 20480/60000 ( 34%) | Loss:  0.379795\n",
            "train epoch: 1 | batch staus: 21120/60000 ( 35%) | Loss:  0.417942\n",
            "train epoch: 1 | batch staus: 21760/60000 ( 36%) | Loss:  0.407985\n",
            "train epoch: 1 | batch staus: 22400/60000 ( 37%) | Loss:  0.426480\n",
            "train epoch: 1 | batch staus: 23040/60000 ( 38%) | Loss:  0.328936\n",
            "train epoch: 1 | batch staus: 23680/60000 ( 39%) | Loss:  0.410991\n",
            "train epoch: 1 | batch staus: 24320/60000 ( 41%) | Loss:  0.415300\n",
            "train epoch: 1 | batch staus: 24960/60000 ( 42%) | Loss:  0.361220\n",
            "train epoch: 1 | batch staus: 25600/60000 ( 43%) | Loss:  0.431001\n",
            "train epoch: 1 | batch staus: 26240/60000 ( 44%) | Loss:  0.343921\n",
            "train epoch: 1 | batch staus: 26880/60000 ( 45%) | Loss:  0.304522\n",
            "train epoch: 1 | batch staus: 27520/60000 ( 46%) | Loss:  0.435342\n",
            "train epoch: 1 | batch staus: 28160/60000 ( 47%) | Loss:  0.294558\n",
            "train epoch: 1 | batch staus: 28800/60000 ( 48%) | Loss:  0.267025\n",
            "train epoch: 1 | batch staus: 29440/60000 ( 49%) | Loss:  0.411004\n",
            "train epoch: 1 | batch staus: 30080/60000 ( 50%) | Loss:  0.334930\n",
            "train epoch: 1 | batch staus: 30720/60000 ( 51%) | Loss:  0.344145\n",
            "train epoch: 1 | batch staus: 31360/60000 ( 52%) | Loss:  0.307319\n",
            "train epoch: 1 | batch staus: 32000/60000 ( 53%) | Loss:  0.390776\n",
            "train epoch: 1 | batch staus: 32640/60000 ( 54%) | Loss:  0.186903\n",
            "train epoch: 1 | batch staus: 33280/60000 ( 55%) | Loss:  0.400687\n",
            "train epoch: 1 | batch staus: 33920/60000 ( 57%) | Loss:  0.487070\n",
            "train epoch: 1 | batch staus: 34560/60000 ( 58%) | Loss:  0.325175\n",
            "train epoch: 1 | batch staus: 35200/60000 ( 59%) | Loss:  0.216657\n",
            "train epoch: 1 | batch staus: 35840/60000 ( 60%) | Loss:  0.582904\n",
            "train epoch: 1 | batch staus: 36480/60000 ( 61%) | Loss:  0.197230\n",
            "train epoch: 1 | batch staus: 37120/60000 ( 62%) | Loss:  0.327679\n",
            "train epoch: 1 | batch staus: 37760/60000 ( 63%) | Loss:  0.201971\n",
            "train epoch: 1 | batch staus: 38400/60000 ( 64%) | Loss:  0.176719\n",
            "train epoch: 1 | batch staus: 39040/60000 ( 65%) | Loss:  0.292159\n",
            "train epoch: 1 | batch staus: 39680/60000 ( 66%) | Loss:  0.384670\n",
            "train epoch: 1 | batch staus: 40320/60000 ( 67%) | Loss:  0.379877\n",
            "train epoch: 1 | batch staus: 40960/60000 ( 68%) | Loss:  0.271460\n",
            "train epoch: 1 | batch staus: 41600/60000 ( 69%) | Loss:  0.361955\n",
            "train epoch: 1 | batch staus: 42240/60000 ( 70%) | Loss:  0.269214\n",
            "train epoch: 1 | batch staus: 42880/60000 ( 71%) | Loss:  0.299972\n",
            "train epoch: 1 | batch staus: 43520/60000 ( 72%) | Loss:  0.271605\n",
            "train epoch: 1 | batch staus: 44160/60000 ( 74%) | Loss:  0.289448\n",
            "train epoch: 1 | batch staus: 44800/60000 ( 75%) | Loss:  0.208414\n",
            "train epoch: 1 | batch staus: 45440/60000 ( 76%) | Loss:  0.181323\n",
            "train epoch: 1 | batch staus: 46080/60000 ( 77%) | Loss:  0.302768\n",
            "train epoch: 1 | batch staus: 46720/60000 ( 78%) | Loss:  0.190782\n",
            "train epoch: 1 | batch staus: 47360/60000 ( 79%) | Loss:  0.132702\n",
            "train epoch: 1 | batch staus: 48000/60000 ( 80%) | Loss:  0.301313\n",
            "train epoch: 1 | batch staus: 48640/60000 ( 81%) | Loss:  0.207415\n",
            "train epoch: 1 | batch staus: 49280/60000 ( 82%) | Loss:  0.186925\n",
            "train epoch: 1 | batch staus: 49920/60000 ( 83%) | Loss:  0.052715\n",
            "train epoch: 1 | batch staus: 50560/60000 ( 84%) | Loss:  0.132654\n",
            "train epoch: 1 | batch staus: 51200/60000 ( 85%) | Loss:  0.166951\n",
            "train epoch: 1 | batch staus: 51840/60000 ( 86%) | Loss:  0.245087\n",
            "train epoch: 1 | batch staus: 52480/60000 ( 87%) | Loss:  0.201867\n",
            "train epoch: 1 | batch staus: 53120/60000 ( 88%) | Loss:  0.317926\n",
            "train epoch: 1 | batch staus: 53760/60000 ( 90%) | Loss:  0.249523\n",
            "train epoch: 1 | batch staus: 54400/60000 ( 91%) | Loss:  0.108045\n",
            "train epoch: 1 | batch staus: 55040/60000 ( 92%) | Loss:  0.227000\n",
            "train epoch: 1 | batch staus: 55680/60000 ( 93%) | Loss:  0.281679\n",
            "train epoch: 1 | batch staus: 56320/60000 ( 94%) | Loss:  0.285334\n",
            "train epoch: 1 | batch staus: 56960/60000 ( 95%) | Loss:  0.186676\n",
            "train epoch: 1 | batch staus: 57600/60000 ( 96%) | Loss:  0.142855\n",
            "train epoch: 1 | batch staus: 58240/60000 ( 97%) | Loss:  0.347611\n",
            "train epoch: 1 | batch staus: 58880/60000 ( 98%) | Loss:  0.290630\n",
            "train epoch: 1 | batch staus: 59520/60000 ( 99%) | Loss:  0.180043\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0030, Accuracy: 9418/10000(94%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 2 | batch staus: 0/60000 ( 0%) | Loss:  0.186358\n",
            "train epoch: 2 | batch staus: 640/60000 ( 1%) | Loss:  0.174151\n",
            "train epoch: 2 | batch staus: 1280/60000 ( 2%) | Loss:  0.138391\n",
            "train epoch: 2 | batch staus: 1920/60000 ( 3%) | Loss:  0.205222\n",
            "train epoch: 2 | batch staus: 2560/60000 ( 4%) | Loss:  0.257913\n",
            "train epoch: 2 | batch staus: 3200/60000 ( 5%) | Loss:  0.107890\n",
            "train epoch: 2 | batch staus: 3840/60000 ( 6%) | Loss:  0.397633\n",
            "train epoch: 2 | batch staus: 4480/60000 ( 7%) | Loss:  0.306254\n",
            "train epoch: 2 | batch staus: 5120/60000 ( 9%) | Loss:  0.163640\n",
            "train epoch: 2 | batch staus: 5760/60000 ( 10%) | Loss:  0.213447\n",
            "train epoch: 2 | batch staus: 6400/60000 ( 11%) | Loss:  0.165245\n",
            "train epoch: 2 | batch staus: 7040/60000 ( 12%) | Loss:  0.426768\n",
            "train epoch: 2 | batch staus: 7680/60000 ( 13%) | Loss:  0.121486\n",
            "train epoch: 2 | batch staus: 8320/60000 ( 14%) | Loss:  0.151448\n",
            "train epoch: 2 | batch staus: 8960/60000 ( 15%) | Loss:  0.227075\n",
            "train epoch: 2 | batch staus: 9600/60000 ( 16%) | Loss:  0.252206\n",
            "train epoch: 2 | batch staus: 10240/60000 ( 17%) | Loss:  0.136054\n",
            "train epoch: 2 | batch staus: 10880/60000 ( 18%) | Loss:  0.208363\n",
            "train epoch: 2 | batch staus: 11520/60000 ( 19%) | Loss:  0.202748\n",
            "train epoch: 2 | batch staus: 12160/60000 ( 20%) | Loss:  0.159523\n",
            "train epoch: 2 | batch staus: 12800/60000 ( 21%) | Loss:  0.165904\n",
            "train epoch: 2 | batch staus: 13440/60000 ( 22%) | Loss:  0.086829\n",
            "train epoch: 2 | batch staus: 14080/60000 ( 23%) | Loss:  0.189956\n",
            "train epoch: 2 | batch staus: 14720/60000 ( 25%) | Loss:  0.079906\n",
            "train epoch: 2 | batch staus: 15360/60000 ( 26%) | Loss:  0.108762\n",
            "train epoch: 2 | batch staus: 16000/60000 ( 27%) | Loss:  0.208638\n",
            "train epoch: 2 | batch staus: 16640/60000 ( 28%) | Loss:  0.232028\n",
            "train epoch: 2 | batch staus: 17280/60000 ( 29%) | Loss:  0.225543\n",
            "train epoch: 2 | batch staus: 17920/60000 ( 30%) | Loss:  0.200964\n",
            "train epoch: 2 | batch staus: 18560/60000 ( 31%) | Loss:  0.109497\n",
            "train epoch: 2 | batch staus: 19200/60000 ( 32%) | Loss:  0.120095\n",
            "train epoch: 2 | batch staus: 19840/60000 ( 33%) | Loss:  0.069448\n",
            "train epoch: 2 | batch staus: 20480/60000 ( 34%) | Loss:  0.271325\n",
            "train epoch: 2 | batch staus: 21120/60000 ( 35%) | Loss:  0.257881\n",
            "train epoch: 2 | batch staus: 21760/60000 ( 36%) | Loss:  0.144611\n",
            "train epoch: 2 | batch staus: 22400/60000 ( 37%) | Loss:  0.138210\n",
            "train epoch: 2 | batch staus: 23040/60000 ( 38%) | Loss:  0.160420\n",
            "train epoch: 2 | batch staus: 23680/60000 ( 39%) | Loss:  0.094699\n",
            "train epoch: 2 | batch staus: 24320/60000 ( 41%) | Loss:  0.213717\n",
            "train epoch: 2 | batch staus: 24960/60000 ( 42%) | Loss:  0.042599\n",
            "train epoch: 2 | batch staus: 25600/60000 ( 43%) | Loss:  0.199009\n",
            "train epoch: 2 | batch staus: 26240/60000 ( 44%) | Loss:  0.270299\n",
            "train epoch: 2 | batch staus: 26880/60000 ( 45%) | Loss:  0.167970\n",
            "train epoch: 2 | batch staus: 27520/60000 ( 46%) | Loss:  0.367282\n",
            "train epoch: 2 | batch staus: 28160/60000 ( 47%) | Loss:  0.095380\n",
            "train epoch: 2 | batch staus: 28800/60000 ( 48%) | Loss:  0.275187\n",
            "train epoch: 2 | batch staus: 29440/60000 ( 49%) | Loss:  0.160278\n",
            "train epoch: 2 | batch staus: 30080/60000 ( 50%) | Loss:  0.098305\n",
            "train epoch: 2 | batch staus: 30720/60000 ( 51%) | Loss:  0.072583\n",
            "train epoch: 2 | batch staus: 31360/60000 ( 52%) | Loss:  0.118575\n",
            "train epoch: 2 | batch staus: 32000/60000 ( 53%) | Loss:  0.131364\n",
            "train epoch: 2 | batch staus: 32640/60000 ( 54%) | Loss:  0.129432\n",
            "train epoch: 2 | batch staus: 33280/60000 ( 55%) | Loss:  0.078200\n",
            "train epoch: 2 | batch staus: 33920/60000 ( 57%) | Loss:  0.135769\n",
            "train epoch: 2 | batch staus: 34560/60000 ( 58%) | Loss:  0.159112\n",
            "train epoch: 2 | batch staus: 35200/60000 ( 59%) | Loss:  0.082271\n",
            "train epoch: 2 | batch staus: 35840/60000 ( 60%) | Loss:  0.314734\n",
            "train epoch: 2 | batch staus: 36480/60000 ( 61%) | Loss:  0.109307\n",
            "train epoch: 2 | batch staus: 37120/60000 ( 62%) | Loss:  0.160231\n",
            "train epoch: 2 | batch staus: 37760/60000 ( 63%) | Loss:  0.106860\n",
            "train epoch: 2 | batch staus: 38400/60000 ( 64%) | Loss:  0.220649\n",
            "train epoch: 2 | batch staus: 39040/60000 ( 65%) | Loss:  0.128053\n",
            "train epoch: 2 | batch staus: 39680/60000 ( 66%) | Loss:  0.048406\n",
            "train epoch: 2 | batch staus: 40320/60000 ( 67%) | Loss:  0.140178\n",
            "train epoch: 2 | batch staus: 40960/60000 ( 68%) | Loss:  0.109052\n",
            "train epoch: 2 | batch staus: 41600/60000 ( 69%) | Loss:  0.331609\n",
            "train epoch: 2 | batch staus: 42240/60000 ( 70%) | Loss:  0.062124\n",
            "train epoch: 2 | batch staus: 42880/60000 ( 71%) | Loss:  0.211324\n",
            "train epoch: 2 | batch staus: 43520/60000 ( 72%) | Loss:  0.085963\n",
            "train epoch: 2 | batch staus: 44160/60000 ( 74%) | Loss:  0.136642\n",
            "train epoch: 2 | batch staus: 44800/60000 ( 75%) | Loss:  0.166947\n",
            "train epoch: 2 | batch staus: 45440/60000 ( 76%) | Loss:  0.247072\n",
            "train epoch: 2 | batch staus: 46080/60000 ( 77%) | Loss:  0.074305\n",
            "train epoch: 2 | batch staus: 46720/60000 ( 78%) | Loss:  0.100290\n",
            "train epoch: 2 | batch staus: 47360/60000 ( 79%) | Loss:  0.282142\n",
            "train epoch: 2 | batch staus: 48000/60000 ( 80%) | Loss:  0.119948\n",
            "train epoch: 2 | batch staus: 48640/60000 ( 81%) | Loss:  0.063947\n",
            "train epoch: 2 | batch staus: 49280/60000 ( 82%) | Loss:  0.245281\n",
            "train epoch: 2 | batch staus: 49920/60000 ( 83%) | Loss:  0.149704\n",
            "train epoch: 2 | batch staus: 50560/60000 ( 84%) | Loss:  0.185801\n",
            "train epoch: 2 | batch staus: 51200/60000 ( 85%) | Loss:  0.193805\n",
            "train epoch: 2 | batch staus: 51840/60000 ( 86%) | Loss:  0.123286\n",
            "train epoch: 2 | batch staus: 52480/60000 ( 87%) | Loss:  0.195225\n",
            "train epoch: 2 | batch staus: 53120/60000 ( 88%) | Loss:  0.112129\n",
            "train epoch: 2 | batch staus: 53760/60000 ( 90%) | Loss:  0.146132\n",
            "train epoch: 2 | batch staus: 54400/60000 ( 91%) | Loss:  0.114810\n",
            "train epoch: 2 | batch staus: 55040/60000 ( 92%) | Loss:  0.177271\n",
            "train epoch: 2 | batch staus: 55680/60000 ( 93%) | Loss:  0.219153\n",
            "train epoch: 2 | batch staus: 56320/60000 ( 94%) | Loss:  0.046143\n",
            "train epoch: 2 | batch staus: 56960/60000 ( 95%) | Loss:  0.149699\n",
            "train epoch: 2 | batch staus: 57600/60000 ( 96%) | Loss:  0.182481\n",
            "train epoch: 2 | batch staus: 58240/60000 ( 97%) | Loss:  0.132505\n",
            "train epoch: 2 | batch staus: 58880/60000 ( 98%) | Loss:  0.131532\n",
            "train epoch: 2 | batch staus: 59520/60000 ( 99%) | Loss:  0.141061\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0018, Accuracy: 9638/10000(96%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 3 | batch staus: 0/60000 ( 0%) | Loss:  0.223464\n",
            "train epoch: 3 | batch staus: 640/60000 ( 1%) | Loss:  0.087289\n",
            "train epoch: 3 | batch staus: 1280/60000 ( 2%) | Loss:  0.125161\n",
            "train epoch: 3 | batch staus: 1920/60000 ( 3%) | Loss:  0.059634\n",
            "train epoch: 3 | batch staus: 2560/60000 ( 4%) | Loss:  0.133865\n",
            "train epoch: 3 | batch staus: 3200/60000 ( 5%) | Loss:  0.028595\n",
            "train epoch: 3 | batch staus: 3840/60000 ( 6%) | Loss:  0.066545\n",
            "train epoch: 3 | batch staus: 4480/60000 ( 7%) | Loss:  0.282889\n",
            "train epoch: 3 | batch staus: 5120/60000 ( 9%) | Loss:  0.039581\n",
            "train epoch: 3 | batch staus: 5760/60000 ( 10%) | Loss:  0.064939\n",
            "train epoch: 3 | batch staus: 6400/60000 ( 11%) | Loss:  0.146967\n",
            "train epoch: 3 | batch staus: 7040/60000 ( 12%) | Loss:  0.150678\n",
            "train epoch: 3 | batch staus: 7680/60000 ( 13%) | Loss:  0.103767\n",
            "train epoch: 3 | batch staus: 8320/60000 ( 14%) | Loss:  0.212999\n",
            "train epoch: 3 | batch staus: 8960/60000 ( 15%) | Loss:  0.085977\n",
            "train epoch: 3 | batch staus: 9600/60000 ( 16%) | Loss:  0.137181\n",
            "train epoch: 3 | batch staus: 10240/60000 ( 17%) | Loss:  0.154920\n",
            "train epoch: 3 | batch staus: 10880/60000 ( 18%) | Loss:  0.115189\n",
            "train epoch: 3 | batch staus: 11520/60000 ( 19%) | Loss:  0.158200\n",
            "train epoch: 3 | batch staus: 12160/60000 ( 20%) | Loss:  0.096644\n",
            "train epoch: 3 | batch staus: 12800/60000 ( 21%) | Loss:  0.076326\n",
            "train epoch: 3 | batch staus: 13440/60000 ( 22%) | Loss:  0.214066\n",
            "train epoch: 3 | batch staus: 14080/60000 ( 23%) | Loss:  0.138591\n",
            "train epoch: 3 | batch staus: 14720/60000 ( 25%) | Loss:  0.223601\n",
            "train epoch: 3 | batch staus: 15360/60000 ( 26%) | Loss:  0.127358\n",
            "train epoch: 3 | batch staus: 16000/60000 ( 27%) | Loss:  0.128364\n",
            "train epoch: 3 | batch staus: 16640/60000 ( 28%) | Loss:  0.113763\n",
            "train epoch: 3 | batch staus: 17280/60000 ( 29%) | Loss:  0.115728\n",
            "train epoch: 3 | batch staus: 17920/60000 ( 30%) | Loss:  0.047991\n",
            "train epoch: 3 | batch staus: 18560/60000 ( 31%) | Loss:  0.103151\n",
            "train epoch: 3 | batch staus: 19200/60000 ( 32%) | Loss:  0.103831\n",
            "train epoch: 3 | batch staus: 19840/60000 ( 33%) | Loss:  0.234808\n",
            "train epoch: 3 | batch staus: 20480/60000 ( 34%) | Loss:  0.057378\n",
            "train epoch: 3 | batch staus: 21120/60000 ( 35%) | Loss:  0.128996\n",
            "train epoch: 3 | batch staus: 21760/60000 ( 36%) | Loss:  0.159856\n",
            "train epoch: 3 | batch staus: 22400/60000 ( 37%) | Loss:  0.084241\n",
            "train epoch: 3 | batch staus: 23040/60000 ( 38%) | Loss:  0.189904\n",
            "train epoch: 3 | batch staus: 23680/60000 ( 39%) | Loss:  0.159950\n",
            "train epoch: 3 | batch staus: 24320/60000 ( 41%) | Loss:  0.115597\n",
            "train epoch: 3 | batch staus: 24960/60000 ( 42%) | Loss:  0.165410\n",
            "train epoch: 3 | batch staus: 25600/60000 ( 43%) | Loss:  0.073129\n",
            "train epoch: 3 | batch staus: 26240/60000 ( 44%) | Loss:  0.093376\n",
            "train epoch: 3 | batch staus: 26880/60000 ( 45%) | Loss:  0.140534\n",
            "train epoch: 3 | batch staus: 27520/60000 ( 46%) | Loss:  0.197286\n",
            "train epoch: 3 | batch staus: 28160/60000 ( 47%) | Loss:  0.142923\n",
            "train epoch: 3 | batch staus: 28800/60000 ( 48%) | Loss:  0.041270\n",
            "train epoch: 3 | batch staus: 29440/60000 ( 49%) | Loss:  0.039305\n",
            "train epoch: 3 | batch staus: 30080/60000 ( 50%) | Loss:  0.146733\n",
            "train epoch: 3 | batch staus: 30720/60000 ( 51%) | Loss:  0.061641\n",
            "train epoch: 3 | batch staus: 31360/60000 ( 52%) | Loss:  0.102768\n",
            "train epoch: 3 | batch staus: 32000/60000 ( 53%) | Loss:  0.143730\n",
            "train epoch: 3 | batch staus: 32640/60000 ( 54%) | Loss:  0.124922\n",
            "train epoch: 3 | batch staus: 33280/60000 ( 55%) | Loss:  0.055696\n",
            "train epoch: 3 | batch staus: 33920/60000 ( 57%) | Loss:  0.130888\n",
            "train epoch: 3 | batch staus: 34560/60000 ( 58%) | Loss:  0.147221\n",
            "train epoch: 3 | batch staus: 35200/60000 ( 59%) | Loss:  0.058695\n",
            "train epoch: 3 | batch staus: 35840/60000 ( 60%) | Loss:  0.288293\n",
            "train epoch: 3 | batch staus: 36480/60000 ( 61%) | Loss:  0.134296\n",
            "train epoch: 3 | batch staus: 37120/60000 ( 62%) | Loss:  0.202389\n",
            "train epoch: 3 | batch staus: 37760/60000 ( 63%) | Loss:  0.090337\n",
            "train epoch: 3 | batch staus: 38400/60000 ( 64%) | Loss:  0.219180\n",
            "train epoch: 3 | batch staus: 39040/60000 ( 65%) | Loss:  0.117903\n",
            "train epoch: 3 | batch staus: 39680/60000 ( 66%) | Loss:  0.130866\n",
            "train epoch: 3 | batch staus: 40320/60000 ( 67%) | Loss:  0.178397\n",
            "train epoch: 3 | batch staus: 40960/60000 ( 68%) | Loss:  0.388102\n",
            "train epoch: 3 | batch staus: 41600/60000 ( 69%) | Loss:  0.311587\n",
            "train epoch: 3 | batch staus: 42240/60000 ( 70%) | Loss:  0.090483\n",
            "train epoch: 3 | batch staus: 42880/60000 ( 71%) | Loss:  0.102250\n",
            "train epoch: 3 | batch staus: 43520/60000 ( 72%) | Loss:  0.099566\n",
            "train epoch: 3 | batch staus: 44160/60000 ( 74%) | Loss:  0.213848\n",
            "train epoch: 3 | batch staus: 44800/60000 ( 75%) | Loss:  0.064540\n",
            "train epoch: 3 | batch staus: 45440/60000 ( 76%) | Loss:  0.156691\n",
            "train epoch: 3 | batch staus: 46080/60000 ( 77%) | Loss:  0.068737\n",
            "train epoch: 3 | batch staus: 46720/60000 ( 78%) | Loss:  0.113316\n",
            "train epoch: 3 | batch staus: 47360/60000 ( 79%) | Loss:  0.119615\n",
            "train epoch: 3 | batch staus: 48000/60000 ( 80%) | Loss:  0.057527\n",
            "train epoch: 3 | batch staus: 48640/60000 ( 81%) | Loss:  0.205934\n",
            "train epoch: 3 | batch staus: 49280/60000 ( 82%) | Loss:  0.127334\n",
            "train epoch: 3 | batch staus: 49920/60000 ( 83%) | Loss:  0.040691\n",
            "train epoch: 3 | batch staus: 50560/60000 ( 84%) | Loss:  0.066701\n",
            "train epoch: 3 | batch staus: 51200/60000 ( 85%) | Loss:  0.138220\n",
            "train epoch: 3 | batch staus: 51840/60000 ( 86%) | Loss:  0.046791\n",
            "train epoch: 3 | batch staus: 52480/60000 ( 87%) | Loss:  0.035760\n",
            "train epoch: 3 | batch staus: 53120/60000 ( 88%) | Loss:  0.086862\n",
            "train epoch: 3 | batch staus: 53760/60000 ( 90%) | Loss:  0.241581\n",
            "train epoch: 3 | batch staus: 54400/60000 ( 91%) | Loss:  0.023898\n",
            "train epoch: 3 | batch staus: 55040/60000 ( 92%) | Loss:  0.091678\n",
            "train epoch: 3 | batch staus: 55680/60000 ( 93%) | Loss:  0.048862\n",
            "train epoch: 3 | batch staus: 56320/60000 ( 94%) | Loss:  0.135528\n",
            "train epoch: 3 | batch staus: 56960/60000 ( 95%) | Loss:  0.072587\n",
            "train epoch: 3 | batch staus: 57600/60000 ( 96%) | Loss:  0.063939\n",
            "train epoch: 3 | batch staus: 58240/60000 ( 97%) | Loss:  0.075851\n",
            "train epoch: 3 | batch staus: 58880/60000 ( 98%) | Loss:  0.202556\n",
            "train epoch: 3 | batch staus: 59520/60000 ( 99%) | Loss:  0.154040\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0014, Accuracy: 9717/10000(97%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 4 | batch staus: 0/60000 ( 0%) | Loss:  0.051748\n",
            "train epoch: 4 | batch staus: 640/60000 ( 1%) | Loss:  0.038775\n",
            "train epoch: 4 | batch staus: 1280/60000 ( 2%) | Loss:  0.041008\n",
            "train epoch: 4 | batch staus: 1920/60000 ( 3%) | Loss:  0.072037\n",
            "train epoch: 4 | batch staus: 2560/60000 ( 4%) | Loss:  0.137321\n",
            "train epoch: 4 | batch staus: 3200/60000 ( 5%) | Loss:  0.097858\n",
            "train epoch: 4 | batch staus: 3840/60000 ( 6%) | Loss:  0.031275\n",
            "train epoch: 4 | batch staus: 4480/60000 ( 7%) | Loss:  0.115914\n",
            "train epoch: 4 | batch staus: 5120/60000 ( 9%) | Loss:  0.051564\n",
            "train epoch: 4 | batch staus: 5760/60000 ( 10%) | Loss:  0.068957\n",
            "train epoch: 4 | batch staus: 6400/60000 ( 11%) | Loss:  0.082935\n",
            "train epoch: 4 | batch staus: 7040/60000 ( 12%) | Loss:  0.124839\n",
            "train epoch: 4 | batch staus: 7680/60000 ( 13%) | Loss:  0.053094\n",
            "train epoch: 4 | batch staus: 8320/60000 ( 14%) | Loss:  0.073412\n",
            "train epoch: 4 | batch staus: 8960/60000 ( 15%) | Loss:  0.038973\n",
            "train epoch: 4 | batch staus: 9600/60000 ( 16%) | Loss:  0.043720\n",
            "train epoch: 4 | batch staus: 10240/60000 ( 17%) | Loss:  0.117792\n",
            "train epoch: 4 | batch staus: 10880/60000 ( 18%) | Loss:  0.126904\n",
            "train epoch: 4 | batch staus: 11520/60000 ( 19%) | Loss:  0.044421\n",
            "train epoch: 4 | batch staus: 12160/60000 ( 20%) | Loss:  0.031151\n",
            "train epoch: 4 | batch staus: 12800/60000 ( 21%) | Loss:  0.045344\n",
            "train epoch: 4 | batch staus: 13440/60000 ( 22%) | Loss:  0.114641\n",
            "train epoch: 4 | batch staus: 14080/60000 ( 23%) | Loss:  0.060288\n",
            "train epoch: 4 | batch staus: 14720/60000 ( 25%) | Loss:  0.058366\n",
            "train epoch: 4 | batch staus: 15360/60000 ( 26%) | Loss:  0.089782\n",
            "train epoch: 4 | batch staus: 16000/60000 ( 27%) | Loss:  0.059767\n",
            "train epoch: 4 | batch staus: 16640/60000 ( 28%) | Loss:  0.070846\n",
            "train epoch: 4 | batch staus: 17280/60000 ( 29%) | Loss:  0.112376\n",
            "train epoch: 4 | batch staus: 17920/60000 ( 30%) | Loss:  0.169226\n",
            "train epoch: 4 | batch staus: 18560/60000 ( 31%) | Loss:  0.125514\n",
            "train epoch: 4 | batch staus: 19200/60000 ( 32%) | Loss:  0.185802\n",
            "train epoch: 4 | batch staus: 19840/60000 ( 33%) | Loss:  0.061001\n",
            "train epoch: 4 | batch staus: 20480/60000 ( 34%) | Loss:  0.094556\n",
            "train epoch: 4 | batch staus: 21120/60000 ( 35%) | Loss:  0.089965\n",
            "train epoch: 4 | batch staus: 21760/60000 ( 36%) | Loss:  0.158720\n",
            "train epoch: 4 | batch staus: 22400/60000 ( 37%) | Loss:  0.022259\n",
            "train epoch: 4 | batch staus: 23040/60000 ( 38%) | Loss:  0.037450\n",
            "train epoch: 4 | batch staus: 23680/60000 ( 39%) | Loss:  0.049472\n",
            "train epoch: 4 | batch staus: 24320/60000 ( 41%) | Loss:  0.156592\n",
            "train epoch: 4 | batch staus: 24960/60000 ( 42%) | Loss:  0.050667\n",
            "train epoch: 4 | batch staus: 25600/60000 ( 43%) | Loss:  0.028351\n",
            "train epoch: 4 | batch staus: 26240/60000 ( 44%) | Loss:  0.359911\n",
            "train epoch: 4 | batch staus: 26880/60000 ( 45%) | Loss:  0.128274\n",
            "train epoch: 4 | batch staus: 27520/60000 ( 46%) | Loss:  0.051518\n",
            "train epoch: 4 | batch staus: 28160/60000 ( 47%) | Loss:  0.036378\n",
            "train epoch: 4 | batch staus: 28800/60000 ( 48%) | Loss:  0.146212\n",
            "train epoch: 4 | batch staus: 29440/60000 ( 49%) | Loss:  0.224224\n",
            "train epoch: 4 | batch staus: 30080/60000 ( 50%) | Loss:  0.114489\n",
            "train epoch: 4 | batch staus: 30720/60000 ( 51%) | Loss:  0.130347\n",
            "train epoch: 4 | batch staus: 31360/60000 ( 52%) | Loss:  0.107150\n",
            "train epoch: 4 | batch staus: 32000/60000 ( 53%) | Loss:  0.047139\n",
            "train epoch: 4 | batch staus: 32640/60000 ( 54%) | Loss:  0.085801\n",
            "train epoch: 4 | batch staus: 33280/60000 ( 55%) | Loss:  0.100585\n",
            "train epoch: 4 | batch staus: 33920/60000 ( 57%) | Loss:  0.079820\n",
            "train epoch: 4 | batch staus: 34560/60000 ( 58%) | Loss:  0.048582\n",
            "train epoch: 4 | batch staus: 35200/60000 ( 59%) | Loss:  0.091540\n",
            "train epoch: 4 | batch staus: 35840/60000 ( 60%) | Loss:  0.076266\n",
            "train epoch: 4 | batch staus: 36480/60000 ( 61%) | Loss:  0.119581\n",
            "train epoch: 4 | batch staus: 37120/60000 ( 62%) | Loss:  0.085021\n",
            "train epoch: 4 | batch staus: 37760/60000 ( 63%) | Loss:  0.211633\n",
            "train epoch: 4 | batch staus: 38400/60000 ( 64%) | Loss:  0.083675\n",
            "train epoch: 4 | batch staus: 39040/60000 ( 65%) | Loss:  0.109177\n",
            "train epoch: 4 | batch staus: 39680/60000 ( 66%) | Loss:  0.183270\n",
            "train epoch: 4 | batch staus: 40320/60000 ( 67%) | Loss:  0.101629\n",
            "train epoch: 4 | batch staus: 40960/60000 ( 68%) | Loss:  0.037914\n",
            "train epoch: 4 | batch staus: 41600/60000 ( 69%) | Loss:  0.148737\n",
            "train epoch: 4 | batch staus: 42240/60000 ( 70%) | Loss:  0.033262\n",
            "train epoch: 4 | batch staus: 42880/60000 ( 71%) | Loss:  0.081451\n",
            "train epoch: 4 | batch staus: 43520/60000 ( 72%) | Loss:  0.070496\n",
            "train epoch: 4 | batch staus: 44160/60000 ( 74%) | Loss:  0.071521\n",
            "train epoch: 4 | batch staus: 44800/60000 ( 75%) | Loss:  0.051947\n",
            "train epoch: 4 | batch staus: 45440/60000 ( 76%) | Loss:  0.068538\n",
            "train epoch: 4 | batch staus: 46080/60000 ( 77%) | Loss:  0.046990\n",
            "train epoch: 4 | batch staus: 46720/60000 ( 78%) | Loss:  0.130938\n",
            "train epoch: 4 | batch staus: 47360/60000 ( 79%) | Loss:  0.117954\n",
            "train epoch: 4 | batch staus: 48000/60000 ( 80%) | Loss:  0.130520\n",
            "train epoch: 4 | batch staus: 48640/60000 ( 81%) | Loss:  0.042090\n",
            "train epoch: 4 | batch staus: 49280/60000 ( 82%) | Loss:  0.069922\n",
            "train epoch: 4 | batch staus: 49920/60000 ( 83%) | Loss:  0.127361\n",
            "train epoch: 4 | batch staus: 50560/60000 ( 84%) | Loss:  0.026427\n",
            "train epoch: 4 | batch staus: 51200/60000 ( 85%) | Loss:  0.202441\n",
            "train epoch: 4 | batch staus: 51840/60000 ( 86%) | Loss:  0.049847\n",
            "train epoch: 4 | batch staus: 52480/60000 ( 87%) | Loss:  0.042327\n",
            "train epoch: 4 | batch staus: 53120/60000 ( 88%) | Loss:  0.127793\n",
            "train epoch: 4 | batch staus: 53760/60000 ( 90%) | Loss:  0.118582\n",
            "train epoch: 4 | batch staus: 54400/60000 ( 91%) | Loss:  0.142036\n",
            "train epoch: 4 | batch staus: 55040/60000 ( 92%) | Loss:  0.148803\n",
            "train epoch: 4 | batch staus: 55680/60000 ( 93%) | Loss:  0.084319\n",
            "train epoch: 4 | batch staus: 56320/60000 ( 94%) | Loss:  0.090393\n",
            "train epoch: 4 | batch staus: 56960/60000 ( 95%) | Loss:  0.017494\n",
            "train epoch: 4 | batch staus: 57600/60000 ( 96%) | Loss:  0.066322\n",
            "train epoch: 4 | batch staus: 58240/60000 ( 97%) | Loss:  0.076923\n",
            "train epoch: 4 | batch staus: 58880/60000 ( 98%) | Loss:  0.121599\n",
            "train epoch: 4 | batch staus: 59520/60000 ( 99%) | Loss:  0.096859\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0011, Accuracy: 9763/10000(98%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 5 | batch staus: 0/60000 ( 0%) | Loss:  0.080453\n",
            "train epoch: 5 | batch staus: 640/60000 ( 1%) | Loss:  0.062804\n",
            "train epoch: 5 | batch staus: 1280/60000 ( 2%) | Loss:  0.027761\n",
            "train epoch: 5 | batch staus: 1920/60000 ( 3%) | Loss:  0.043478\n",
            "train epoch: 5 | batch staus: 2560/60000 ( 4%) | Loss:  0.080787\n",
            "train epoch: 5 | batch staus: 3200/60000 ( 5%) | Loss:  0.049269\n",
            "train epoch: 5 | batch staus: 3840/60000 ( 6%) | Loss:  0.104194\n",
            "train epoch: 5 | batch staus: 4480/60000 ( 7%) | Loss:  0.101748\n",
            "train epoch: 5 | batch staus: 5120/60000 ( 9%) | Loss:  0.034168\n",
            "train epoch: 5 | batch staus: 5760/60000 ( 10%) | Loss:  0.145308\n",
            "train epoch: 5 | batch staus: 6400/60000 ( 11%) | Loss:  0.103169\n",
            "train epoch: 5 | batch staus: 7040/60000 ( 12%) | Loss:  0.012402\n",
            "train epoch: 5 | batch staus: 7680/60000 ( 13%) | Loss:  0.123140\n",
            "train epoch: 5 | batch staus: 8320/60000 ( 14%) | Loss:  0.112874\n",
            "train epoch: 5 | batch staus: 8960/60000 ( 15%) | Loss:  0.029084\n",
            "train epoch: 5 | batch staus: 9600/60000 ( 16%) | Loss:  0.194331\n",
            "train epoch: 5 | batch staus: 10240/60000 ( 17%) | Loss:  0.112812\n",
            "train epoch: 5 | batch staus: 10880/60000 ( 18%) | Loss:  0.095558\n",
            "train epoch: 5 | batch staus: 11520/60000 ( 19%) | Loss:  0.061263\n",
            "train epoch: 5 | batch staus: 12160/60000 ( 20%) | Loss:  0.139354\n",
            "train epoch: 5 | batch staus: 12800/60000 ( 21%) | Loss:  0.059539\n",
            "train epoch: 5 | batch staus: 13440/60000 ( 22%) | Loss:  0.143749\n",
            "train epoch: 5 | batch staus: 14080/60000 ( 23%) | Loss:  0.201257\n",
            "train epoch: 5 | batch staus: 14720/60000 ( 25%) | Loss:  0.040287\n",
            "train epoch: 5 | batch staus: 15360/60000 ( 26%) | Loss:  0.020956\n",
            "train epoch: 5 | batch staus: 16000/60000 ( 27%) | Loss:  0.114143\n",
            "train epoch: 5 | batch staus: 16640/60000 ( 28%) | Loss:  0.091440\n",
            "train epoch: 5 | batch staus: 17280/60000 ( 29%) | Loss:  0.117903\n",
            "train epoch: 5 | batch staus: 17920/60000 ( 30%) | Loss:  0.087229\n",
            "train epoch: 5 | batch staus: 18560/60000 ( 31%) | Loss:  0.099519\n",
            "train epoch: 5 | batch staus: 19200/60000 ( 32%) | Loss:  0.151277\n",
            "train epoch: 5 | batch staus: 19840/60000 ( 33%) | Loss:  0.098186\n",
            "train epoch: 5 | batch staus: 20480/60000 ( 34%) | Loss:  0.121264\n",
            "train epoch: 5 | batch staus: 21120/60000 ( 35%) | Loss:  0.071958\n",
            "train epoch: 5 | batch staus: 21760/60000 ( 36%) | Loss:  0.070628\n",
            "train epoch: 5 | batch staus: 22400/60000 ( 37%) | Loss:  0.052788\n",
            "train epoch: 5 | batch staus: 23040/60000 ( 38%) | Loss:  0.190633\n",
            "train epoch: 5 | batch staus: 23680/60000 ( 39%) | Loss:  0.052440\n",
            "train epoch: 5 | batch staus: 24320/60000 ( 41%) | Loss:  0.019193\n",
            "train epoch: 5 | batch staus: 24960/60000 ( 42%) | Loss:  0.240621\n",
            "train epoch: 5 | batch staus: 25600/60000 ( 43%) | Loss:  0.232312\n",
            "train epoch: 5 | batch staus: 26240/60000 ( 44%) | Loss:  0.092312\n",
            "train epoch: 5 | batch staus: 26880/60000 ( 45%) | Loss:  0.016583\n",
            "train epoch: 5 | batch staus: 27520/60000 ( 46%) | Loss:  0.060566\n",
            "train epoch: 5 | batch staus: 28160/60000 ( 47%) | Loss:  0.064530\n",
            "train epoch: 5 | batch staus: 28800/60000 ( 48%) | Loss:  0.137980\n",
            "train epoch: 5 | batch staus: 29440/60000 ( 49%) | Loss:  0.102188\n",
            "train epoch: 5 | batch staus: 30080/60000 ( 50%) | Loss:  0.225637\n",
            "train epoch: 5 | batch staus: 30720/60000 ( 51%) | Loss:  0.018506\n",
            "train epoch: 5 | batch staus: 31360/60000 ( 52%) | Loss:  0.056817\n",
            "train epoch: 5 | batch staus: 32000/60000 ( 53%) | Loss:  0.086546\n",
            "train epoch: 5 | batch staus: 32640/60000 ( 54%) | Loss:  0.020279\n",
            "train epoch: 5 | batch staus: 33280/60000 ( 55%) | Loss:  0.089594\n",
            "train epoch: 5 | batch staus: 33920/60000 ( 57%) | Loss:  0.077736\n",
            "train epoch: 5 | batch staus: 34560/60000 ( 58%) | Loss:  0.045594\n",
            "train epoch: 5 | batch staus: 35200/60000 ( 59%) | Loss:  0.055792\n",
            "train epoch: 5 | batch staus: 35840/60000 ( 60%) | Loss:  0.049817\n",
            "train epoch: 5 | batch staus: 36480/60000 ( 61%) | Loss:  0.014364\n",
            "train epoch: 5 | batch staus: 37120/60000 ( 62%) | Loss:  0.067950\n",
            "train epoch: 5 | batch staus: 37760/60000 ( 63%) | Loss:  0.111556\n",
            "train epoch: 5 | batch staus: 38400/60000 ( 64%) | Loss:  0.041851\n",
            "train epoch: 5 | batch staus: 39040/60000 ( 65%) | Loss:  0.125144\n",
            "train epoch: 5 | batch staus: 39680/60000 ( 66%) | Loss:  0.079497\n",
            "train epoch: 5 | batch staus: 40320/60000 ( 67%) | Loss:  0.046999\n",
            "train epoch: 5 | batch staus: 40960/60000 ( 68%) | Loss:  0.078831\n",
            "train epoch: 5 | batch staus: 41600/60000 ( 69%) | Loss:  0.137098\n",
            "train epoch: 5 | batch staus: 42240/60000 ( 70%) | Loss:  0.147139\n",
            "train epoch: 5 | batch staus: 42880/60000 ( 71%) | Loss:  0.034881\n",
            "train epoch: 5 | batch staus: 43520/60000 ( 72%) | Loss:  0.096147\n",
            "train epoch: 5 | batch staus: 44160/60000 ( 74%) | Loss:  0.084768\n",
            "train epoch: 5 | batch staus: 44800/60000 ( 75%) | Loss:  0.021559\n",
            "train epoch: 5 | batch staus: 45440/60000 ( 76%) | Loss:  0.024112\n",
            "train epoch: 5 | batch staus: 46080/60000 ( 77%) | Loss:  0.018881\n",
            "train epoch: 5 | batch staus: 46720/60000 ( 78%) | Loss:  0.107640\n",
            "train epoch: 5 | batch staus: 47360/60000 ( 79%) | Loss:  0.128418\n",
            "train epoch: 5 | batch staus: 48000/60000 ( 80%) | Loss:  0.053680\n",
            "train epoch: 5 | batch staus: 48640/60000 ( 81%) | Loss:  0.084344\n",
            "train epoch: 5 | batch staus: 49280/60000 ( 82%) | Loss:  0.152429\n",
            "train epoch: 5 | batch staus: 49920/60000 ( 83%) | Loss:  0.033490\n",
            "train epoch: 5 | batch staus: 50560/60000 ( 84%) | Loss:  0.077084\n",
            "train epoch: 5 | batch staus: 51200/60000 ( 85%) | Loss:  0.106398\n",
            "train epoch: 5 | batch staus: 51840/60000 ( 86%) | Loss:  0.037143\n",
            "train epoch: 5 | batch staus: 52480/60000 ( 87%) | Loss:  0.041973\n",
            "train epoch: 5 | batch staus: 53120/60000 ( 88%) | Loss:  0.046341\n",
            "train epoch: 5 | batch staus: 53760/60000 ( 90%) | Loss:  0.040181\n",
            "train epoch: 5 | batch staus: 54400/60000 ( 91%) | Loss:  0.011827\n",
            "train epoch: 5 | batch staus: 55040/60000 ( 92%) | Loss:  0.084419\n",
            "train epoch: 5 | batch staus: 55680/60000 ( 93%) | Loss:  0.074396\n",
            "train epoch: 5 | batch staus: 56320/60000 ( 94%) | Loss:  0.101211\n",
            "train epoch: 5 | batch staus: 56960/60000 ( 95%) | Loss:  0.047752\n",
            "train epoch: 5 | batch staus: 57600/60000 ( 96%) | Loss:  0.042188\n",
            "train epoch: 5 | batch staus: 58240/60000 ( 97%) | Loss:  0.135142\n",
            "train epoch: 5 | batch staus: 58880/60000 ( 98%) | Loss:  0.084750\n",
            "train epoch: 5 | batch staus: 59520/60000 ( 99%) | Loss:  0.078312\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0015, Accuracy: 9702/10000(97%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 6 | batch staus: 0/60000 ( 0%) | Loss:  0.137103\n",
            "train epoch: 6 | batch staus: 640/60000 ( 1%) | Loss:  0.009037\n",
            "train epoch: 6 | batch staus: 1280/60000 ( 2%) | Loss:  0.118997\n",
            "train epoch: 6 | batch staus: 1920/60000 ( 3%) | Loss:  0.018655\n",
            "train epoch: 6 | batch staus: 2560/60000 ( 4%) | Loss:  0.006980\n",
            "train epoch: 6 | batch staus: 3200/60000 ( 5%) | Loss:  0.120705\n",
            "train epoch: 6 | batch staus: 3840/60000 ( 6%) | Loss:  0.209912\n",
            "train epoch: 6 | batch staus: 4480/60000 ( 7%) | Loss:  0.024724\n",
            "train epoch: 6 | batch staus: 5120/60000 ( 9%) | Loss:  0.107188\n",
            "train epoch: 6 | batch staus: 5760/60000 ( 10%) | Loss:  0.086619\n",
            "train epoch: 6 | batch staus: 6400/60000 ( 11%) | Loss:  0.076342\n",
            "train epoch: 6 | batch staus: 7040/60000 ( 12%) | Loss:  0.175080\n",
            "train epoch: 6 | batch staus: 7680/60000 ( 13%) | Loss:  0.082339\n",
            "train epoch: 6 | batch staus: 8320/60000 ( 14%) | Loss:  0.182898\n",
            "train epoch: 6 | batch staus: 8960/60000 ( 15%) | Loss:  0.068459\n",
            "train epoch: 6 | batch staus: 9600/60000 ( 16%) | Loss:  0.061027\n",
            "train epoch: 6 | batch staus: 10240/60000 ( 17%) | Loss:  0.033234\n",
            "train epoch: 6 | batch staus: 10880/60000 ( 18%) | Loss:  0.159579\n",
            "train epoch: 6 | batch staus: 11520/60000 ( 19%) | Loss:  0.059285\n",
            "train epoch: 6 | batch staus: 12160/60000 ( 20%) | Loss:  0.096934\n",
            "train epoch: 6 | batch staus: 12800/60000 ( 21%) | Loss:  0.079728\n",
            "train epoch: 6 | batch staus: 13440/60000 ( 22%) | Loss:  0.051823\n",
            "train epoch: 6 | batch staus: 14080/60000 ( 23%) | Loss:  0.112597\n",
            "train epoch: 6 | batch staus: 14720/60000 ( 25%) | Loss:  0.079670\n",
            "train epoch: 6 | batch staus: 15360/60000 ( 26%) | Loss:  0.032804\n",
            "train epoch: 6 | batch staus: 16000/60000 ( 27%) | Loss:  0.049098\n",
            "train epoch: 6 | batch staus: 16640/60000 ( 28%) | Loss:  0.071613\n",
            "train epoch: 6 | batch staus: 17280/60000 ( 29%) | Loss:  0.010399\n",
            "train epoch: 6 | batch staus: 17920/60000 ( 30%) | Loss:  0.068570\n",
            "train epoch: 6 | batch staus: 18560/60000 ( 31%) | Loss:  0.030786\n",
            "train epoch: 6 | batch staus: 19200/60000 ( 32%) | Loss:  0.119120\n",
            "train epoch: 6 | batch staus: 19840/60000 ( 33%) | Loss:  0.048198\n",
            "train epoch: 6 | batch staus: 20480/60000 ( 34%) | Loss:  0.079354\n",
            "train epoch: 6 | batch staus: 21120/60000 ( 35%) | Loss:  0.105962\n",
            "train epoch: 6 | batch staus: 21760/60000 ( 36%) | Loss:  0.089064\n",
            "train epoch: 6 | batch staus: 22400/60000 ( 37%) | Loss:  0.111456\n",
            "train epoch: 6 | batch staus: 23040/60000 ( 38%) | Loss:  0.013025\n",
            "train epoch: 6 | batch staus: 23680/60000 ( 39%) | Loss:  0.055402\n",
            "train epoch: 6 | batch staus: 24320/60000 ( 41%) | Loss:  0.037446\n",
            "train epoch: 6 | batch staus: 24960/60000 ( 42%) | Loss:  0.047672\n",
            "train epoch: 6 | batch staus: 25600/60000 ( 43%) | Loss:  0.123975\n",
            "train epoch: 6 | batch staus: 26240/60000 ( 44%) | Loss:  0.051600\n",
            "train epoch: 6 | batch staus: 26880/60000 ( 45%) | Loss:  0.137184\n",
            "train epoch: 6 | batch staus: 27520/60000 ( 46%) | Loss:  0.033341\n",
            "train epoch: 6 | batch staus: 28160/60000 ( 47%) | Loss:  0.051678\n",
            "train epoch: 6 | batch staus: 28800/60000 ( 48%) | Loss:  0.040570\n",
            "train epoch: 6 | batch staus: 29440/60000 ( 49%) | Loss:  0.088893\n",
            "train epoch: 6 | batch staus: 30080/60000 ( 50%) | Loss:  0.139249\n",
            "train epoch: 6 | batch staus: 30720/60000 ( 51%) | Loss:  0.037853\n",
            "train epoch: 6 | batch staus: 31360/60000 ( 52%) | Loss:  0.140354\n",
            "train epoch: 6 | batch staus: 32000/60000 ( 53%) | Loss:  0.027178\n",
            "train epoch: 6 | batch staus: 32640/60000 ( 54%) | Loss:  0.092912\n",
            "train epoch: 6 | batch staus: 33280/60000 ( 55%) | Loss:  0.111785\n",
            "train epoch: 6 | batch staus: 33920/60000 ( 57%) | Loss:  0.106523\n",
            "train epoch: 6 | batch staus: 34560/60000 ( 58%) | Loss:  0.020531\n",
            "train epoch: 6 | batch staus: 35200/60000 ( 59%) | Loss:  0.271078\n",
            "train epoch: 6 | batch staus: 35840/60000 ( 60%) | Loss:  0.050106\n",
            "train epoch: 6 | batch staus: 36480/60000 ( 61%) | Loss:  0.083910\n",
            "train epoch: 6 | batch staus: 37120/60000 ( 62%) | Loss:  0.134330\n",
            "train epoch: 6 | batch staus: 37760/60000 ( 63%) | Loss:  0.020442\n",
            "train epoch: 6 | batch staus: 38400/60000 ( 64%) | Loss:  0.024026\n",
            "train epoch: 6 | batch staus: 39040/60000 ( 65%) | Loss:  0.062407\n",
            "train epoch: 6 | batch staus: 39680/60000 ( 66%) | Loss:  0.107089\n",
            "train epoch: 6 | batch staus: 40320/60000 ( 67%) | Loss:  0.043989\n",
            "train epoch: 6 | batch staus: 40960/60000 ( 68%) | Loss:  0.030098\n",
            "train epoch: 6 | batch staus: 41600/60000 ( 69%) | Loss:  0.039885\n",
            "train epoch: 6 | batch staus: 42240/60000 ( 70%) | Loss:  0.024588\n",
            "train epoch: 6 | batch staus: 42880/60000 ( 71%) | Loss:  0.057397\n",
            "train epoch: 6 | batch staus: 43520/60000 ( 72%) | Loss:  0.035290\n",
            "train epoch: 6 | batch staus: 44160/60000 ( 74%) | Loss:  0.057152\n",
            "train epoch: 6 | batch staus: 44800/60000 ( 75%) | Loss:  0.050258\n",
            "train epoch: 6 | batch staus: 45440/60000 ( 76%) | Loss:  0.008816\n",
            "train epoch: 6 | batch staus: 46080/60000 ( 77%) | Loss:  0.084997\n",
            "train epoch: 6 | batch staus: 46720/60000 ( 78%) | Loss:  0.054775\n",
            "train epoch: 6 | batch staus: 47360/60000 ( 79%) | Loss:  0.049507\n",
            "train epoch: 6 | batch staus: 48000/60000 ( 80%) | Loss:  0.187398\n",
            "train epoch: 6 | batch staus: 48640/60000 ( 81%) | Loss:  0.008869\n",
            "train epoch: 6 | batch staus: 49280/60000 ( 82%) | Loss:  0.036465\n",
            "train epoch: 6 | batch staus: 49920/60000 ( 83%) | Loss:  0.090268\n",
            "train epoch: 6 | batch staus: 50560/60000 ( 84%) | Loss:  0.109803\n",
            "train epoch: 6 | batch staus: 51200/60000 ( 85%) | Loss:  0.010315\n",
            "train epoch: 6 | batch staus: 51840/60000 ( 86%) | Loss:  0.033671\n",
            "train epoch: 6 | batch staus: 52480/60000 ( 87%) | Loss:  0.079376\n",
            "train epoch: 6 | batch staus: 53120/60000 ( 88%) | Loss:  0.123925\n",
            "train epoch: 6 | batch staus: 53760/60000 ( 90%) | Loss:  0.010931\n",
            "train epoch: 6 | batch staus: 54400/60000 ( 91%) | Loss:  0.102865\n",
            "train epoch: 6 | batch staus: 55040/60000 ( 92%) | Loss:  0.074353\n",
            "train epoch: 6 | batch staus: 55680/60000 ( 93%) | Loss:  0.013317\n",
            "train epoch: 6 | batch staus: 56320/60000 ( 94%) | Loss:  0.061665\n",
            "train epoch: 6 | batch staus: 56960/60000 ( 95%) | Loss:  0.048795\n",
            "train epoch: 6 | batch staus: 57600/60000 ( 96%) | Loss:  0.041095\n",
            "train epoch: 6 | batch staus: 58240/60000 ( 97%) | Loss:  0.108772\n",
            "train epoch: 6 | batch staus: 58880/60000 ( 98%) | Loss:  0.118871\n",
            "train epoch: 6 | batch staus: 59520/60000 ( 99%) | Loss:  0.125731\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0010, Accuracy: 9811/10000(98%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 7 | batch staus: 0/60000 ( 0%) | Loss:  0.038918\n",
            "train epoch: 7 | batch staus: 640/60000 ( 1%) | Loss:  0.034342\n",
            "train epoch: 7 | batch staus: 1280/60000 ( 2%) | Loss:  0.059989\n",
            "train epoch: 7 | batch staus: 1920/60000 ( 3%) | Loss:  0.009804\n",
            "train epoch: 7 | batch staus: 2560/60000 ( 4%) | Loss:  0.207912\n",
            "train epoch: 7 | batch staus: 3200/60000 ( 5%) | Loss:  0.108028\n",
            "train epoch: 7 | batch staus: 3840/60000 ( 6%) | Loss:  0.032335\n",
            "train epoch: 7 | batch staus: 4480/60000 ( 7%) | Loss:  0.085459\n",
            "train epoch: 7 | batch staus: 5120/60000 ( 9%) | Loss:  0.044284\n",
            "train epoch: 7 | batch staus: 5760/60000 ( 10%) | Loss:  0.011048\n",
            "train epoch: 7 | batch staus: 6400/60000 ( 11%) | Loss:  0.097877\n",
            "train epoch: 7 | batch staus: 7040/60000 ( 12%) | Loss:  0.048556\n",
            "train epoch: 7 | batch staus: 7680/60000 ( 13%) | Loss:  0.037374\n",
            "train epoch: 7 | batch staus: 8320/60000 ( 14%) | Loss:  0.111990\n",
            "train epoch: 7 | batch staus: 8960/60000 ( 15%) | Loss:  0.051099\n",
            "train epoch: 7 | batch staus: 9600/60000 ( 16%) | Loss:  0.027557\n",
            "train epoch: 7 | batch staus: 10240/60000 ( 17%) | Loss:  0.063365\n",
            "train epoch: 7 | batch staus: 10880/60000 ( 18%) | Loss:  0.020041\n",
            "train epoch: 7 | batch staus: 11520/60000 ( 19%) | Loss:  0.068706\n",
            "train epoch: 7 | batch staus: 12160/60000 ( 20%) | Loss:  0.066725\n",
            "train epoch: 7 | batch staus: 12800/60000 ( 21%) | Loss:  0.154814\n",
            "train epoch: 7 | batch staus: 13440/60000 ( 22%) | Loss:  0.014684\n",
            "train epoch: 7 | batch staus: 14080/60000 ( 23%) | Loss:  0.074775\n",
            "train epoch: 7 | batch staus: 14720/60000 ( 25%) | Loss:  0.067578\n",
            "train epoch: 7 | batch staus: 15360/60000 ( 26%) | Loss:  0.079349\n",
            "train epoch: 7 | batch staus: 16000/60000 ( 27%) | Loss:  0.075061\n",
            "train epoch: 7 | batch staus: 16640/60000 ( 28%) | Loss:  0.034620\n",
            "train epoch: 7 | batch staus: 17280/60000 ( 29%) | Loss:  0.043213\n",
            "train epoch: 7 | batch staus: 17920/60000 ( 30%) | Loss:  0.046134\n",
            "train epoch: 7 | batch staus: 18560/60000 ( 31%) | Loss:  0.024324\n",
            "train epoch: 7 | batch staus: 19200/60000 ( 32%) | Loss:  0.054768\n",
            "train epoch: 7 | batch staus: 19840/60000 ( 33%) | Loss:  0.054820\n",
            "train epoch: 7 | batch staus: 20480/60000 ( 34%) | Loss:  0.072888\n",
            "train epoch: 7 | batch staus: 21120/60000 ( 35%) | Loss:  0.023732\n",
            "train epoch: 7 | batch staus: 21760/60000 ( 36%) | Loss:  0.035175\n",
            "train epoch: 7 | batch staus: 22400/60000 ( 37%) | Loss:  0.059328\n",
            "train epoch: 7 | batch staus: 23040/60000 ( 38%) | Loss:  0.061196\n",
            "train epoch: 7 | batch staus: 23680/60000 ( 39%) | Loss:  0.035700\n",
            "train epoch: 7 | batch staus: 24320/60000 ( 41%) | Loss:  0.133012\n",
            "train epoch: 7 | batch staus: 24960/60000 ( 42%) | Loss:  0.109646\n",
            "train epoch: 7 | batch staus: 25600/60000 ( 43%) | Loss:  0.057279\n",
            "train epoch: 7 | batch staus: 26240/60000 ( 44%) | Loss:  0.064836\n",
            "train epoch: 7 | batch staus: 26880/60000 ( 45%) | Loss:  0.060282\n",
            "train epoch: 7 | batch staus: 27520/60000 ( 46%) | Loss:  0.131486\n",
            "train epoch: 7 | batch staus: 28160/60000 ( 47%) | Loss:  0.130976\n",
            "train epoch: 7 | batch staus: 28800/60000 ( 48%) | Loss:  0.040194\n",
            "train epoch: 7 | batch staus: 29440/60000 ( 49%) | Loss:  0.034546\n",
            "train epoch: 7 | batch staus: 30080/60000 ( 50%) | Loss:  0.042861\n",
            "train epoch: 7 | batch staus: 30720/60000 ( 51%) | Loss:  0.099724\n",
            "train epoch: 7 | batch staus: 31360/60000 ( 52%) | Loss:  0.086372\n",
            "train epoch: 7 | batch staus: 32000/60000 ( 53%) | Loss:  0.033314\n",
            "train epoch: 7 | batch staus: 32640/60000 ( 54%) | Loss:  0.137163\n",
            "train epoch: 7 | batch staus: 33280/60000 ( 55%) | Loss:  0.013018\n",
            "train epoch: 7 | batch staus: 33920/60000 ( 57%) | Loss:  0.009157\n",
            "train epoch: 7 | batch staus: 34560/60000 ( 58%) | Loss:  0.023518\n",
            "train epoch: 7 | batch staus: 35200/60000 ( 59%) | Loss:  0.020225\n",
            "train epoch: 7 | batch staus: 35840/60000 ( 60%) | Loss:  0.078491\n",
            "train epoch: 7 | batch staus: 36480/60000 ( 61%) | Loss:  0.108843\n",
            "train epoch: 7 | batch staus: 37120/60000 ( 62%) | Loss:  0.110368\n",
            "train epoch: 7 | batch staus: 37760/60000 ( 63%) | Loss:  0.074607\n",
            "train epoch: 7 | batch staus: 38400/60000 ( 64%) | Loss:  0.062900\n",
            "train epoch: 7 | batch staus: 39040/60000 ( 65%) | Loss:  0.048276\n",
            "train epoch: 7 | batch staus: 39680/60000 ( 66%) | Loss:  0.008801\n",
            "train epoch: 7 | batch staus: 40320/60000 ( 67%) | Loss:  0.090060\n",
            "train epoch: 7 | batch staus: 40960/60000 ( 68%) | Loss:  0.057718\n",
            "train epoch: 7 | batch staus: 41600/60000 ( 69%) | Loss:  0.215172\n",
            "train epoch: 7 | batch staus: 42240/60000 ( 70%) | Loss:  0.042833\n",
            "train epoch: 7 | batch staus: 42880/60000 ( 71%) | Loss:  0.086406\n",
            "train epoch: 7 | batch staus: 43520/60000 ( 72%) | Loss:  0.074110\n",
            "train epoch: 7 | batch staus: 44160/60000 ( 74%) | Loss:  0.008408\n",
            "train epoch: 7 | batch staus: 44800/60000 ( 75%) | Loss:  0.006664\n",
            "train epoch: 7 | batch staus: 45440/60000 ( 76%) | Loss:  0.063530\n",
            "train epoch: 7 | batch staus: 46080/60000 ( 77%) | Loss:  0.031314\n",
            "train epoch: 7 | batch staus: 46720/60000 ( 78%) | Loss:  0.008076\n",
            "train epoch: 7 | batch staus: 47360/60000 ( 79%) | Loss:  0.034944\n",
            "train epoch: 7 | batch staus: 48000/60000 ( 80%) | Loss:  0.019375\n",
            "train epoch: 7 | batch staus: 48640/60000 ( 81%) | Loss:  0.132528\n",
            "train epoch: 7 | batch staus: 49280/60000 ( 82%) | Loss:  0.043721\n",
            "train epoch: 7 | batch staus: 49920/60000 ( 83%) | Loss:  0.030771\n",
            "train epoch: 7 | batch staus: 50560/60000 ( 84%) | Loss:  0.054904\n",
            "train epoch: 7 | batch staus: 51200/60000 ( 85%) | Loss:  0.043795\n",
            "train epoch: 7 | batch staus: 51840/60000 ( 86%) | Loss:  0.006033\n",
            "train epoch: 7 | batch staus: 52480/60000 ( 87%) | Loss:  0.115309\n",
            "train epoch: 7 | batch staus: 53120/60000 ( 88%) | Loss:  0.054362\n",
            "train epoch: 7 | batch staus: 53760/60000 ( 90%) | Loss:  0.038283\n",
            "train epoch: 7 | batch staus: 54400/60000 ( 91%) | Loss:  0.091836\n",
            "train epoch: 7 | batch staus: 55040/60000 ( 92%) | Loss:  0.025908\n",
            "train epoch: 7 | batch staus: 55680/60000 ( 93%) | Loss:  0.098682\n",
            "train epoch: 7 | batch staus: 56320/60000 ( 94%) | Loss:  0.071344\n",
            "train epoch: 7 | batch staus: 56960/60000 ( 95%) | Loss:  0.018340\n",
            "train epoch: 7 | batch staus: 57600/60000 ( 96%) | Loss:  0.043428\n",
            "train epoch: 7 | batch staus: 58240/60000 ( 97%) | Loss:  0.023709\n",
            "train epoch: 7 | batch staus: 58880/60000 ( 98%) | Loss:  0.036671\n",
            "train epoch: 7 | batch staus: 59520/60000 ( 99%) | Loss:  0.099653\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0009, Accuracy: 9810/10000(98%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 8 | batch staus: 0/60000 ( 0%) | Loss:  0.045128\n",
            "train epoch: 8 | batch staus: 640/60000 ( 1%) | Loss:  0.098780\n",
            "train epoch: 8 | batch staus: 1280/60000 ( 2%) | Loss:  0.066881\n",
            "train epoch: 8 | batch staus: 1920/60000 ( 3%) | Loss:  0.049027\n",
            "train epoch: 8 | batch staus: 2560/60000 ( 4%) | Loss:  0.042202\n",
            "train epoch: 8 | batch staus: 3200/60000 ( 5%) | Loss:  0.056653\n",
            "train epoch: 8 | batch staus: 3840/60000 ( 6%) | Loss:  0.039083\n",
            "train epoch: 8 | batch staus: 4480/60000 ( 7%) | Loss:  0.080335\n",
            "train epoch: 8 | batch staus: 5120/60000 ( 9%) | Loss:  0.067125\n",
            "train epoch: 8 | batch staus: 5760/60000 ( 10%) | Loss:  0.017904\n",
            "train epoch: 8 | batch staus: 6400/60000 ( 11%) | Loss:  0.040577\n",
            "train epoch: 8 | batch staus: 7040/60000 ( 12%) | Loss:  0.020886\n",
            "train epoch: 8 | batch staus: 7680/60000 ( 13%) | Loss:  0.025234\n",
            "train epoch: 8 | batch staus: 8320/60000 ( 14%) | Loss:  0.197031\n",
            "train epoch: 8 | batch staus: 8960/60000 ( 15%) | Loss:  0.085455\n",
            "train epoch: 8 | batch staus: 9600/60000 ( 16%) | Loss:  0.083116\n",
            "train epoch: 8 | batch staus: 10240/60000 ( 17%) | Loss:  0.099766\n",
            "train epoch: 8 | batch staus: 10880/60000 ( 18%) | Loss:  0.013375\n",
            "train epoch: 8 | batch staus: 11520/60000 ( 19%) | Loss:  0.036933\n",
            "train epoch: 8 | batch staus: 12160/60000 ( 20%) | Loss:  0.013476\n",
            "train epoch: 8 | batch staus: 12800/60000 ( 21%) | Loss:  0.025189\n",
            "train epoch: 8 | batch staus: 13440/60000 ( 22%) | Loss:  0.176363\n",
            "train epoch: 8 | batch staus: 14080/60000 ( 23%) | Loss:  0.023962\n",
            "train epoch: 8 | batch staus: 14720/60000 ( 25%) | Loss:  0.084102\n",
            "train epoch: 8 | batch staus: 15360/60000 ( 26%) | Loss:  0.164631\n",
            "train epoch: 8 | batch staus: 16000/60000 ( 27%) | Loss:  0.081658\n",
            "train epoch: 8 | batch staus: 16640/60000 ( 28%) | Loss:  0.090920\n",
            "train epoch: 8 | batch staus: 17280/60000 ( 29%) | Loss:  0.046564\n",
            "train epoch: 8 | batch staus: 17920/60000 ( 30%) | Loss:  0.022223\n",
            "train epoch: 8 | batch staus: 18560/60000 ( 31%) | Loss:  0.167928\n",
            "train epoch: 8 | batch staus: 19200/60000 ( 32%) | Loss:  0.049976\n",
            "train epoch: 8 | batch staus: 19840/60000 ( 33%) | Loss:  0.032562\n",
            "train epoch: 8 | batch staus: 20480/60000 ( 34%) | Loss:  0.077556\n",
            "train epoch: 8 | batch staus: 21120/60000 ( 35%) | Loss:  0.011470\n",
            "train epoch: 8 | batch staus: 21760/60000 ( 36%) | Loss:  0.050352\n",
            "train epoch: 8 | batch staus: 22400/60000 ( 37%) | Loss:  0.070887\n",
            "train epoch: 8 | batch staus: 23040/60000 ( 38%) | Loss:  0.022618\n",
            "train epoch: 8 | batch staus: 23680/60000 ( 39%) | Loss:  0.021214\n",
            "train epoch: 8 | batch staus: 24320/60000 ( 41%) | Loss:  0.081157\n",
            "train epoch: 8 | batch staus: 24960/60000 ( 42%) | Loss:  0.053252\n",
            "train epoch: 8 | batch staus: 25600/60000 ( 43%) | Loss:  0.099054\n",
            "train epoch: 8 | batch staus: 26240/60000 ( 44%) | Loss:  0.130009\n",
            "train epoch: 8 | batch staus: 26880/60000 ( 45%) | Loss:  0.030220\n",
            "train epoch: 8 | batch staus: 27520/60000 ( 46%) | Loss:  0.053686\n",
            "train epoch: 8 | batch staus: 28160/60000 ( 47%) | Loss:  0.207144\n",
            "train epoch: 8 | batch staus: 28800/60000 ( 48%) | Loss:  0.039825\n",
            "train epoch: 8 | batch staus: 29440/60000 ( 49%) | Loss:  0.026267\n",
            "train epoch: 8 | batch staus: 30080/60000 ( 50%) | Loss:  0.023085\n",
            "train epoch: 8 | batch staus: 30720/60000 ( 51%) | Loss:  0.059244\n",
            "train epoch: 8 | batch staus: 31360/60000 ( 52%) | Loss:  0.149192\n",
            "train epoch: 8 | batch staus: 32000/60000 ( 53%) | Loss:  0.078182\n",
            "train epoch: 8 | batch staus: 32640/60000 ( 54%) | Loss:  0.028097\n",
            "train epoch: 8 | batch staus: 33280/60000 ( 55%) | Loss:  0.002698\n",
            "train epoch: 8 | batch staus: 33920/60000 ( 57%) | Loss:  0.012122\n",
            "train epoch: 8 | batch staus: 34560/60000 ( 58%) | Loss:  0.012889\n",
            "train epoch: 8 | batch staus: 35200/60000 ( 59%) | Loss:  0.033436\n",
            "train epoch: 8 | batch staus: 35840/60000 ( 60%) | Loss:  0.086589\n",
            "train epoch: 8 | batch staus: 36480/60000 ( 61%) | Loss:  0.047127\n",
            "train epoch: 8 | batch staus: 37120/60000 ( 62%) | Loss:  0.069602\n",
            "train epoch: 8 | batch staus: 37760/60000 ( 63%) | Loss:  0.007813\n",
            "train epoch: 8 | batch staus: 38400/60000 ( 64%) | Loss:  0.063074\n",
            "train epoch: 8 | batch staus: 39040/60000 ( 65%) | Loss:  0.034824\n",
            "train epoch: 8 | batch staus: 39680/60000 ( 66%) | Loss:  0.021797\n",
            "train epoch: 8 | batch staus: 40320/60000 ( 67%) | Loss:  0.124065\n",
            "train epoch: 8 | batch staus: 40960/60000 ( 68%) | Loss:  0.178095\n",
            "train epoch: 8 | batch staus: 41600/60000 ( 69%) | Loss:  0.151036\n",
            "train epoch: 8 | batch staus: 42240/60000 ( 70%) | Loss:  0.109155\n",
            "train epoch: 8 | batch staus: 42880/60000 ( 71%) | Loss:  0.038118\n",
            "train epoch: 8 | batch staus: 43520/60000 ( 72%) | Loss:  0.060752\n",
            "train epoch: 8 | batch staus: 44160/60000 ( 74%) | Loss:  0.082583\n",
            "train epoch: 8 | batch staus: 44800/60000 ( 75%) | Loss:  0.016800\n",
            "train epoch: 8 | batch staus: 45440/60000 ( 76%) | Loss:  0.065690\n",
            "train epoch: 8 | batch staus: 46080/60000 ( 77%) | Loss:  0.008878\n",
            "train epoch: 8 | batch staus: 46720/60000 ( 78%) | Loss:  0.040746\n",
            "train epoch: 8 | batch staus: 47360/60000 ( 79%) | Loss:  0.028073\n",
            "train epoch: 8 | batch staus: 48000/60000 ( 80%) | Loss:  0.014021\n",
            "train epoch: 8 | batch staus: 48640/60000 ( 81%) | Loss:  0.100763\n",
            "train epoch: 8 | batch staus: 49280/60000 ( 82%) | Loss:  0.174402\n",
            "train epoch: 8 | batch staus: 49920/60000 ( 83%) | Loss:  0.023077\n",
            "train epoch: 8 | batch staus: 50560/60000 ( 84%) | Loss:  0.160078\n",
            "train epoch: 8 | batch staus: 51200/60000 ( 85%) | Loss:  0.056927\n",
            "train epoch: 8 | batch staus: 51840/60000 ( 86%) | Loss:  0.070845\n",
            "train epoch: 8 | batch staus: 52480/60000 ( 87%) | Loss:  0.017253\n",
            "train epoch: 8 | batch staus: 53120/60000 ( 88%) | Loss:  0.017343\n",
            "train epoch: 8 | batch staus: 53760/60000 ( 90%) | Loss:  0.046249\n",
            "train epoch: 8 | batch staus: 54400/60000 ( 91%) | Loss:  0.040846\n",
            "train epoch: 8 | batch staus: 55040/60000 ( 92%) | Loss:  0.112547\n",
            "train epoch: 8 | batch staus: 55680/60000 ( 93%) | Loss:  0.096801\n",
            "train epoch: 8 | batch staus: 56320/60000 ( 94%) | Loss:  0.173464\n",
            "train epoch: 8 | batch staus: 56960/60000 ( 95%) | Loss:  0.102017\n",
            "train epoch: 8 | batch staus: 57600/60000 ( 96%) | Loss:  0.182497\n",
            "train epoch: 8 | batch staus: 58240/60000 ( 97%) | Loss:  0.021550\n",
            "train epoch: 8 | batch staus: 58880/60000 ( 98%) | Loss:  0.082700\n",
            "train epoch: 8 | batch staus: 59520/60000 ( 99%) | Loss:  0.114318\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0008, Accuracy: 9837/10000(98%)\n",
            "Testing time: 0m 25s\n",
            "train epoch: 9 | batch staus: 0/60000 ( 0%) | Loss:  0.008631\n",
            "train epoch: 9 | batch staus: 640/60000 ( 1%) | Loss:  0.088326\n",
            "train epoch: 9 | batch staus: 1280/60000 ( 2%) | Loss:  0.173001\n",
            "train epoch: 9 | batch staus: 1920/60000 ( 3%) | Loss:  0.048234\n",
            "train epoch: 9 | batch staus: 2560/60000 ( 4%) | Loss:  0.142406\n",
            "train epoch: 9 | batch staus: 3200/60000 ( 5%) | Loss:  0.034900\n",
            "train epoch: 9 | batch staus: 3840/60000 ( 6%) | Loss:  0.010870\n",
            "train epoch: 9 | batch staus: 4480/60000 ( 7%) | Loss:  0.044106\n",
            "train epoch: 9 | batch staus: 5120/60000 ( 9%) | Loss:  0.013577\n",
            "train epoch: 9 | batch staus: 5760/60000 ( 10%) | Loss:  0.013303\n",
            "train epoch: 9 | batch staus: 6400/60000 ( 11%) | Loss:  0.059343\n",
            "train epoch: 9 | batch staus: 7040/60000 ( 12%) | Loss:  0.174077\n",
            "train epoch: 9 | batch staus: 7680/60000 ( 13%) | Loss:  0.009063\n",
            "train epoch: 9 | batch staus: 8320/60000 ( 14%) | Loss:  0.022009\n",
            "train epoch: 9 | batch staus: 8960/60000 ( 15%) | Loss:  0.167866\n",
            "train epoch: 9 | batch staus: 9600/60000 ( 16%) | Loss:  0.037674\n",
            "train epoch: 9 | batch staus: 10240/60000 ( 17%) | Loss:  0.012927\n",
            "train epoch: 9 | batch staus: 10880/60000 ( 18%) | Loss:  0.071993\n",
            "train epoch: 9 | batch staus: 11520/60000 ( 19%) | Loss:  0.083898\n",
            "train epoch: 9 | batch staus: 12160/60000 ( 20%) | Loss:  0.065249\n",
            "train epoch: 9 | batch staus: 12800/60000 ( 21%) | Loss:  0.008001\n",
            "train epoch: 9 | batch staus: 13440/60000 ( 22%) | Loss:  0.113864\n",
            "train epoch: 9 | batch staus: 14080/60000 ( 23%) | Loss:  0.010504\n",
            "train epoch: 9 | batch staus: 14720/60000 ( 25%) | Loss:  0.125243\n",
            "train epoch: 9 | batch staus: 15360/60000 ( 26%) | Loss:  0.044022\n",
            "train epoch: 9 | batch staus: 16000/60000 ( 27%) | Loss:  0.030273\n",
            "train epoch: 9 | batch staus: 16640/60000 ( 28%) | Loss:  0.045668\n",
            "train epoch: 9 | batch staus: 17280/60000 ( 29%) | Loss:  0.023411\n",
            "train epoch: 9 | batch staus: 17920/60000 ( 30%) | Loss:  0.023258\n",
            "train epoch: 9 | batch staus: 18560/60000 ( 31%) | Loss:  0.012286\n",
            "train epoch: 9 | batch staus: 19200/60000 ( 32%) | Loss:  0.037770\n",
            "train epoch: 9 | batch staus: 19840/60000 ( 33%) | Loss:  0.038397\n",
            "train epoch: 9 | batch staus: 20480/60000 ( 34%) | Loss:  0.051906\n",
            "train epoch: 9 | batch staus: 21120/60000 ( 35%) | Loss:  0.036585\n",
            "train epoch: 9 | batch staus: 21760/60000 ( 36%) | Loss:  0.019798\n",
            "train epoch: 9 | batch staus: 22400/60000 ( 37%) | Loss:  0.097638\n",
            "train epoch: 9 | batch staus: 23040/60000 ( 38%) | Loss:  0.108211\n",
            "train epoch: 9 | batch staus: 23680/60000 ( 39%) | Loss:  0.046656\n",
            "train epoch: 9 | batch staus: 24320/60000 ( 41%) | Loss:  0.118340\n",
            "train epoch: 9 | batch staus: 24960/60000 ( 42%) | Loss:  0.004389\n",
            "train epoch: 9 | batch staus: 25600/60000 ( 43%) | Loss:  0.006748\n",
            "train epoch: 9 | batch staus: 26240/60000 ( 44%) | Loss:  0.044036\n",
            "train epoch: 9 | batch staus: 26880/60000 ( 45%) | Loss:  0.007229\n",
            "train epoch: 9 | batch staus: 27520/60000 ( 46%) | Loss:  0.051493\n",
            "train epoch: 9 | batch staus: 28160/60000 ( 47%) | Loss:  0.091881\n",
            "train epoch: 9 | batch staus: 28800/60000 ( 48%) | Loss:  0.029740\n",
            "train epoch: 9 | batch staus: 29440/60000 ( 49%) | Loss:  0.142617\n",
            "train epoch: 9 | batch staus: 30080/60000 ( 50%) | Loss:  0.071344\n",
            "train epoch: 9 | batch staus: 30720/60000 ( 51%) | Loss:  0.032597\n",
            "train epoch: 9 | batch staus: 31360/60000 ( 52%) | Loss:  0.099769\n",
            "train epoch: 9 | batch staus: 32000/60000 ( 53%) | Loss:  0.110170\n",
            "train epoch: 9 | batch staus: 32640/60000 ( 54%) | Loss:  0.028065\n",
            "train epoch: 9 | batch staus: 33280/60000 ( 55%) | Loss:  0.051481\n",
            "train epoch: 9 | batch staus: 33920/60000 ( 57%) | Loss:  0.011950\n",
            "train epoch: 9 | batch staus: 34560/60000 ( 58%) | Loss:  0.120986\n",
            "train epoch: 9 | batch staus: 35200/60000 ( 59%) | Loss:  0.055533\n",
            "train epoch: 9 | batch staus: 35840/60000 ( 60%) | Loss:  0.130801\n",
            "train epoch: 9 | batch staus: 36480/60000 ( 61%) | Loss:  0.052266\n",
            "train epoch: 9 | batch staus: 37120/60000 ( 62%) | Loss:  0.019913\n",
            "train epoch: 9 | batch staus: 37760/60000 ( 63%) | Loss:  0.047858\n",
            "train epoch: 9 | batch staus: 38400/60000 ( 64%) | Loss:  0.142765\n",
            "train epoch: 9 | batch staus: 39040/60000 ( 65%) | Loss:  0.063235\n",
            "train epoch: 9 | batch staus: 39680/60000 ( 66%) | Loss:  0.066685\n",
            "train epoch: 9 | batch staus: 40320/60000 ( 67%) | Loss:  0.014409\n",
            "train epoch: 9 | batch staus: 40960/60000 ( 68%) | Loss:  0.098714\n",
            "train epoch: 9 | batch staus: 41600/60000 ( 69%) | Loss:  0.036861\n",
            "train epoch: 9 | batch staus: 42240/60000 ( 70%) | Loss:  0.014147\n",
            "train epoch: 9 | batch staus: 42880/60000 ( 71%) | Loss:  0.053141\n",
            "train epoch: 9 | batch staus: 43520/60000 ( 72%) | Loss:  0.050429\n",
            "train epoch: 9 | batch staus: 44160/60000 ( 74%) | Loss:  0.076646\n",
            "train epoch: 9 | batch staus: 44800/60000 ( 75%) | Loss:  0.110110\n",
            "train epoch: 9 | batch staus: 45440/60000 ( 76%) | Loss:  0.010235\n",
            "train epoch: 9 | batch staus: 46080/60000 ( 77%) | Loss:  0.013851\n",
            "train epoch: 9 | batch staus: 46720/60000 ( 78%) | Loss:  0.096495\n",
            "train epoch: 9 | batch staus: 47360/60000 ( 79%) | Loss:  0.128330\n",
            "train epoch: 9 | batch staus: 48000/60000 ( 80%) | Loss:  0.019614\n",
            "train epoch: 9 | batch staus: 48640/60000 ( 81%) | Loss:  0.028010\n",
            "train epoch: 9 | batch staus: 49280/60000 ( 82%) | Loss:  0.030900\n",
            "train epoch: 9 | batch staus: 49920/60000 ( 83%) | Loss:  0.046494\n",
            "train epoch: 9 | batch staus: 50560/60000 ( 84%) | Loss:  0.209163\n",
            "train epoch: 9 | batch staus: 51200/60000 ( 85%) | Loss:  0.053330\n",
            "train epoch: 9 | batch staus: 51840/60000 ( 86%) | Loss:  0.067249\n",
            "train epoch: 9 | batch staus: 52480/60000 ( 87%) | Loss:  0.040523\n",
            "train epoch: 9 | batch staus: 53120/60000 ( 88%) | Loss:  0.078221\n",
            "train epoch: 9 | batch staus: 53760/60000 ( 90%) | Loss:  0.025442\n",
            "train epoch: 9 | batch staus: 54400/60000 ( 91%) | Loss:  0.039955\n",
            "train epoch: 9 | batch staus: 55040/60000 ( 92%) | Loss:  0.027072\n",
            "train epoch: 9 | batch staus: 55680/60000 ( 93%) | Loss:  0.016617\n",
            "train epoch: 9 | batch staus: 56320/60000 ( 94%) | Loss:  0.018651\n",
            "train epoch: 9 | batch staus: 56960/60000 ( 95%) | Loss:  0.108260\n",
            "train epoch: 9 | batch staus: 57600/60000 ( 96%) | Loss:  0.107057\n",
            "train epoch: 9 | batch staus: 58240/60000 ( 97%) | Loss:  0.036881\n",
            "train epoch: 9 | batch staus: 58880/60000 ( 98%) | Loss:  0.187467\n",
            "train epoch: 9 | batch staus: 59520/60000 ( 99%) | Loss:  0.108126\n",
            "Training time: 0m 23s\n",
            "=======\n",
            " test set: average loss:  0.0008, Accuracy: 9835/10000(98%)\n",
            "Testing time: 0m 25s\n",
            "total time: 3m  44s \n",
            " model was trained on cpu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP8YWqUPmJss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj1_EEyQzg5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34a940f-aaef-4fa2-9001-f387003923d3"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch import nn, Tensor\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "batch_size=64\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing mnist model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "train_dataset = datasets.MNIST(root = './mnist_data?',\n",
        "                                train = True,\n",
        "                                transform=transforms.ToTensor(),\n",
        "                                download=True\n",
        "                                )\n",
        "test_dataset=datasets.MNIST(root = './mnist_data?',\n",
        "                                train = False,\n",
        "                                transform=transforms.ToTensor(),\n",
        "                                )\n",
        "train_loader=data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "test_loader=data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "class InceptionA(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(InceptionA, self).__init__()\n",
        "    self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "    self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1) \n",
        "    self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
        "    self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "  def forward(self, x):\n",
        "    branch1x1 = self.branch1x1(x)\n",
        "    branch5x5 = self.branch5x5_1(x)\n",
        "    branch5x5 = self.branch5x5_2(branch5x5)\n",
        "    branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "    branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "    branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "    branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "    branch_pool = self.branch_pool(branch_pool)\n",
        "    outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "    return torch.cat(outputs, 1)\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
        "    self.incept1 = InceptionA(in_channels=10)\n",
        "    self.incept2 = InceptionA(in_channels=20)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(1408, 10)\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = self.incept1(x)\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = self.incept2(x)\n",
        "    x = x.view(in_size, -1) # flatten the tensor\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)\n",
        "\n",
        "model=Net()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traing mnist model on cpu\n",
            "============================================\n",
            "train epoch: 1 | batch staus: 0/60000 ( 0%) | Loss:  2.299056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch: 1 | batch staus: 640/60000 ( 1%) | Loss:  2.304716\n",
            "train epoch: 1 | batch staus: 1280/60000 ( 2%) | Loss:  2.320404\n",
            "train epoch: 1 | batch staus: 1920/60000 ( 3%) | Loss:  2.295493\n",
            "train epoch: 1 | batch staus: 2560/60000 ( 4%) | Loss:  2.291165\n",
            "train epoch: 1 | batch staus: 3200/60000 ( 5%) | Loss:  2.300784\n",
            "train epoch: 1 | batch staus: 3840/60000 ( 6%) | Loss:  2.294770\n",
            "train epoch: 1 | batch staus: 4480/60000 ( 7%) | Loss:  2.302199\n",
            "train epoch: 1 | batch staus: 5120/60000 ( 9%) | Loss:  2.288033\n",
            "train epoch: 1 | batch staus: 5760/60000 ( 10%) | Loss:  2.305830\n",
            "train epoch: 1 | batch staus: 6400/60000 ( 11%) | Loss:  2.312325\n",
            "train epoch: 1 | batch staus: 7040/60000 ( 12%) | Loss:  2.290023\n",
            "train epoch: 1 | batch staus: 7680/60000 ( 13%) | Loss:  2.282470\n",
            "train epoch: 1 | batch staus: 8320/60000 ( 14%) | Loss:  2.292900\n",
            "train epoch: 1 | batch staus: 8960/60000 ( 15%) | Loss:  2.279922\n",
            "train epoch: 1 | batch staus: 9600/60000 ( 16%) | Loss:  2.294288\n",
            "train epoch: 1 | batch staus: 10240/60000 ( 17%) | Loss:  2.266493\n",
            "train epoch: 1 | batch staus: 10880/60000 ( 18%) | Loss:  2.277244\n",
            "train epoch: 1 | batch staus: 11520/60000 ( 19%) | Loss:  2.269403\n",
            "train epoch: 1 | batch staus: 12160/60000 ( 20%) | Loss:  2.237714\n",
            "train epoch: 1 | batch staus: 12800/60000 ( 21%) | Loss:  2.231574\n",
            "train epoch: 1 | batch staus: 13440/60000 ( 22%) | Loss:  2.199876\n",
            "train epoch: 1 | batch staus: 14080/60000 ( 23%) | Loss:  2.198837\n",
            "train epoch: 1 | batch staus: 14720/60000 ( 25%) | Loss:  2.107781\n",
            "train epoch: 1 | batch staus: 15360/60000 ( 26%) | Loss:  2.020872\n",
            "train epoch: 1 | batch staus: 16000/60000 ( 27%) | Loss:  1.855247\n",
            "train epoch: 1 | batch staus: 16640/60000 ( 28%) | Loss:  1.787890\n",
            "train epoch: 1 | batch staus: 17280/60000 ( 29%) | Loss:  1.428818\n",
            "train epoch: 1 | batch staus: 17920/60000 ( 30%) | Loss:  1.127959\n",
            "train epoch: 1 | batch staus: 18560/60000 ( 31%) | Loss:  0.803707\n",
            "train epoch: 1 | batch staus: 19200/60000 ( 32%) | Loss:  1.001747\n",
            "train epoch: 1 | batch staus: 19840/60000 ( 33%) | Loss:  0.864272\n",
            "train epoch: 1 | batch staus: 20480/60000 ( 34%) | Loss:  0.707479\n",
            "train epoch: 1 | batch staus: 21120/60000 ( 35%) | Loss:  0.668931\n",
            "train epoch: 1 | batch staus: 21760/60000 ( 36%) | Loss:  0.621189\n",
            "train epoch: 1 | batch staus: 22400/60000 ( 37%) | Loss:  0.696433\n",
            "train epoch: 1 | batch staus: 23040/60000 ( 38%) | Loss:  0.563552\n",
            "train epoch: 1 | batch staus: 23680/60000 ( 39%) | Loss:  0.528429\n",
            "train epoch: 1 | batch staus: 24320/60000 ( 41%) | Loss:  0.620101\n",
            "train epoch: 1 | batch staus: 24960/60000 ( 42%) | Loss:  0.381258\n",
            "train epoch: 1 | batch staus: 25600/60000 ( 43%) | Loss:  0.415333\n",
            "train epoch: 1 | batch staus: 26240/60000 ( 44%) | Loss:  0.456156\n",
            "train epoch: 1 | batch staus: 26880/60000 ( 45%) | Loss:  0.518551\n",
            "train epoch: 1 | batch staus: 27520/60000 ( 46%) | Loss:  0.640132\n",
            "train epoch: 1 | batch staus: 28160/60000 ( 47%) | Loss:  0.370855\n",
            "train epoch: 1 | batch staus: 28800/60000 ( 48%) | Loss:  0.462783\n",
            "train epoch: 1 | batch staus: 29440/60000 ( 49%) | Loss:  0.328603\n",
            "train epoch: 1 | batch staus: 30080/60000 ( 50%) | Loss:  0.499379\n",
            "train epoch: 1 | batch staus: 30720/60000 ( 51%) | Loss:  0.312543\n",
            "train epoch: 1 | batch staus: 31360/60000 ( 52%) | Loss:  0.461317\n",
            "train epoch: 1 | batch staus: 32000/60000 ( 53%) | Loss:  0.518007\n",
            "train epoch: 1 | batch staus: 32640/60000 ( 54%) | Loss:  0.523000\n",
            "train epoch: 1 | batch staus: 33280/60000 ( 55%) | Loss:  0.233799\n",
            "train epoch: 1 | batch staus: 33920/60000 ( 57%) | Loss:  0.425271\n",
            "train epoch: 1 | batch staus: 34560/60000 ( 58%) | Loss:  0.226044\n",
            "train epoch: 1 | batch staus: 35200/60000 ( 59%) | Loss:  0.483210\n",
            "train epoch: 1 | batch staus: 35840/60000 ( 60%) | Loss:  0.418851\n",
            "train epoch: 1 | batch staus: 36480/60000 ( 61%) | Loss:  0.379424\n",
            "train epoch: 1 | batch staus: 37120/60000 ( 62%) | Loss:  0.454276\n",
            "train epoch: 1 | batch staus: 37760/60000 ( 63%) | Loss:  0.484011\n",
            "train epoch: 1 | batch staus: 38400/60000 ( 64%) | Loss:  0.250196\n",
            "train epoch: 1 | batch staus: 39040/60000 ( 65%) | Loss:  0.373983\n",
            "train epoch: 1 | batch staus: 39680/60000 ( 66%) | Loss:  0.344659\n",
            "train epoch: 1 | batch staus: 40320/60000 ( 67%) | Loss:  0.411364\n",
            "train epoch: 1 | batch staus: 40960/60000 ( 68%) | Loss:  0.199851\n",
            "train epoch: 1 | batch staus: 41600/60000 ( 69%) | Loss:  0.570579\n",
            "train epoch: 1 | batch staus: 42240/60000 ( 70%) | Loss:  0.391259\n",
            "train epoch: 1 | batch staus: 42880/60000 ( 71%) | Loss:  0.410021\n",
            "train epoch: 1 | batch staus: 43520/60000 ( 72%) | Loss:  0.271622\n",
            "train epoch: 1 | batch staus: 44160/60000 ( 74%) | Loss:  0.375224\n",
            "train epoch: 1 | batch staus: 44800/60000 ( 75%) | Loss:  0.214136\n",
            "train epoch: 1 | batch staus: 45440/60000 ( 76%) | Loss:  0.271285\n",
            "train epoch: 1 | batch staus: 46080/60000 ( 77%) | Loss:  0.572422\n",
            "train epoch: 1 | batch staus: 46720/60000 ( 78%) | Loss:  0.276052\n",
            "train epoch: 1 | batch staus: 47360/60000 ( 79%) | Loss:  0.639419\n",
            "train epoch: 1 | batch staus: 48000/60000 ( 80%) | Loss:  0.306092\n",
            "train epoch: 1 | batch staus: 48640/60000 ( 81%) | Loss:  0.382599\n",
            "train epoch: 1 | batch staus: 49280/60000 ( 82%) | Loss:  0.177057\n",
            "train epoch: 1 | batch staus: 49920/60000 ( 83%) | Loss:  0.300478\n",
            "train epoch: 1 | batch staus: 50560/60000 ( 84%) | Loss:  0.235588\n",
            "train epoch: 1 | batch staus: 51200/60000 ( 85%) | Loss:  0.341372\n",
            "train epoch: 1 | batch staus: 51840/60000 ( 86%) | Loss:  0.245373\n",
            "train epoch: 1 | batch staus: 52480/60000 ( 87%) | Loss:  0.223809\n",
            "train epoch: 1 | batch staus: 53120/60000 ( 88%) | Loss:  0.294236\n",
            "train epoch: 1 | batch staus: 53760/60000 ( 90%) | Loss:  0.229955\n",
            "train epoch: 1 | batch staus: 54400/60000 ( 91%) | Loss:  0.221021\n",
            "train epoch: 1 | batch staus: 55040/60000 ( 92%) | Loss:  0.471856\n",
            "train epoch: 1 | batch staus: 55680/60000 ( 93%) | Loss:  0.120389\n",
            "train epoch: 1 | batch staus: 56320/60000 ( 94%) | Loss:  0.363244\n",
            "train epoch: 1 | batch staus: 56960/60000 ( 95%) | Loss:  0.282160\n",
            "train epoch: 1 | batch staus: 57600/60000 ( 96%) | Loss:  0.204086\n",
            "train epoch: 1 | batch staus: 58240/60000 ( 97%) | Loss:  0.349271\n",
            "train epoch: 1 | batch staus: 58880/60000 ( 98%) | Loss:  0.415285\n",
            "train epoch: 1 | batch staus: 59520/60000 ( 99%) | Loss:  0.314185\n",
            "Training time: 1m 22s\n",
            "=======\n",
            " test set: average loss:  0.0038, Accuracy: 9266/10000(93%)\n",
            "Testing time: 1m 27s\n",
            "train epoch: 2 | batch staus: 0/60000 ( 0%) | Loss:  0.330504\n",
            "train epoch: 2 | batch staus: 640/60000 ( 1%) | Loss:  0.358188\n",
            "train epoch: 2 | batch staus: 1280/60000 ( 2%) | Loss:  0.228438\n",
            "train epoch: 2 | batch staus: 1920/60000 ( 3%) | Loss:  0.318352\n",
            "train epoch: 2 | batch staus: 2560/60000 ( 4%) | Loss:  0.335489\n",
            "train epoch: 2 | batch staus: 3200/60000 ( 5%) | Loss:  0.205576\n",
            "train epoch: 2 | batch staus: 3840/60000 ( 6%) | Loss:  0.268828\n",
            "train epoch: 2 | batch staus: 4480/60000 ( 7%) | Loss:  0.211316\n",
            "train epoch: 2 | batch staus: 5120/60000 ( 9%) | Loss:  0.157855\n",
            "train epoch: 2 | batch staus: 5760/60000 ( 10%) | Loss:  0.110262\n",
            "train epoch: 2 | batch staus: 6400/60000 ( 11%) | Loss:  0.357835\n",
            "train epoch: 2 | batch staus: 7040/60000 ( 12%) | Loss:  0.198073\n",
            "train epoch: 2 | batch staus: 7680/60000 ( 13%) | Loss:  0.242939\n",
            "train epoch: 2 | batch staus: 8320/60000 ( 14%) | Loss:  0.158176\n",
            "train epoch: 2 | batch staus: 8960/60000 ( 15%) | Loss:  0.206629\n",
            "train epoch: 2 | batch staus: 9600/60000 ( 16%) | Loss:  0.282915\n",
            "train epoch: 2 | batch staus: 10240/60000 ( 17%) | Loss:  0.158985\n",
            "train epoch: 2 | batch staus: 10880/60000 ( 18%) | Loss:  0.244098\n",
            "train epoch: 2 | batch staus: 11520/60000 ( 19%) | Loss:  0.353415\n",
            "train epoch: 2 | batch staus: 12160/60000 ( 20%) | Loss:  0.109061\n",
            "train epoch: 2 | batch staus: 12800/60000 ( 21%) | Loss:  0.234956\n",
            "train epoch: 2 | batch staus: 13440/60000 ( 22%) | Loss:  0.272388\n",
            "train epoch: 2 | batch staus: 14080/60000 ( 23%) | Loss:  0.193842\n",
            "train epoch: 2 | batch staus: 14720/60000 ( 25%) | Loss:  0.263230\n",
            "train epoch: 2 | batch staus: 15360/60000 ( 26%) | Loss:  0.407416\n",
            "train epoch: 2 | batch staus: 16000/60000 ( 27%) | Loss:  0.370744\n",
            "train epoch: 2 | batch staus: 16640/60000 ( 28%) | Loss:  0.478361\n",
            "train epoch: 2 | batch staus: 17280/60000 ( 29%) | Loss:  0.192698\n",
            "train epoch: 2 | batch staus: 17920/60000 ( 30%) | Loss:  0.366155\n",
            "train epoch: 2 | batch staus: 18560/60000 ( 31%) | Loss:  0.226330\n",
            "train epoch: 2 | batch staus: 19200/60000 ( 32%) | Loss:  0.105770\n",
            "train epoch: 2 | batch staus: 19840/60000 ( 33%) | Loss:  0.161246\n",
            "train epoch: 2 | batch staus: 20480/60000 ( 34%) | Loss:  0.161934\n",
            "train epoch: 2 | batch staus: 21120/60000 ( 35%) | Loss:  0.186717\n",
            "train epoch: 2 | batch staus: 21760/60000 ( 36%) | Loss:  0.133148\n",
            "train epoch: 2 | batch staus: 22400/60000 ( 37%) | Loss:  0.227863\n",
            "train epoch: 2 | batch staus: 23040/60000 ( 38%) | Loss:  0.267025\n",
            "train epoch: 2 | batch staus: 23680/60000 ( 39%) | Loss:  0.219914\n",
            "train epoch: 2 | batch staus: 24320/60000 ( 41%) | Loss:  0.101231\n",
            "train epoch: 2 | batch staus: 24960/60000 ( 42%) | Loss:  0.171112\n",
            "train epoch: 2 | batch staus: 25600/60000 ( 43%) | Loss:  0.390078\n",
            "train epoch: 2 | batch staus: 26240/60000 ( 44%) | Loss:  0.309330\n",
            "train epoch: 2 | batch staus: 26880/60000 ( 45%) | Loss:  0.135663\n",
            "train epoch: 2 | batch staus: 27520/60000 ( 46%) | Loss:  0.326362\n",
            "train epoch: 2 | batch staus: 28160/60000 ( 47%) | Loss:  0.095770\n",
            "train epoch: 2 | batch staus: 28800/60000 ( 48%) | Loss:  0.187707\n",
            "train epoch: 2 | batch staus: 29440/60000 ( 49%) | Loss:  0.175360\n",
            "train epoch: 2 | batch staus: 30080/60000 ( 50%) | Loss:  0.123221\n",
            "train epoch: 2 | batch staus: 30720/60000 ( 51%) | Loss:  0.053285\n",
            "train epoch: 2 | batch staus: 31360/60000 ( 52%) | Loss:  0.215713\n",
            "train epoch: 2 | batch staus: 32000/60000 ( 53%) | Loss:  0.187471\n",
            "train epoch: 2 | batch staus: 32640/60000 ( 54%) | Loss:  0.085014\n",
            "train epoch: 2 | batch staus: 33280/60000 ( 55%) | Loss:  0.157502\n",
            "train epoch: 2 | batch staus: 33920/60000 ( 57%) | Loss:  0.358029\n",
            "train epoch: 2 | batch staus: 34560/60000 ( 58%) | Loss:  0.102727\n",
            "train epoch: 2 | batch staus: 35200/60000 ( 59%) | Loss:  0.186751\n",
            "train epoch: 2 | batch staus: 35840/60000 ( 60%) | Loss:  0.203711\n",
            "train epoch: 2 | batch staus: 36480/60000 ( 61%) | Loss:  0.175341\n",
            "train epoch: 2 | batch staus: 37120/60000 ( 62%) | Loss:  0.151069\n",
            "train epoch: 2 | batch staus: 37760/60000 ( 63%) | Loss:  0.094477\n",
            "train epoch: 2 | batch staus: 38400/60000 ( 64%) | Loss:  0.105584\n",
            "train epoch: 2 | batch staus: 39040/60000 ( 65%) | Loss:  0.283083\n",
            "train epoch: 2 | batch staus: 39680/60000 ( 66%) | Loss:  0.086390\n",
            "train epoch: 2 | batch staus: 40320/60000 ( 67%) | Loss:  0.065693\n",
            "train epoch: 2 | batch staus: 40960/60000 ( 68%) | Loss:  0.089168\n",
            "train epoch: 2 | batch staus: 41600/60000 ( 69%) | Loss:  0.167316\n",
            "train epoch: 2 | batch staus: 42240/60000 ( 70%) | Loss:  0.223928\n",
            "train epoch: 2 | batch staus: 42880/60000 ( 71%) | Loss:  0.226047\n",
            "train epoch: 2 | batch staus: 43520/60000 ( 72%) | Loss:  0.202301\n",
            "train epoch: 2 | batch staus: 44160/60000 ( 74%) | Loss:  0.183709\n",
            "train epoch: 2 | batch staus: 44800/60000 ( 75%) | Loss:  0.189148\n",
            "train epoch: 2 | batch staus: 45440/60000 ( 76%) | Loss:  0.104722\n",
            "train epoch: 2 | batch staus: 46080/60000 ( 77%) | Loss:  0.114789\n",
            "train epoch: 2 | batch staus: 46720/60000 ( 78%) | Loss:  0.059186\n",
            "train epoch: 2 | batch staus: 47360/60000 ( 79%) | Loss:  0.106513\n",
            "train epoch: 2 | batch staus: 48000/60000 ( 80%) | Loss:  0.077261\n",
            "train epoch: 2 | batch staus: 48640/60000 ( 81%) | Loss:  0.100909\n",
            "train epoch: 2 | batch staus: 49280/60000 ( 82%) | Loss:  0.084394\n",
            "train epoch: 2 | batch staus: 49920/60000 ( 83%) | Loss:  0.255938\n",
            "train epoch: 2 | batch staus: 50560/60000 ( 84%) | Loss:  0.075975\n",
            "train epoch: 2 | batch staus: 51200/60000 ( 85%) | Loss:  0.079179\n",
            "train epoch: 2 | batch staus: 51840/60000 ( 86%) | Loss:  0.135522\n",
            "train epoch: 2 | batch staus: 52480/60000 ( 87%) | Loss:  0.031979\n",
            "train epoch: 2 | batch staus: 53120/60000 ( 88%) | Loss:  0.319743\n",
            "train epoch: 2 | batch staus: 53760/60000 ( 90%) | Loss:  0.190666\n",
            "train epoch: 2 | batch staus: 54400/60000 ( 91%) | Loss:  0.080234\n",
            "train epoch: 2 | batch staus: 55040/60000 ( 92%) | Loss:  0.198041\n",
            "train epoch: 2 | batch staus: 55680/60000 ( 93%) | Loss:  0.086562\n",
            "train epoch: 2 | batch staus: 56320/60000 ( 94%) | Loss:  0.076802\n",
            "train epoch: 2 | batch staus: 56960/60000 ( 95%) | Loss:  0.073252\n",
            "train epoch: 2 | batch staus: 57600/60000 ( 96%) | Loss:  0.193674\n",
            "train epoch: 2 | batch staus: 58240/60000 ( 97%) | Loss:  0.089505\n",
            "train epoch: 2 | batch staus: 58880/60000 ( 98%) | Loss:  0.486224\n",
            "train epoch: 2 | batch staus: 59520/60000 ( 99%) | Loss:  0.295055\n",
            "Training time: 1m 22s\n",
            "=======\n",
            " test set: average loss:  0.0018, Accuracy: 9633/10000(96%)\n",
            "Testing time: 1m 27s\n",
            "train epoch: 3 | batch staus: 0/60000 ( 0%) | Loss:  0.147512\n",
            "train epoch: 3 | batch staus: 640/60000 ( 1%) | Loss:  0.100859\n",
            "train epoch: 3 | batch staus: 1280/60000 ( 2%) | Loss:  0.214745\n",
            "train epoch: 3 | batch staus: 1920/60000 ( 3%) | Loss:  0.141601\n",
            "train epoch: 3 | batch staus: 2560/60000 ( 4%) | Loss:  0.260523\n",
            "train epoch: 3 | batch staus: 3200/60000 ( 5%) | Loss:  0.069923\n",
            "train epoch: 3 | batch staus: 3840/60000 ( 6%) | Loss:  0.177044\n",
            "train epoch: 3 | batch staus: 4480/60000 ( 7%) | Loss:  0.073978\n",
            "train epoch: 3 | batch staus: 5120/60000 ( 9%) | Loss:  0.171006\n",
            "train epoch: 3 | batch staus: 5760/60000 ( 10%) | Loss:  0.044992\n",
            "train epoch: 3 | batch staus: 6400/60000 ( 11%) | Loss:  0.233383\n",
            "train epoch: 3 | batch staus: 7040/60000 ( 12%) | Loss:  0.106783\n",
            "train epoch: 3 | batch staus: 7680/60000 ( 13%) | Loss:  0.085445\n",
            "train epoch: 3 | batch staus: 8320/60000 ( 14%) | Loss:  0.166530\n",
            "train epoch: 3 | batch staus: 8960/60000 ( 15%) | Loss:  0.081009\n",
            "train epoch: 3 | batch staus: 9600/60000 ( 16%) | Loss:  0.073370\n",
            "train epoch: 3 | batch staus: 10240/60000 ( 17%) | Loss:  0.139816\n",
            "train epoch: 3 | batch staus: 10880/60000 ( 18%) | Loss:  0.059281\n",
            "train epoch: 3 | batch staus: 11520/60000 ( 19%) | Loss:  0.071027\n",
            "train epoch: 3 | batch staus: 12160/60000 ( 20%) | Loss:  0.039018\n",
            "train epoch: 3 | batch staus: 12800/60000 ( 21%) | Loss:  0.112655\n",
            "train epoch: 3 | batch staus: 13440/60000 ( 22%) | Loss:  0.293424\n",
            "train epoch: 3 | batch staus: 14080/60000 ( 23%) | Loss:  0.055196\n",
            "train epoch: 3 | batch staus: 14720/60000 ( 25%) | Loss:  0.060199\n",
            "train epoch: 3 | batch staus: 15360/60000 ( 26%) | Loss:  0.111745\n",
            "train epoch: 3 | batch staus: 16000/60000 ( 27%) | Loss:  0.123722\n",
            "train epoch: 3 | batch staus: 16640/60000 ( 28%) | Loss:  0.144536\n",
            "train epoch: 3 | batch staus: 17280/60000 ( 29%) | Loss:  0.389437\n",
            "train epoch: 3 | batch staus: 17920/60000 ( 30%) | Loss:  0.075599\n",
            "train epoch: 3 | batch staus: 18560/60000 ( 31%) | Loss:  0.076992\n",
            "train epoch: 3 | batch staus: 19200/60000 ( 32%) | Loss:  0.050791\n",
            "train epoch: 3 | batch staus: 19840/60000 ( 33%) | Loss:  0.086178\n",
            "train epoch: 3 | batch staus: 20480/60000 ( 34%) | Loss:  0.080110\n",
            "train epoch: 3 | batch staus: 21120/60000 ( 35%) | Loss:  0.077994\n",
            "train epoch: 3 | batch staus: 21760/60000 ( 36%) | Loss:  0.067827\n",
            "train epoch: 3 | batch staus: 22400/60000 ( 37%) | Loss:  0.087511\n",
            "train epoch: 3 | batch staus: 23040/60000 ( 38%) | Loss:  0.132479\n",
            "train epoch: 3 | batch staus: 23680/60000 ( 39%) | Loss:  0.129447\n",
            "train epoch: 3 | batch staus: 24320/60000 ( 41%) | Loss:  0.108366\n",
            "train epoch: 3 | batch staus: 24960/60000 ( 42%) | Loss:  0.137064\n",
            "train epoch: 3 | batch staus: 25600/60000 ( 43%) | Loss:  0.151275\n",
            "train epoch: 3 | batch staus: 26240/60000 ( 44%) | Loss:  0.072716\n",
            "train epoch: 3 | batch staus: 26880/60000 ( 45%) | Loss:  0.061758\n",
            "train epoch: 3 | batch staus: 27520/60000 ( 46%) | Loss:  0.033100\n",
            "train epoch: 3 | batch staus: 28160/60000 ( 47%) | Loss:  0.064263\n",
            "train epoch: 3 | batch staus: 28800/60000 ( 48%) | Loss:  0.021154\n",
            "train epoch: 3 | batch staus: 29440/60000 ( 49%) | Loss:  0.102587\n",
            "train epoch: 3 | batch staus: 30080/60000 ( 50%) | Loss:  0.182165\n",
            "train epoch: 3 | batch staus: 30720/60000 ( 51%) | Loss:  0.060597\n",
            "train epoch: 3 | batch staus: 31360/60000 ( 52%) | Loss:  0.157133\n",
            "train epoch: 3 | batch staus: 32000/60000 ( 53%) | Loss:  0.189167\n",
            "train epoch: 3 | batch staus: 32640/60000 ( 54%) | Loss:  0.120197\n",
            "train epoch: 3 | batch staus: 33280/60000 ( 55%) | Loss:  0.036642\n",
            "train epoch: 3 | batch staus: 33920/60000 ( 57%) | Loss:  0.102558\n",
            "train epoch: 3 | batch staus: 34560/60000 ( 58%) | Loss:  0.109992\n",
            "train epoch: 3 | batch staus: 35200/60000 ( 59%) | Loss:  0.123739\n",
            "train epoch: 3 | batch staus: 35840/60000 ( 60%) | Loss:  0.120144\n",
            "train epoch: 3 | batch staus: 36480/60000 ( 61%) | Loss:  0.179608\n",
            "train epoch: 3 | batch staus: 37120/60000 ( 62%) | Loss:  0.125438\n",
            "train epoch: 3 | batch staus: 37760/60000 ( 63%) | Loss:  0.153282\n",
            "train epoch: 3 | batch staus: 38400/60000 ( 64%) | Loss:  0.117154\n",
            "train epoch: 3 | batch staus: 39040/60000 ( 65%) | Loss:  0.101763\n",
            "train epoch: 3 | batch staus: 39680/60000 ( 66%) | Loss:  0.068845\n",
            "train epoch: 3 | batch staus: 40320/60000 ( 67%) | Loss:  0.111663\n",
            "train epoch: 3 | batch staus: 40960/60000 ( 68%) | Loss:  0.229008\n",
            "train epoch: 3 | batch staus: 41600/60000 ( 69%) | Loss:  0.168083\n",
            "train epoch: 3 | batch staus: 42240/60000 ( 70%) | Loss:  0.068641\n",
            "train epoch: 3 | batch staus: 42880/60000 ( 71%) | Loss:  0.044827\n",
            "train epoch: 3 | batch staus: 43520/60000 ( 72%) | Loss:  0.062417\n",
            "train epoch: 3 | batch staus: 44160/60000 ( 74%) | Loss:  0.136062\n",
            "train epoch: 3 | batch staus: 44800/60000 ( 75%) | Loss:  0.061386\n",
            "train epoch: 3 | batch staus: 45440/60000 ( 76%) | Loss:  0.067124\n",
            "train epoch: 3 | batch staus: 46080/60000 ( 77%) | Loss:  0.053924\n",
            "train epoch: 3 | batch staus: 46720/60000 ( 78%) | Loss:  0.038104\n",
            "train epoch: 3 | batch staus: 47360/60000 ( 79%) | Loss:  0.111546\n",
            "train epoch: 3 | batch staus: 48000/60000 ( 80%) | Loss:  0.049476\n",
            "train epoch: 3 | batch staus: 48640/60000 ( 81%) | Loss:  0.080335\n",
            "train epoch: 3 | batch staus: 49280/60000 ( 82%) | Loss:  0.051197\n",
            "train epoch: 3 | batch staus: 49920/60000 ( 83%) | Loss:  0.212168\n",
            "train epoch: 3 | batch staus: 50560/60000 ( 84%) | Loss:  0.045009\n",
            "train epoch: 3 | batch staus: 51200/60000 ( 85%) | Loss:  0.051276\n",
            "train epoch: 3 | batch staus: 51840/60000 ( 86%) | Loss:  0.155314\n",
            "train epoch: 3 | batch staus: 52480/60000 ( 87%) | Loss:  0.369733\n",
            "train epoch: 3 | batch staus: 53120/60000 ( 88%) | Loss:  0.168820\n",
            "train epoch: 3 | batch staus: 53760/60000 ( 90%) | Loss:  0.086381\n",
            "train epoch: 3 | batch staus: 54400/60000 ( 91%) | Loss:  0.192332\n",
            "train epoch: 3 | batch staus: 55040/60000 ( 92%) | Loss:  0.041826\n",
            "train epoch: 3 | batch staus: 55680/60000 ( 93%) | Loss:  0.046360\n",
            "train epoch: 3 | batch staus: 56320/60000 ( 94%) | Loss:  0.065971\n",
            "train epoch: 3 | batch staus: 56960/60000 ( 95%) | Loss:  0.071659\n",
            "train epoch: 3 | batch staus: 57600/60000 ( 96%) | Loss:  0.123869\n",
            "train epoch: 3 | batch staus: 58240/60000 ( 97%) | Loss:  0.015259\n",
            "train epoch: 3 | batch staus: 58880/60000 ( 98%) | Loss:  0.116232\n",
            "train epoch: 3 | batch staus: 59520/60000 ( 99%) | Loss:  0.085741\n",
            "Training time: 1m 22s\n",
            "=======\n",
            " test set: average loss:  0.0012, Accuracy: 9756/10000(98%)\n",
            "Testing time: 1m 27s\n",
            "train epoch: 4 | batch staus: 0/60000 ( 0%) | Loss:  0.246852\n",
            "train epoch: 4 | batch staus: 640/60000 ( 1%) | Loss:  0.067375\n",
            "train epoch: 4 | batch staus: 1280/60000 ( 2%) | Loss:  0.050753\n",
            "train epoch: 4 | batch staus: 1920/60000 ( 3%) | Loss:  0.034642\n",
            "train epoch: 4 | batch staus: 2560/60000 ( 4%) | Loss:  0.023595\n",
            "train epoch: 4 | batch staus: 3200/60000 ( 5%) | Loss:  0.055022\n",
            "train epoch: 4 | batch staus: 3840/60000 ( 6%) | Loss:  0.106585\n",
            "train epoch: 4 | batch staus: 4480/60000 ( 7%) | Loss:  0.152457\n",
            "train epoch: 4 | batch staus: 5120/60000 ( 9%) | Loss:  0.106001\n",
            "train epoch: 4 | batch staus: 5760/60000 ( 10%) | Loss:  0.045723\n",
            "train epoch: 4 | batch staus: 6400/60000 ( 11%) | Loss:  0.227943\n",
            "train epoch: 4 | batch staus: 7040/60000 ( 12%) | Loss:  0.026819\n",
            "train epoch: 4 | batch staus: 7680/60000 ( 13%) | Loss:  0.033093\n",
            "train epoch: 4 | batch staus: 8320/60000 ( 14%) | Loss:  0.030278\n",
            "train epoch: 4 | batch staus: 8960/60000 ( 15%) | Loss:  0.095804\n",
            "train epoch: 4 | batch staus: 9600/60000 ( 16%) | Loss:  0.091203\n",
            "train epoch: 4 | batch staus: 10240/60000 ( 17%) | Loss:  0.067038\n",
            "train epoch: 4 | batch staus: 10880/60000 ( 18%) | Loss:  0.035999\n",
            "train epoch: 4 | batch staus: 11520/60000 ( 19%) | Loss:  0.084504\n",
            "train epoch: 4 | batch staus: 12160/60000 ( 20%) | Loss:  0.124667\n",
            "train epoch: 4 | batch staus: 12800/60000 ( 21%) | Loss:  0.149408\n",
            "train epoch: 4 | batch staus: 13440/60000 ( 22%) | Loss:  0.027270\n",
            "train epoch: 4 | batch staus: 14080/60000 ( 23%) | Loss:  0.099601\n",
            "train epoch: 4 | batch staus: 14720/60000 ( 25%) | Loss:  0.119034\n",
            "train epoch: 4 | batch staus: 15360/60000 ( 26%) | Loss:  0.137778\n",
            "train epoch: 4 | batch staus: 16000/60000 ( 27%) | Loss:  0.027017\n",
            "train epoch: 4 | batch staus: 16640/60000 ( 28%) | Loss:  0.193201\n",
            "train epoch: 4 | batch staus: 17280/60000 ( 29%) | Loss:  0.079289\n",
            "train epoch: 4 | batch staus: 17920/60000 ( 30%) | Loss:  0.049280\n",
            "train epoch: 4 | batch staus: 18560/60000 ( 31%) | Loss:  0.023269\n",
            "train epoch: 4 | batch staus: 19200/60000 ( 32%) | Loss:  0.192273\n",
            "train epoch: 4 | batch staus: 19840/60000 ( 33%) | Loss:  0.077049\n",
            "train epoch: 4 | batch staus: 20480/60000 ( 34%) | Loss:  0.045008\n",
            "train epoch: 4 | batch staus: 21120/60000 ( 35%) | Loss:  0.146504\n",
            "train epoch: 4 | batch staus: 21760/60000 ( 36%) | Loss:  0.093903\n",
            "train epoch: 4 | batch staus: 22400/60000 ( 37%) | Loss:  0.134286\n",
            "train epoch: 4 | batch staus: 23040/60000 ( 38%) | Loss:  0.037879\n",
            "train epoch: 4 | batch staus: 23680/60000 ( 39%) | Loss:  0.117814\n",
            "train epoch: 4 | batch staus: 24320/60000 ( 41%) | Loss:  0.117322\n",
            "train epoch: 4 | batch staus: 24960/60000 ( 42%) | Loss:  0.048480\n",
            "train epoch: 4 | batch staus: 25600/60000 ( 43%) | Loss:  0.066554\n",
            "train epoch: 4 | batch staus: 26240/60000 ( 44%) | Loss:  0.131572\n",
            "train epoch: 4 | batch staus: 26880/60000 ( 45%) | Loss:  0.099088\n",
            "train epoch: 4 | batch staus: 27520/60000 ( 46%) | Loss:  0.011586\n",
            "train epoch: 4 | batch staus: 28160/60000 ( 47%) | Loss:  0.111750\n",
            "train epoch: 4 | batch staus: 28800/60000 ( 48%) | Loss:  0.051811\n",
            "train epoch: 4 | batch staus: 29440/60000 ( 49%) | Loss:  0.072731\n",
            "train epoch: 4 | batch staus: 30080/60000 ( 50%) | Loss:  0.179514\n",
            "train epoch: 4 | batch staus: 30720/60000 ( 51%) | Loss:  0.125767\n",
            "train epoch: 4 | batch staus: 31360/60000 ( 52%) | Loss:  0.070268\n",
            "train epoch: 4 | batch staus: 32000/60000 ( 53%) | Loss:  0.031687\n",
            "train epoch: 4 | batch staus: 32640/60000 ( 54%) | Loss:  0.028854\n",
            "train epoch: 4 | batch staus: 33280/60000 ( 55%) | Loss:  0.096299\n",
            "train epoch: 4 | batch staus: 33920/60000 ( 57%) | Loss:  0.059713\n",
            "train epoch: 4 | batch staus: 34560/60000 ( 58%) | Loss:  0.107462\n",
            "train epoch: 4 | batch staus: 35200/60000 ( 59%) | Loss:  0.032100\n",
            "train epoch: 4 | batch staus: 35840/60000 ( 60%) | Loss:  0.018609\n",
            "train epoch: 4 | batch staus: 36480/60000 ( 61%) | Loss:  0.123889\n",
            "train epoch: 4 | batch staus: 37120/60000 ( 62%) | Loss:  0.114670\n",
            "train epoch: 4 | batch staus: 37760/60000 ( 63%) | Loss:  0.041976\n",
            "train epoch: 4 | batch staus: 38400/60000 ( 64%) | Loss:  0.213359\n",
            "train epoch: 4 | batch staus: 39040/60000 ( 65%) | Loss:  0.054174\n",
            "train epoch: 4 | batch staus: 39680/60000 ( 66%) | Loss:  0.036317\n",
            "train epoch: 4 | batch staus: 40320/60000 ( 67%) | Loss:  0.062853\n",
            "train epoch: 4 | batch staus: 40960/60000 ( 68%) | Loss:  0.023730\n",
            "train epoch: 4 | batch staus: 41600/60000 ( 69%) | Loss:  0.018534\n",
            "train epoch: 4 | batch staus: 42240/60000 ( 70%) | Loss:  0.031110\n",
            "train epoch: 4 | batch staus: 42880/60000 ( 71%) | Loss:  0.055583\n",
            "train epoch: 4 | batch staus: 43520/60000 ( 72%) | Loss:  0.083198\n",
            "train epoch: 4 | batch staus: 44160/60000 ( 74%) | Loss:  0.284656\n",
            "train epoch: 4 | batch staus: 44800/60000 ( 75%) | Loss:  0.028623\n",
            "train epoch: 4 | batch staus: 45440/60000 ( 76%) | Loss:  0.080300\n",
            "train epoch: 4 | batch staus: 46080/60000 ( 77%) | Loss:  0.119163\n",
            "train epoch: 4 | batch staus: 46720/60000 ( 78%) | Loss:  0.177688\n",
            "train epoch: 4 | batch staus: 47360/60000 ( 79%) | Loss:  0.085400\n",
            "train epoch: 4 | batch staus: 48000/60000 ( 80%) | Loss:  0.072954\n",
            "train epoch: 4 | batch staus: 48640/60000 ( 81%) | Loss:  0.087820\n",
            "train epoch: 4 | batch staus: 49280/60000 ( 82%) | Loss:  0.053399\n",
            "train epoch: 4 | batch staus: 49920/60000 ( 83%) | Loss:  0.048149\n",
            "train epoch: 4 | batch staus: 50560/60000 ( 84%) | Loss:  0.038676\n",
            "train epoch: 4 | batch staus: 51200/60000 ( 85%) | Loss:  0.090339\n",
            "train epoch: 4 | batch staus: 51840/60000 ( 86%) | Loss:  0.074118\n",
            "train epoch: 4 | batch staus: 52480/60000 ( 87%) | Loss:  0.082681\n",
            "train epoch: 4 | batch staus: 53120/60000 ( 88%) | Loss:  0.100020\n",
            "train epoch: 4 | batch staus: 53760/60000 ( 90%) | Loss:  0.142496\n",
            "train epoch: 4 | batch staus: 54400/60000 ( 91%) | Loss:  0.013986\n",
            "train epoch: 4 | batch staus: 55040/60000 ( 92%) | Loss:  0.084734\n",
            "train epoch: 4 | batch staus: 55680/60000 ( 93%) | Loss:  0.037417\n",
            "train epoch: 4 | batch staus: 56320/60000 ( 94%) | Loss:  0.022852\n",
            "train epoch: 4 | batch staus: 56960/60000 ( 95%) | Loss:  0.082745\n",
            "train epoch: 4 | batch staus: 57600/60000 ( 96%) | Loss:  0.019911\n",
            "train epoch: 4 | batch staus: 58240/60000 ( 97%) | Loss:  0.020169\n",
            "train epoch: 4 | batch staus: 58880/60000 ( 98%) | Loss:  0.046787\n",
            "train epoch: 4 | batch staus: 59520/60000 ( 99%) | Loss:  0.159673\n",
            "Training time: 1m 22s\n",
            "=======\n",
            " test set: average loss:  0.0010, Accuracy: 9789/10000(98%)\n",
            "Testing time: 1m 27s\n",
            "train epoch: 5 | batch staus: 0/60000 ( 0%) | Loss:  0.048906\n",
            "train epoch: 5 | batch staus: 640/60000 ( 1%) | Loss:  0.104205\n",
            "train epoch: 5 | batch staus: 1280/60000 ( 2%) | Loss:  0.007907\n",
            "train epoch: 5 | batch staus: 1920/60000 ( 3%) | Loss:  0.041515\n",
            "train epoch: 5 | batch staus: 2560/60000 ( 4%) | Loss:  0.194453\n",
            "train epoch: 5 | batch staus: 3200/60000 ( 5%) | Loss:  0.016398\n",
            "train epoch: 5 | batch staus: 3840/60000 ( 6%) | Loss:  0.065432\n",
            "train epoch: 5 | batch staus: 4480/60000 ( 7%) | Loss:  0.138847\n",
            "train epoch: 5 | batch staus: 5120/60000 ( 9%) | Loss:  0.044577\n",
            "train epoch: 5 | batch staus: 5760/60000 ( 10%) | Loss:  0.125808\n",
            "train epoch: 5 | batch staus: 6400/60000 ( 11%) | Loss:  0.037026\n",
            "train epoch: 5 | batch staus: 7040/60000 ( 12%) | Loss:  0.051007\n",
            "train epoch: 5 | batch staus: 7680/60000 ( 13%) | Loss:  0.048576\n",
            "train epoch: 5 | batch staus: 8320/60000 ( 14%) | Loss:  0.126670\n",
            "train epoch: 5 | batch staus: 8960/60000 ( 15%) | Loss:  0.049518\n",
            "train epoch: 5 | batch staus: 9600/60000 ( 16%) | Loss:  0.027128\n",
            "train epoch: 5 | batch staus: 10240/60000 ( 17%) | Loss:  0.050452\n",
            "train epoch: 5 | batch staus: 10880/60000 ( 18%) | Loss:  0.030978\n",
            "train epoch: 5 | batch staus: 11520/60000 ( 19%) | Loss:  0.129528\n",
            "train epoch: 5 | batch staus: 12160/60000 ( 20%) | Loss:  0.155603\n",
            "train epoch: 5 | batch staus: 12800/60000 ( 21%) | Loss:  0.086787\n",
            "train epoch: 5 | batch staus: 13440/60000 ( 22%) | Loss:  0.079830\n",
            "train epoch: 5 | batch staus: 14080/60000 ( 23%) | Loss:  0.198802\n",
            "train epoch: 5 | batch staus: 14720/60000 ( 25%) | Loss:  0.191795\n",
            "train epoch: 5 | batch staus: 15360/60000 ( 26%) | Loss:  0.255341\n",
            "train epoch: 5 | batch staus: 16000/60000 ( 27%) | Loss:  0.048025\n",
            "train epoch: 5 | batch staus: 16640/60000 ( 28%) | Loss:  0.020202\n",
            "train epoch: 5 | batch staus: 17280/60000 ( 29%) | Loss:  0.117935\n",
            "train epoch: 5 | batch staus: 17920/60000 ( 30%) | Loss:  0.075620\n",
            "train epoch: 5 | batch staus: 18560/60000 ( 31%) | Loss:  0.090355\n",
            "train epoch: 5 | batch staus: 19200/60000 ( 32%) | Loss:  0.012449\n",
            "train epoch: 5 | batch staus: 19840/60000 ( 33%) | Loss:  0.191700\n",
            "train epoch: 5 | batch staus: 20480/60000 ( 34%) | Loss:  0.112960\n",
            "train epoch: 5 | batch staus: 21120/60000 ( 35%) | Loss:  0.107112\n",
            "train epoch: 5 | batch staus: 21760/60000 ( 36%) | Loss:  0.091421\n",
            "train epoch: 5 | batch staus: 22400/60000 ( 37%) | Loss:  0.126395\n",
            "train epoch: 5 | batch staus: 23040/60000 ( 38%) | Loss:  0.202931\n",
            "train epoch: 5 | batch staus: 23680/60000 ( 39%) | Loss:  0.061548\n",
            "train epoch: 5 | batch staus: 24320/60000 ( 41%) | Loss:  0.012097\n",
            "train epoch: 5 | batch staus: 24960/60000 ( 42%) | Loss:  0.096429\n",
            "train epoch: 5 | batch staus: 25600/60000 ( 43%) | Loss:  0.083108\n",
            "train epoch: 5 | batch staus: 26240/60000 ( 44%) | Loss:  0.161082\n",
            "train epoch: 5 | batch staus: 26880/60000 ( 45%) | Loss:  0.086868\n",
            "train epoch: 5 | batch staus: 27520/60000 ( 46%) | Loss:  0.101317\n",
            "train epoch: 5 | batch staus: 28160/60000 ( 47%) | Loss:  0.041864\n",
            "train epoch: 5 | batch staus: 28800/60000 ( 48%) | Loss:  0.067392\n",
            "train epoch: 5 | batch staus: 29440/60000 ( 49%) | Loss:  0.078673\n",
            "train epoch: 5 | batch staus: 30080/60000 ( 50%) | Loss:  0.140949\n",
            "train epoch: 5 | batch staus: 30720/60000 ( 51%) | Loss:  0.058222\n",
            "train epoch: 5 | batch staus: 31360/60000 ( 52%) | Loss:  0.099599\n",
            "train epoch: 5 | batch staus: 32000/60000 ( 53%) | Loss:  0.016954\n",
            "train epoch: 5 | batch staus: 32640/60000 ( 54%) | Loss:  0.016162\n",
            "train epoch: 5 | batch staus: 33280/60000 ( 55%) | Loss:  0.037920\n",
            "train epoch: 5 | batch staus: 33920/60000 ( 57%) | Loss:  0.089145\n",
            "train epoch: 5 | batch staus: 34560/60000 ( 58%) | Loss:  0.105056\n",
            "train epoch: 5 | batch staus: 35200/60000 ( 59%) | Loss:  0.008957\n",
            "train epoch: 5 | batch staus: 35840/60000 ( 60%) | Loss:  0.018884\n",
            "train epoch: 5 | batch staus: 36480/60000 ( 61%) | Loss:  0.087140\n",
            "train epoch: 5 | batch staus: 37120/60000 ( 62%) | Loss:  0.217851\n",
            "train epoch: 5 | batch staus: 37760/60000 ( 63%) | Loss:  0.082215\n",
            "train epoch: 5 | batch staus: 38400/60000 ( 64%) | Loss:  0.075670\n",
            "train epoch: 5 | batch staus: 39040/60000 ( 65%) | Loss:  0.044261\n",
            "train epoch: 5 | batch staus: 39680/60000 ( 66%) | Loss:  0.035043\n",
            "train epoch: 5 | batch staus: 40320/60000 ( 67%) | Loss:  0.033390\n",
            "train epoch: 5 | batch staus: 40960/60000 ( 68%) | Loss:  0.050680\n",
            "train epoch: 5 | batch staus: 41600/60000 ( 69%) | Loss:  0.100054\n",
            "train epoch: 5 | batch staus: 42240/60000 ( 70%) | Loss:  0.192003\n",
            "train epoch: 5 | batch staus: 42880/60000 ( 71%) | Loss:  0.022586\n",
            "train epoch: 5 | batch staus: 43520/60000 ( 72%) | Loss:  0.109674\n",
            "train epoch: 5 | batch staus: 44160/60000 ( 74%) | Loss:  0.017565\n",
            "train epoch: 5 | batch staus: 44800/60000 ( 75%) | Loss:  0.083703\n",
            "train epoch: 5 | batch staus: 45440/60000 ( 76%) | Loss:  0.041425\n",
            "train epoch: 5 | batch staus: 46080/60000 ( 77%) | Loss:  0.107473\n",
            "train epoch: 5 | batch staus: 46720/60000 ( 78%) | Loss:  0.183111\n",
            "train epoch: 5 | batch staus: 47360/60000 ( 79%) | Loss:  0.024136\n",
            "train epoch: 5 | batch staus: 48000/60000 ( 80%) | Loss:  0.136712\n",
            "train epoch: 5 | batch staus: 48640/60000 ( 81%) | Loss:  0.021436\n",
            "train epoch: 5 | batch staus: 49280/60000 ( 82%) | Loss:  0.142014\n",
            "train epoch: 5 | batch staus: 49920/60000 ( 83%) | Loss:  0.027324\n",
            "train epoch: 5 | batch staus: 50560/60000 ( 84%) | Loss:  0.061449\n",
            "train epoch: 5 | batch staus: 51200/60000 ( 85%) | Loss:  0.036891\n",
            "train epoch: 5 | batch staus: 51840/60000 ( 86%) | Loss:  0.026473\n",
            "train epoch: 5 | batch staus: 52480/60000 ( 87%) | Loss:  0.048855\n",
            "train epoch: 5 | batch staus: 53120/60000 ( 88%) | Loss:  0.025576\n",
            "train epoch: 5 | batch staus: 53760/60000 ( 90%) | Loss:  0.033792\n",
            "train epoch: 5 | batch staus: 54400/60000 ( 91%) | Loss:  0.093332\n",
            "train epoch: 5 | batch staus: 55040/60000 ( 92%) | Loss:  0.016420\n",
            "train epoch: 5 | batch staus: 55680/60000 ( 93%) | Loss:  0.044006\n",
            "train epoch: 5 | batch staus: 56320/60000 ( 94%) | Loss:  0.105098\n",
            "train epoch: 5 | batch staus: 56960/60000 ( 95%) | Loss:  0.086969\n",
            "train epoch: 5 | batch staus: 57600/60000 ( 96%) | Loss:  0.027489\n",
            "train epoch: 5 | batch staus: 58240/60000 ( 97%) | Loss:  0.012880\n",
            "train epoch: 5 | batch staus: 58880/60000 ( 98%) | Loss:  0.240315\n",
            "train epoch: 5 | batch staus: 59520/60000 ( 99%) | Loss:  0.181499\n",
            "Training time: 1m 52s\n",
            "=======\n",
            " test set: average loss:  0.0009, Accuracy: 9823/10000(98%)\n",
            "Testing time: 1m 59s\n",
            "train epoch: 6 | batch staus: 0/60000 ( 0%) | Loss:  0.043800\n",
            "train epoch: 6 | batch staus: 640/60000 ( 1%) | Loss:  0.008749\n",
            "train epoch: 6 | batch staus: 1280/60000 ( 2%) | Loss:  0.109803\n",
            "train epoch: 6 | batch staus: 1920/60000 ( 3%) | Loss:  0.037297\n",
            "train epoch: 6 | batch staus: 2560/60000 ( 4%) | Loss:  0.030756\n",
            "train epoch: 6 | batch staus: 3200/60000 ( 5%) | Loss:  0.111268\n",
            "train epoch: 6 | batch staus: 3840/60000 ( 6%) | Loss:  0.116427\n",
            "train epoch: 6 | batch staus: 4480/60000 ( 7%) | Loss:  0.007067\n",
            "train epoch: 6 | batch staus: 5120/60000 ( 9%) | Loss:  0.048684\n",
            "train epoch: 6 | batch staus: 5760/60000 ( 10%) | Loss:  0.058142\n",
            "train epoch: 6 | batch staus: 6400/60000 ( 11%) | Loss:  0.085759\n",
            "train epoch: 6 | batch staus: 7040/60000 ( 12%) | Loss:  0.028906\n",
            "train epoch: 6 | batch staus: 7680/60000 ( 13%) | Loss:  0.022867\n",
            "train epoch: 6 | batch staus: 8320/60000 ( 14%) | Loss:  0.039495\n",
            "train epoch: 6 | batch staus: 8960/60000 ( 15%) | Loss:  0.052197\n",
            "train epoch: 6 | batch staus: 9600/60000 ( 16%) | Loss:  0.031081\n",
            "train epoch: 6 | batch staus: 10240/60000 ( 17%) | Loss:  0.197817\n",
            "train epoch: 6 | batch staus: 10880/60000 ( 18%) | Loss:  0.102693\n",
            "train epoch: 6 | batch staus: 11520/60000 ( 19%) | Loss:  0.070401\n",
            "train epoch: 6 | batch staus: 12160/60000 ( 20%) | Loss:  0.016265\n",
            "train epoch: 6 | batch staus: 12800/60000 ( 21%) | Loss:  0.054815\n",
            "train epoch: 6 | batch staus: 13440/60000 ( 22%) | Loss:  0.067898\n",
            "train epoch: 6 | batch staus: 14080/60000 ( 23%) | Loss:  0.082958\n",
            "train epoch: 6 | batch staus: 14720/60000 ( 25%) | Loss:  0.111398\n",
            "train epoch: 6 | batch staus: 15360/60000 ( 26%) | Loss:  0.019630\n",
            "train epoch: 6 | batch staus: 16000/60000 ( 27%) | Loss:  0.064290\n",
            "train epoch: 6 | batch staus: 16640/60000 ( 28%) | Loss:  0.014645\n",
            "train epoch: 6 | batch staus: 17280/60000 ( 29%) | Loss:  0.100831\n",
            "train epoch: 6 | batch staus: 17920/60000 ( 30%) | Loss:  0.026909\n",
            "train epoch: 6 | batch staus: 18560/60000 ( 31%) | Loss:  0.052903\n",
            "train epoch: 6 | batch staus: 19200/60000 ( 32%) | Loss:  0.005456\n",
            "train epoch: 6 | batch staus: 19840/60000 ( 33%) | Loss:  0.070865\n",
            "train epoch: 6 | batch staus: 20480/60000 ( 34%) | Loss:  0.039480\n",
            "train epoch: 6 | batch staus: 21120/60000 ( 35%) | Loss:  0.052949\n",
            "train epoch: 6 | batch staus: 21760/60000 ( 36%) | Loss:  0.043910\n",
            "train epoch: 6 | batch staus: 22400/60000 ( 37%) | Loss:  0.048107\n",
            "train epoch: 6 | batch staus: 23040/60000 ( 38%) | Loss:  0.051029\n",
            "train epoch: 6 | batch staus: 23680/60000 ( 39%) | Loss:  0.039439\n",
            "train epoch: 6 | batch staus: 24320/60000 ( 41%) | Loss:  0.031831\n",
            "train epoch: 6 | batch staus: 24960/60000 ( 42%) | Loss:  0.086785\n",
            "train epoch: 6 | batch staus: 25600/60000 ( 43%) | Loss:  0.040784\n",
            "train epoch: 6 | batch staus: 26240/60000 ( 44%) | Loss:  0.135765\n",
            "train epoch: 6 | batch staus: 26880/60000 ( 45%) | Loss:  0.091277\n",
            "train epoch: 6 | batch staus: 27520/60000 ( 46%) | Loss:  0.221457\n",
            "train epoch: 6 | batch staus: 28160/60000 ( 47%) | Loss:  0.122418\n",
            "train epoch: 6 | batch staus: 28800/60000 ( 48%) | Loss:  0.090210\n",
            "train epoch: 6 | batch staus: 29440/60000 ( 49%) | Loss:  0.005746\n",
            "train epoch: 6 | batch staus: 30080/60000 ( 50%) | Loss:  0.107994\n",
            "train epoch: 6 | batch staus: 30720/60000 ( 51%) | Loss:  0.066437\n",
            "train epoch: 6 | batch staus: 31360/60000 ( 52%) | Loss:  0.078523\n",
            "train epoch: 6 | batch staus: 32000/60000 ( 53%) | Loss:  0.049334\n",
            "train epoch: 6 | batch staus: 32640/60000 ( 54%) | Loss:  0.064282\n",
            "train epoch: 6 | batch staus: 33280/60000 ( 55%) | Loss:  0.088264\n",
            "train epoch: 6 | batch staus: 33920/60000 ( 57%) | Loss:  0.196559\n",
            "train epoch: 6 | batch staus: 34560/60000 ( 58%) | Loss:  0.109462\n",
            "train epoch: 6 | batch staus: 35200/60000 ( 59%) | Loss:  0.027485\n",
            "train epoch: 6 | batch staus: 35840/60000 ( 60%) | Loss:  0.064202\n",
            "train epoch: 6 | batch staus: 36480/60000 ( 61%) | Loss:  0.143756\n",
            "train epoch: 6 | batch staus: 37120/60000 ( 62%) | Loss:  0.042880\n",
            "train epoch: 6 | batch staus: 37760/60000 ( 63%) | Loss:  0.165498\n",
            "train epoch: 6 | batch staus: 38400/60000 ( 64%) | Loss:  0.104084\n",
            "train epoch: 6 | batch staus: 39040/60000 ( 65%) | Loss:  0.060990\n",
            "train epoch: 6 | batch staus: 39680/60000 ( 66%) | Loss:  0.071316\n",
            "train epoch: 6 | batch staus: 40320/60000 ( 67%) | Loss:  0.027131\n",
            "train epoch: 6 | batch staus: 40960/60000 ( 68%) | Loss:  0.051759\n",
            "train epoch: 6 | batch staus: 41600/60000 ( 69%) | Loss:  0.144619\n",
            "train epoch: 6 | batch staus: 42240/60000 ( 70%) | Loss:  0.029271\n",
            "train epoch: 6 | batch staus: 42880/60000 ( 71%) | Loss:  0.016439\n",
            "train epoch: 6 | batch staus: 43520/60000 ( 72%) | Loss:  0.024337\n",
            "train epoch: 6 | batch staus: 44160/60000 ( 74%) | Loss:  0.031261\n",
            "train epoch: 6 | batch staus: 44800/60000 ( 75%) | Loss:  0.043550\n",
            "train epoch: 6 | batch staus: 45440/60000 ( 76%) | Loss:  0.144649\n",
            "train epoch: 6 | batch staus: 46080/60000 ( 77%) | Loss:  0.099876\n",
            "train epoch: 6 | batch staus: 46720/60000 ( 78%) | Loss:  0.168098\n",
            "train epoch: 6 | batch staus: 47360/60000 ( 79%) | Loss:  0.050436\n",
            "train epoch: 6 | batch staus: 48000/60000 ( 80%) | Loss:  0.087972\n",
            "train epoch: 6 | batch staus: 48640/60000 ( 81%) | Loss:  0.042514\n",
            "train epoch: 6 | batch staus: 49280/60000 ( 82%) | Loss:  0.041065\n",
            "train epoch: 6 | batch staus: 49920/60000 ( 83%) | Loss:  0.130069\n",
            "train epoch: 6 | batch staus: 50560/60000 ( 84%) | Loss:  0.091975\n",
            "train epoch: 6 | batch staus: 51200/60000 ( 85%) | Loss:  0.117388\n",
            "train epoch: 6 | batch staus: 51840/60000 ( 86%) | Loss:  0.022562\n",
            "train epoch: 6 | batch staus: 52480/60000 ( 87%) | Loss:  0.137802\n",
            "train epoch: 6 | batch staus: 53120/60000 ( 88%) | Loss:  0.102187\n",
            "train epoch: 6 | batch staus: 53760/60000 ( 90%) | Loss:  0.034049\n",
            "train epoch: 6 | batch staus: 54400/60000 ( 91%) | Loss:  0.174021\n",
            "train epoch: 6 | batch staus: 55040/60000 ( 92%) | Loss:  0.056918\n",
            "train epoch: 6 | batch staus: 55680/60000 ( 93%) | Loss:  0.118770\n",
            "train epoch: 6 | batch staus: 56320/60000 ( 94%) | Loss:  0.015719\n",
            "train epoch: 6 | batch staus: 56960/60000 ( 95%) | Loss:  0.127777\n",
            "train epoch: 6 | batch staus: 57600/60000 ( 96%) | Loss:  0.186531\n",
            "train epoch: 6 | batch staus: 58240/60000 ( 97%) | Loss:  0.060433\n",
            "train epoch: 6 | batch staus: 58880/60000 ( 98%) | Loss:  0.014514\n",
            "train epoch: 6 | batch staus: 59520/60000 ( 99%) | Loss:  0.008585\n",
            "Training time: 1m 48s\n",
            "=======\n",
            " test set: average loss:  0.0009, Accuracy: 9817/10000(98%)\n",
            "Testing time: 1m 55s\n",
            "train epoch: 7 | batch staus: 0/60000 ( 0%) | Loss:  0.021474\n",
            "train epoch: 7 | batch staus: 640/60000 ( 1%) | Loss:  0.035382\n",
            "train epoch: 7 | batch staus: 1280/60000 ( 2%) | Loss:  0.027582\n",
            "train epoch: 7 | batch staus: 1920/60000 ( 3%) | Loss:  0.006392\n",
            "train epoch: 7 | batch staus: 2560/60000 ( 4%) | Loss:  0.040353\n",
            "train epoch: 7 | batch staus: 3200/60000 ( 5%) | Loss:  0.562824\n",
            "train epoch: 7 | batch staus: 3840/60000 ( 6%) | Loss:  0.023210\n",
            "train epoch: 7 | batch staus: 4480/60000 ( 7%) | Loss:  0.044908\n",
            "train epoch: 7 | batch staus: 5120/60000 ( 9%) | Loss:  0.033613\n",
            "train epoch: 7 | batch staus: 5760/60000 ( 10%) | Loss:  0.039496\n",
            "train epoch: 7 | batch staus: 6400/60000 ( 11%) | Loss:  0.069957\n",
            "train epoch: 7 | batch staus: 7040/60000 ( 12%) | Loss:  0.051208\n",
            "train epoch: 7 | batch staus: 7680/60000 ( 13%) | Loss:  0.043778\n",
            "train epoch: 7 | batch staus: 8320/60000 ( 14%) | Loss:  0.010409\n",
            "train epoch: 7 | batch staus: 8960/60000 ( 15%) | Loss:  0.017581\n",
            "train epoch: 7 | batch staus: 9600/60000 ( 16%) | Loss:  0.019415\n",
            "train epoch: 7 | batch staus: 10240/60000 ( 17%) | Loss:  0.080294\n",
            "train epoch: 7 | batch staus: 10880/60000 ( 18%) | Loss:  0.035890\n",
            "train epoch: 7 | batch staus: 11520/60000 ( 19%) | Loss:  0.054281\n",
            "train epoch: 7 | batch staus: 12160/60000 ( 20%) | Loss:  0.014513\n",
            "train epoch: 7 | batch staus: 12800/60000 ( 21%) | Loss:  0.132591\n",
            "train epoch: 7 | batch staus: 13440/60000 ( 22%) | Loss:  0.042430\n",
            "train epoch: 7 | batch staus: 14080/60000 ( 23%) | Loss:  0.009086\n",
            "train epoch: 7 | batch staus: 14720/60000 ( 25%) | Loss:  0.031751\n",
            "train epoch: 7 | batch staus: 15360/60000 ( 26%) | Loss:  0.019518\n",
            "train epoch: 7 | batch staus: 16000/60000 ( 27%) | Loss:  0.078710\n",
            "train epoch: 7 | batch staus: 16640/60000 ( 28%) | Loss:  0.020455\n",
            "train epoch: 7 | batch staus: 17280/60000 ( 29%) | Loss:  0.032477\n",
            "train epoch: 7 | batch staus: 17920/60000 ( 30%) | Loss:  0.124917\n",
            "train epoch: 7 | batch staus: 18560/60000 ( 31%) | Loss:  0.051458\n",
            "train epoch: 7 | batch staus: 19200/60000 ( 32%) | Loss:  0.014878\n",
            "train epoch: 7 | batch staus: 19840/60000 ( 33%) | Loss:  0.054020\n",
            "train epoch: 7 | batch staus: 20480/60000 ( 34%) | Loss:  0.137015\n",
            "train epoch: 7 | batch staus: 21120/60000 ( 35%) | Loss:  0.195479\n",
            "train epoch: 7 | batch staus: 21760/60000 ( 36%) | Loss:  0.018980\n",
            "train epoch: 7 | batch staus: 22400/60000 ( 37%) | Loss:  0.015181\n",
            "train epoch: 7 | batch staus: 23040/60000 ( 38%) | Loss:  0.044952\n",
            "train epoch: 7 | batch staus: 23680/60000 ( 39%) | Loss:  0.040824\n",
            "train epoch: 7 | batch staus: 24320/60000 ( 41%) | Loss:  0.028306\n",
            "train epoch: 7 | batch staus: 24960/60000 ( 42%) | Loss:  0.065909\n",
            "train epoch: 7 | batch staus: 25600/60000 ( 43%) | Loss:  0.104452\n",
            "train epoch: 7 | batch staus: 26240/60000 ( 44%) | Loss:  0.016755\n",
            "train epoch: 7 | batch staus: 26880/60000 ( 45%) | Loss:  0.069514\n",
            "train epoch: 7 | batch staus: 27520/60000 ( 46%) | Loss:  0.014033\n",
            "train epoch: 7 | batch staus: 28160/60000 ( 47%) | Loss:  0.047129\n",
            "train epoch: 7 | batch staus: 28800/60000 ( 48%) | Loss:  0.057454\n",
            "train epoch: 7 | batch staus: 29440/60000 ( 49%) | Loss:  0.158793\n",
            "train epoch: 7 | batch staus: 30080/60000 ( 50%) | Loss:  0.121859\n",
            "train epoch: 7 | batch staus: 30720/60000 ( 51%) | Loss:  0.070346\n",
            "train epoch: 7 | batch staus: 31360/60000 ( 52%) | Loss:  0.045230\n",
            "train epoch: 7 | batch staus: 32000/60000 ( 53%) | Loss:  0.091953\n",
            "train epoch: 7 | batch staus: 32640/60000 ( 54%) | Loss:  0.059160\n",
            "train epoch: 7 | batch staus: 33280/60000 ( 55%) | Loss:  0.012657\n",
            "train epoch: 7 | batch staus: 33920/60000 ( 57%) | Loss:  0.061969\n",
            "train epoch: 7 | batch staus: 34560/60000 ( 58%) | Loss:  0.026276\n",
            "train epoch: 7 | batch staus: 35200/60000 ( 59%) | Loss:  0.015552\n",
            "train epoch: 7 | batch staus: 35840/60000 ( 60%) | Loss:  0.011401\n",
            "train epoch: 7 | batch staus: 36480/60000 ( 61%) | Loss:  0.107661\n",
            "train epoch: 7 | batch staus: 37120/60000 ( 62%) | Loss:  0.134129\n",
            "train epoch: 7 | batch staus: 37760/60000 ( 63%) | Loss:  0.045030\n",
            "train epoch: 7 | batch staus: 38400/60000 ( 64%) | Loss:  0.034672\n",
            "train epoch: 7 | batch staus: 39040/60000 ( 65%) | Loss:  0.033251\n",
            "train epoch: 7 | batch staus: 39680/60000 ( 66%) | Loss:  0.138618\n",
            "train epoch: 7 | batch staus: 40320/60000 ( 67%) | Loss:  0.021493\n",
            "train epoch: 7 | batch staus: 40960/60000 ( 68%) | Loss:  0.025516\n",
            "train epoch: 7 | batch staus: 41600/60000 ( 69%) | Loss:  0.058284\n",
            "train epoch: 7 | batch staus: 42240/60000 ( 70%) | Loss:  0.051146\n",
            "train epoch: 7 | batch staus: 42880/60000 ( 71%) | Loss:  0.039047\n",
            "train epoch: 7 | batch staus: 43520/60000 ( 72%) | Loss:  0.066613\n",
            "train epoch: 7 | batch staus: 44160/60000 ( 74%) | Loss:  0.019797\n",
            "train epoch: 7 | batch staus: 44800/60000 ( 75%) | Loss:  0.165224\n",
            "train epoch: 7 | batch staus: 45440/60000 ( 76%) | Loss:  0.052034\n",
            "train epoch: 7 | batch staus: 46080/60000 ( 77%) | Loss:  0.106239\n",
            "train epoch: 7 | batch staus: 46720/60000 ( 78%) | Loss:  0.017508\n",
            "train epoch: 7 | batch staus: 47360/60000 ( 79%) | Loss:  0.015801\n",
            "train epoch: 7 | batch staus: 48000/60000 ( 80%) | Loss:  0.042726\n",
            "train epoch: 7 | batch staus: 48640/60000 ( 81%) | Loss:  0.039000\n",
            "train epoch: 7 | batch staus: 49280/60000 ( 82%) | Loss:  0.029850\n",
            "train epoch: 7 | batch staus: 49920/60000 ( 83%) | Loss:  0.015662\n",
            "train epoch: 7 | batch staus: 50560/60000 ( 84%) | Loss:  0.097070\n",
            "train epoch: 7 | batch staus: 51200/60000 ( 85%) | Loss:  0.041196\n",
            "train epoch: 7 | batch staus: 51840/60000 ( 86%) | Loss:  0.005175\n",
            "train epoch: 7 | batch staus: 52480/60000 ( 87%) | Loss:  0.090999\n",
            "train epoch: 7 | batch staus: 53120/60000 ( 88%) | Loss:  0.091300\n",
            "train epoch: 7 | batch staus: 53760/60000 ( 90%) | Loss:  0.026043\n",
            "train epoch: 7 | batch staus: 54400/60000 ( 91%) | Loss:  0.022568\n",
            "train epoch: 7 | batch staus: 55040/60000 ( 92%) | Loss:  0.070964\n",
            "train epoch: 7 | batch staus: 55680/60000 ( 93%) | Loss:  0.194551\n",
            "train epoch: 7 | batch staus: 56320/60000 ( 94%) | Loss:  0.057277\n",
            "train epoch: 7 | batch staus: 56960/60000 ( 95%) | Loss:  0.065194\n",
            "train epoch: 7 | batch staus: 57600/60000 ( 96%) | Loss:  0.043067\n",
            "train epoch: 7 | batch staus: 58240/60000 ( 97%) | Loss:  0.018609\n",
            "train epoch: 7 | batch staus: 58880/60000 ( 98%) | Loss:  0.071965\n",
            "train epoch: 7 | batch staus: 59520/60000 ( 99%) | Loss:  0.034831\n",
            "Training time: 1m 49s\n",
            "=======\n",
            " test set: average loss:  0.0008, Accuracy: 9818/10000(98%)\n",
            "Testing time: 1m 55s\n",
            "train epoch: 8 | batch staus: 0/60000 ( 0%) | Loss:  0.030430\n",
            "train epoch: 8 | batch staus: 640/60000 ( 1%) | Loss:  0.035781\n",
            "train epoch: 8 | batch staus: 1280/60000 ( 2%) | Loss:  0.078741\n",
            "train epoch: 8 | batch staus: 1920/60000 ( 3%) | Loss:  0.049557\n",
            "train epoch: 8 | batch staus: 2560/60000 ( 4%) | Loss:  0.009770\n",
            "train epoch: 8 | batch staus: 3200/60000 ( 5%) | Loss:  0.073834\n",
            "train epoch: 8 | batch staus: 3840/60000 ( 6%) | Loss:  0.056622\n",
            "train epoch: 8 | batch staus: 4480/60000 ( 7%) | Loss:  0.021266\n",
            "train epoch: 8 | batch staus: 5120/60000 ( 9%) | Loss:  0.115203\n",
            "train epoch: 8 | batch staus: 5760/60000 ( 10%) | Loss:  0.043483\n",
            "train epoch: 8 | batch staus: 6400/60000 ( 11%) | Loss:  0.067259\n",
            "train epoch: 8 | batch staus: 7040/60000 ( 12%) | Loss:  0.073803\n",
            "train epoch: 8 | batch staus: 7680/60000 ( 13%) | Loss:  0.035250\n",
            "train epoch: 8 | batch staus: 8320/60000 ( 14%) | Loss:  0.074633\n",
            "train epoch: 8 | batch staus: 8960/60000 ( 15%) | Loss:  0.055085\n",
            "train epoch: 8 | batch staus: 9600/60000 ( 16%) | Loss:  0.024472\n",
            "train epoch: 8 | batch staus: 10240/60000 ( 17%) | Loss:  0.118256\n",
            "train epoch: 8 | batch staus: 10880/60000 ( 18%) | Loss:  0.016055\n",
            "train epoch: 8 | batch staus: 11520/60000 ( 19%) | Loss:  0.032929\n",
            "train epoch: 8 | batch staus: 12160/60000 ( 20%) | Loss:  0.034750\n",
            "train epoch: 8 | batch staus: 12800/60000 ( 21%) | Loss:  0.148198\n",
            "train epoch: 8 | batch staus: 13440/60000 ( 22%) | Loss:  0.061185\n",
            "train epoch: 8 | batch staus: 14080/60000 ( 23%) | Loss:  0.017500\n",
            "train epoch: 8 | batch staus: 14720/60000 ( 25%) | Loss:  0.040569\n",
            "train epoch: 8 | batch staus: 15360/60000 ( 26%) | Loss:  0.060740\n",
            "train epoch: 8 | batch staus: 16000/60000 ( 27%) | Loss:  0.024981\n",
            "train epoch: 8 | batch staus: 16640/60000 ( 28%) | Loss:  0.014654\n",
            "train epoch: 8 | batch staus: 17280/60000 ( 29%) | Loss:  0.020800\n",
            "train epoch: 8 | batch staus: 17920/60000 ( 30%) | Loss:  0.036437\n",
            "train epoch: 8 | batch staus: 18560/60000 ( 31%) | Loss:  0.007919\n",
            "train epoch: 8 | batch staus: 19200/60000 ( 32%) | Loss:  0.058968\n",
            "train epoch: 8 | batch staus: 19840/60000 ( 33%) | Loss:  0.021635\n",
            "train epoch: 8 | batch staus: 20480/60000 ( 34%) | Loss:  0.052767\n",
            "train epoch: 8 | batch staus: 21120/60000 ( 35%) | Loss:  0.072115\n",
            "train epoch: 8 | batch staus: 21760/60000 ( 36%) | Loss:  0.063864\n",
            "train epoch: 8 | batch staus: 22400/60000 ( 37%) | Loss:  0.061695\n",
            "train epoch: 8 | batch staus: 23040/60000 ( 38%) | Loss:  0.007429\n",
            "train epoch: 8 | batch staus: 23680/60000 ( 39%) | Loss:  0.066367\n",
            "train epoch: 8 | batch staus: 24320/60000 ( 41%) | Loss:  0.062975\n",
            "train epoch: 8 | batch staus: 24960/60000 ( 42%) | Loss:  0.009184\n",
            "train epoch: 8 | batch staus: 25600/60000 ( 43%) | Loss:  0.011873\n",
            "train epoch: 8 | batch staus: 26240/60000 ( 44%) | Loss:  0.037647\n",
            "train epoch: 8 | batch staus: 26880/60000 ( 45%) | Loss:  0.030453\n",
            "train epoch: 8 | batch staus: 27520/60000 ( 46%) | Loss:  0.122210\n",
            "train epoch: 8 | batch staus: 28160/60000 ( 47%) | Loss:  0.005897\n",
            "train epoch: 8 | batch staus: 28800/60000 ( 48%) | Loss:  0.052628\n",
            "train epoch: 8 | batch staus: 29440/60000 ( 49%) | Loss:  0.036775\n",
            "train epoch: 8 | batch staus: 30080/60000 ( 50%) | Loss:  0.034555\n",
            "train epoch: 8 | batch staus: 30720/60000 ( 51%) | Loss:  0.110059\n",
            "train epoch: 8 | batch staus: 31360/60000 ( 52%) | Loss:  0.017365\n",
            "train epoch: 8 | batch staus: 32000/60000 ( 53%) | Loss:  0.032703\n",
            "train epoch: 8 | batch staus: 32640/60000 ( 54%) | Loss:  0.025742\n",
            "train epoch: 8 | batch staus: 33280/60000 ( 55%) | Loss:  0.003341\n",
            "train epoch: 8 | batch staus: 33920/60000 ( 57%) | Loss:  0.015518\n",
            "train epoch: 8 | batch staus: 34560/60000 ( 58%) | Loss:  0.173196\n",
            "train epoch: 8 | batch staus: 35200/60000 ( 59%) | Loss:  0.056462\n",
            "train epoch: 8 | batch staus: 35840/60000 ( 60%) | Loss:  0.043314\n",
            "train epoch: 8 | batch staus: 36480/60000 ( 61%) | Loss:  0.048172\n",
            "train epoch: 8 | batch staus: 37120/60000 ( 62%) | Loss:  0.103575\n",
            "train epoch: 8 | batch staus: 37760/60000 ( 63%) | Loss:  0.009699\n",
            "train epoch: 8 | batch staus: 38400/60000 ( 64%) | Loss:  0.082895\n",
            "train epoch: 8 | batch staus: 39040/60000 ( 65%) | Loss:  0.017458\n",
            "train epoch: 8 | batch staus: 39680/60000 ( 66%) | Loss:  0.062091\n",
            "train epoch: 8 | batch staus: 40320/60000 ( 67%) | Loss:  0.024617\n",
            "train epoch: 8 | batch staus: 40960/60000 ( 68%) | Loss:  0.119420\n",
            "train epoch: 8 | batch staus: 41600/60000 ( 69%) | Loss:  0.100554\n",
            "train epoch: 8 | batch staus: 42240/60000 ( 70%) | Loss:  0.098085\n",
            "train epoch: 8 | batch staus: 42880/60000 ( 71%) | Loss:  0.026183\n",
            "train epoch: 8 | batch staus: 43520/60000 ( 72%) | Loss:  0.006123\n",
            "train epoch: 8 | batch staus: 44160/60000 ( 74%) | Loss:  0.079098\n",
            "train epoch: 8 | batch staus: 44800/60000 ( 75%) | Loss:  0.014657\n",
            "train epoch: 8 | batch staus: 45440/60000 ( 76%) | Loss:  0.089270\n",
            "train epoch: 8 | batch staus: 46080/60000 ( 77%) | Loss:  0.041534\n",
            "train epoch: 8 | batch staus: 46720/60000 ( 78%) | Loss:  0.129030\n",
            "train epoch: 8 | batch staus: 47360/60000 ( 79%) | Loss:  0.009779\n",
            "train epoch: 8 | batch staus: 48000/60000 ( 80%) | Loss:  0.019988\n",
            "train epoch: 8 | batch staus: 48640/60000 ( 81%) | Loss:  0.034526\n",
            "train epoch: 8 | batch staus: 49280/60000 ( 82%) | Loss:  0.005175\n",
            "train epoch: 8 | batch staus: 49920/60000 ( 83%) | Loss:  0.031197\n",
            "train epoch: 8 | batch staus: 50560/60000 ( 84%) | Loss:  0.032416\n",
            "train epoch: 8 | batch staus: 51200/60000 ( 85%) | Loss:  0.051138\n",
            "train epoch: 8 | batch staus: 51840/60000 ( 86%) | Loss:  0.027205\n",
            "train epoch: 8 | batch staus: 52480/60000 ( 87%) | Loss:  0.022080\n",
            "train epoch: 8 | batch staus: 53120/60000 ( 88%) | Loss:  0.057272\n",
            "train epoch: 8 | batch staus: 53760/60000 ( 90%) | Loss:  0.004366\n",
            "train epoch: 8 | batch staus: 54400/60000 ( 91%) | Loss:  0.020989\n",
            "train epoch: 8 | batch staus: 55040/60000 ( 92%) | Loss:  0.032066\n",
            "train epoch: 8 | batch staus: 55680/60000 ( 93%) | Loss:  0.063205\n",
            "train epoch: 8 | batch staus: 56320/60000 ( 94%) | Loss:  0.005467\n",
            "train epoch: 8 | batch staus: 56960/60000 ( 95%) | Loss:  0.008533\n",
            "train epoch: 8 | batch staus: 57600/60000 ( 96%) | Loss:  0.095465\n",
            "train epoch: 8 | batch staus: 58240/60000 ( 97%) | Loss:  0.006524\n",
            "train epoch: 8 | batch staus: 58880/60000 ( 98%) | Loss:  0.048591\n",
            "train epoch: 8 | batch staus: 59520/60000 ( 99%) | Loss:  0.020938\n",
            "Training time: 1m 49s\n",
            "=======\n",
            " test set: average loss:  0.0008, Accuracy: 9843/10000(98%)\n",
            "Testing time: 1m 56s\n",
            "train epoch: 9 | batch staus: 0/60000 ( 0%) | Loss:  0.078459\n",
            "train epoch: 9 | batch staus: 640/60000 ( 1%) | Loss:  0.139544\n",
            "train epoch: 9 | batch staus: 1280/60000 ( 2%) | Loss:  0.028479\n",
            "train epoch: 9 | batch staus: 1920/60000 ( 3%) | Loss:  0.025615\n",
            "train epoch: 9 | batch staus: 2560/60000 ( 4%) | Loss:  0.054479\n",
            "train epoch: 9 | batch staus: 3200/60000 ( 5%) | Loss:  0.015587\n",
            "train epoch: 9 | batch staus: 3840/60000 ( 6%) | Loss:  0.058760\n",
            "train epoch: 9 | batch staus: 4480/60000 ( 7%) | Loss:  0.036900\n",
            "train epoch: 9 | batch staus: 5120/60000 ( 9%) | Loss:  0.027666\n",
            "train epoch: 9 | batch staus: 5760/60000 ( 10%) | Loss:  0.019215\n",
            "train epoch: 9 | batch staus: 6400/60000 ( 11%) | Loss:  0.007881\n",
            "train epoch: 9 | batch staus: 7040/60000 ( 12%) | Loss:  0.007467\n",
            "train epoch: 9 | batch staus: 7680/60000 ( 13%) | Loss:  0.093006\n",
            "train epoch: 9 | batch staus: 8320/60000 ( 14%) | Loss:  0.029825\n",
            "train epoch: 9 | batch staus: 8960/60000 ( 15%) | Loss:  0.053337\n",
            "train epoch: 9 | batch staus: 9600/60000 ( 16%) | Loss:  0.041131\n",
            "train epoch: 9 | batch staus: 10240/60000 ( 17%) | Loss:  0.065505\n",
            "train epoch: 9 | batch staus: 10880/60000 ( 18%) | Loss:  0.028742\n",
            "train epoch: 9 | batch staus: 11520/60000 ( 19%) | Loss:  0.057441\n",
            "train epoch: 9 | batch staus: 12160/60000 ( 20%) | Loss:  0.069446\n",
            "train epoch: 9 | batch staus: 12800/60000 ( 21%) | Loss:  0.008627\n",
            "train epoch: 9 | batch staus: 13440/60000 ( 22%) | Loss:  0.084883\n",
            "train epoch: 9 | batch staus: 14080/60000 ( 23%) | Loss:  0.017834\n",
            "train epoch: 9 | batch staus: 14720/60000 ( 25%) | Loss:  0.015939\n",
            "train epoch: 9 | batch staus: 15360/60000 ( 26%) | Loss:  0.054298\n",
            "train epoch: 9 | batch staus: 16000/60000 ( 27%) | Loss:  0.002453\n",
            "train epoch: 9 | batch staus: 16640/60000 ( 28%) | Loss:  0.028002\n",
            "train epoch: 9 | batch staus: 17280/60000 ( 29%) | Loss:  0.008663\n",
            "train epoch: 9 | batch staus: 17920/60000 ( 30%) | Loss:  0.025031\n",
            "train epoch: 9 | batch staus: 18560/60000 ( 31%) | Loss:  0.034154\n",
            "train epoch: 9 | batch staus: 19200/60000 ( 32%) | Loss:  0.208419\n",
            "train epoch: 9 | batch staus: 19840/60000 ( 33%) | Loss:  0.004260\n",
            "train epoch: 9 | batch staus: 20480/60000 ( 34%) | Loss:  0.025679\n",
            "train epoch: 9 | batch staus: 21120/60000 ( 35%) | Loss:  0.088866\n",
            "train epoch: 9 | batch staus: 21760/60000 ( 36%) | Loss:  0.112214\n",
            "train epoch: 9 | batch staus: 22400/60000 ( 37%) | Loss:  0.116617\n",
            "train epoch: 9 | batch staus: 23040/60000 ( 38%) | Loss:  0.022794\n",
            "train epoch: 9 | batch staus: 23680/60000 ( 39%) | Loss:  0.038843\n",
            "train epoch: 9 | batch staus: 24320/60000 ( 41%) | Loss:  0.117943\n",
            "train epoch: 9 | batch staus: 24960/60000 ( 42%) | Loss:  0.010435\n",
            "train epoch: 9 | batch staus: 25600/60000 ( 43%) | Loss:  0.050615\n",
            "train epoch: 9 | batch staus: 26240/60000 ( 44%) | Loss:  0.018799\n",
            "train epoch: 9 | batch staus: 26880/60000 ( 45%) | Loss:  0.031291\n",
            "train epoch: 9 | batch staus: 27520/60000 ( 46%) | Loss:  0.099918\n",
            "train epoch: 9 | batch staus: 28160/60000 ( 47%) | Loss:  0.045523\n",
            "train epoch: 9 | batch staus: 28800/60000 ( 48%) | Loss:  0.032838\n",
            "train epoch: 9 | batch staus: 29440/60000 ( 49%) | Loss:  0.063908\n",
            "train epoch: 9 | batch staus: 30080/60000 ( 50%) | Loss:  0.094803\n",
            "train epoch: 9 | batch staus: 30720/60000 ( 51%) | Loss:  0.058998\n",
            "train epoch: 9 | batch staus: 31360/60000 ( 52%) | Loss:  0.017971\n",
            "train epoch: 9 | batch staus: 32000/60000 ( 53%) | Loss:  0.032443\n",
            "train epoch: 9 | batch staus: 32640/60000 ( 54%) | Loss:  0.175540\n",
            "train epoch: 9 | batch staus: 33280/60000 ( 55%) | Loss:  0.019708\n",
            "train epoch: 9 | batch staus: 33920/60000 ( 57%) | Loss:  0.061307\n",
            "train epoch: 9 | batch staus: 34560/60000 ( 58%) | Loss:  0.036211\n",
            "train epoch: 9 | batch staus: 35200/60000 ( 59%) | Loss:  0.039370\n",
            "train epoch: 9 | batch staus: 35840/60000 ( 60%) | Loss:  0.015732\n",
            "train epoch: 9 | batch staus: 36480/60000 ( 61%) | Loss:  0.014817\n",
            "train epoch: 9 | batch staus: 37120/60000 ( 62%) | Loss:  0.039148\n",
            "train epoch: 9 | batch staus: 37760/60000 ( 63%) | Loss:  0.029361\n",
            "train epoch: 9 | batch staus: 38400/60000 ( 64%) | Loss:  0.013092\n",
            "train epoch: 9 | batch staus: 39040/60000 ( 65%) | Loss:  0.023130\n",
            "train epoch: 9 | batch staus: 39680/60000 ( 66%) | Loss:  0.021778\n",
            "train epoch: 9 | batch staus: 40320/60000 ( 67%) | Loss:  0.093788\n",
            "train epoch: 9 | batch staus: 40960/60000 ( 68%) | Loss:  0.026074\n",
            "train epoch: 9 | batch staus: 41600/60000 ( 69%) | Loss:  0.048162\n",
            "train epoch: 9 | batch staus: 42240/60000 ( 70%) | Loss:  0.088348\n",
            "train epoch: 9 | batch staus: 42880/60000 ( 71%) | Loss:  0.165489\n",
            "train epoch: 9 | batch staus: 43520/60000 ( 72%) | Loss:  0.035027\n",
            "train epoch: 9 | batch staus: 44160/60000 ( 74%) | Loss:  0.088000\n",
            "train epoch: 9 | batch staus: 44800/60000 ( 75%) | Loss:  0.104071\n",
            "train epoch: 9 | batch staus: 45440/60000 ( 76%) | Loss:  0.049765\n",
            "train epoch: 9 | batch staus: 46080/60000 ( 77%) | Loss:  0.015243\n",
            "train epoch: 9 | batch staus: 46720/60000 ( 78%) | Loss:  0.124898\n",
            "train epoch: 9 | batch staus: 47360/60000 ( 79%) | Loss:  0.065720\n",
            "train epoch: 9 | batch staus: 48000/60000 ( 80%) | Loss:  0.083477\n",
            "train epoch: 9 | batch staus: 48640/60000 ( 81%) | Loss:  0.124700\n",
            "train epoch: 9 | batch staus: 49280/60000 ( 82%) | Loss:  0.037012\n",
            "train epoch: 9 | batch staus: 49920/60000 ( 83%) | Loss:  0.023113\n",
            "train epoch: 9 | batch staus: 50560/60000 ( 84%) | Loss:  0.096542\n",
            "train epoch: 9 | batch staus: 51200/60000 ( 85%) | Loss:  0.029488\n",
            "train epoch: 9 | batch staus: 51840/60000 ( 86%) | Loss:  0.075818\n",
            "train epoch: 9 | batch staus: 52480/60000 ( 87%) | Loss:  0.031647\n",
            "train epoch: 9 | batch staus: 53120/60000 ( 88%) | Loss:  0.125996\n",
            "train epoch: 9 | batch staus: 53760/60000 ( 90%) | Loss:  0.012357\n",
            "train epoch: 9 | batch staus: 54400/60000 ( 91%) | Loss:  0.041457\n",
            "train epoch: 9 | batch staus: 55040/60000 ( 92%) | Loss:  0.006081\n",
            "train epoch: 9 | batch staus: 55680/60000 ( 93%) | Loss:  0.049504\n",
            "train epoch: 9 | batch staus: 56320/60000 ( 94%) | Loss:  0.020753\n",
            "train epoch: 9 | batch staus: 56960/60000 ( 95%) | Loss:  0.086733\n",
            "train epoch: 9 | batch staus: 57600/60000 ( 96%) | Loss:  0.088465\n",
            "train epoch: 9 | batch staus: 58240/60000 ( 97%) | Loss:  0.147652\n",
            "train epoch: 9 | batch staus: 58880/60000 ( 98%) | Loss:  0.168006\n",
            "train epoch: 9 | batch staus: 59520/60000 ( 99%) | Loss:  0.041921\n",
            "Training time: 1m 47s\n",
            "=======\n",
            " test set: average loss:  0.0007, Accuracy: 9865/10000(99%)\n",
            "Testing time: 1m 54s\n",
            "total time: 15m  27s \n",
            " model was trained on cpu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novFj8CSmPL2",
        "outputId": "aec9fa44-9316-46b7-d4ad-eb06986c869d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "\"\"\" inceptionv3 in pytorch\n",
        "\n",
        "\n",
        "[1] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna\n",
        "\n",
        "    Rethinking the Inception Architecture for Computer Vision\n",
        "    https://arxiv.org/abs/1512.00567v3\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "#same naive inception module\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, pool_features):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 48, kernel_size=1),\n",
        "            BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, pool_features, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1(same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        #x -> 1x1 -> 5x5(same)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        #branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        #x -> 1x1 -> 3x3 -> 3x3(same)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> pool -> 1x1(same)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "#downsample\n",
        "#Factorization into smaller convolutions\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x - > 3x3(downsample)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> 3x3 -> 3x3(downsample)\n",
        "        branch3x3stack = self.branch3x3stack(x)\n",
        "\n",
        "        #x -> avgpool(downsample)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        #\"\"\"We can use two parallel stride 2 blocks: P and C. P is a pooling\n",
        "        #layer (either average or maximum pooling) the activation, both of\n",
        "        #them are stride 2 the filter banks of which are concatenated as in\n",
        "        #figure 10.\"\"\"\n",
        "        outputs = [branch3x3, branch3x3stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "#Factorizing Convolutions with Large Filter Size\n",
        "class InceptionC(nn.Module):\n",
        "    def __init__(self, input_channels, channels_7x7):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "\n",
        "        #In theory, we could go even further and argue that one can replace any n  n\n",
        "        #convolution by a 1  n convolution followed by a n  1 convolution and the\n",
        "        #computational cost saving increases dramatically as n grows (see figure 6).\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, c7, kernel_size=1),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        )\n",
        "\n",
        "        self.branch7x7stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, c7, kernel_size=1),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1(same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        #x -> 1layer 1*7 and 7*1 (same)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "\n",
        "        #x-> 2layer 1*7 and 7*1(same)\n",
        "        branch7x7stack = self.branch7x7stack(x)\n",
        "\n",
        "        #x-> avgpool (same)\n",
        "        branchpool = self.branch_pool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1 -> 3x3(downsample)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> 1x1 -> 1x7 -> 7x1 -> 3x3 (downsample)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "\n",
        "        #x -> avgpool (downsample)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch7x7, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "#same\n",
        "class InceptionE(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3stack_1 = BasicConv2d(input_channels, 448, kernel_size=1)\n",
        "        self.branch3x3stack_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3stack_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3stack_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1 (same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        # x -> 1x1 -> 3x1\n",
        "        # x -> 1x1 -> 1x3\n",
        "        # concatenate(3x1, 1x3)\n",
        "        #\"\"\"7. Inception modules with expanded the filter bank outputs.\n",
        "        #This architecture is used on the coarsest (8  8) grids to promote\n",
        "        #high dimensional representations, as suggested by principle\n",
        "        #2 of Section 2.\"\"\"\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3)\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        # x -> 1x1 -> 3x3 -> 1x3\n",
        "        # x -> 1x1 -> 3x3 -> 3x1\n",
        "        #concatenate(1x3, 3x1)\n",
        "        branch3x3stack = self.branch3x3stack_1(x)\n",
        "        branch3x3stack = self.branch3x3stack_2(branch3x3stack)\n",
        "        branch3x3stack = [\n",
        "            self.branch3x3stack_3a(branch3x3stack),\n",
        "            self.branch3x3stack_3b(branch3x3stack)\n",
        "        ]\n",
        "        branch3x3stack = torch.cat(branch3x3stack, 1)\n",
        "\n",
        "        branchpool = self.branch_pool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
        "\n",
        "        #naive inception module\n",
        "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
        "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
        "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
        "\n",
        "        #downsample\n",
        "        self.Mixed_6a = InceptionB(288)\n",
        "\n",
        "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
        "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
        "\n",
        "        #downsample\n",
        "        self.Mixed_7a = InceptionD(768)\n",
        "\n",
        "        self.Mixed_7b = InceptionE(1280)\n",
        "        self.Mixed_7c = InceptionE(2048)\n",
        "\n",
        "        #6*6 feature size\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout2d()\n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #32 -> 30\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "\n",
        "        #30 -> 30\n",
        "        x = self.Mixed_5b(x)\n",
        "        x = self.Mixed_5c(x)\n",
        "        x = self.Mixed_5d(x)\n",
        "\n",
        "        #30 -> 14\n",
        "        #Efficient Grid Size Reduction to avoid representation\n",
        "        #bottleneck\n",
        "        x = self.Mixed_6a(x)\n",
        "\n",
        "        #14 -> 14\n",
        "        #\"\"\"In practice, we have found that employing this factorization does not\n",
        "        #work well on early layers, but it gives very good results on medium\n",
        "        #grid-sizes (On m  m feature maps, where m ranges between 12 and 20).\n",
        "        #On that level, very good results can be achieved by using 1  7 convolutions\n",
        "        #followed by 7  1 convolutions.\"\"\"\n",
        "        x = self.Mixed_6b(x)\n",
        "        x = self.Mixed_6c(x)\n",
        "        x = self.Mixed_6d(x)\n",
        "        x = self.Mixed_6e(x)\n",
        "\n",
        "        #14 -> 6\n",
        "        #Efficient Grid Size Reduction\n",
        "        x = self.Mixed_7a(x)\n",
        "\n",
        "        #6 -> 6\n",
        "        #We are using this solution only on the coarsest grid,\n",
        "        #since that is the place where producing high dimensional\n",
        "        #sparse representation is the most critical as the ratio of\n",
        "        #local processing (by 1  1 convolutions) is increased compared\n",
        "        #to the spatial aggregation.\"\"\"\n",
        "        x = self.Mixed_7b(x)\n",
        "        x = self.Mixed_7c(x)\n",
        "\n",
        "        #6 -> 1\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def inceptionv3():\n",
        "    return InceptionV3()\n",
        "\n",
        "model=inceptionv3()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a785249cc63b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minceptionv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a785249cc63b>\u001b[0m in \u001b[0;36minceptionv3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minceptionv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a785249cc63b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_1a_3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_2a_3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d_2b_3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a785249cc63b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_channels, output_channels, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'kernel_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAWeRUy90Doo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "41dd6345-0abb-4e38-affa-90ab2f0c2c99"
      },
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\" inceptionv4 in pytorch\n",
        "\n",
        "\n",
        "[1] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n",
        "\n",
        "    Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\n",
        "    https://arxiv.org/abs/1602.07261\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device='cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'traing mnist model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Inception_Stem(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 3. The schema for stem of the pure Inception-v4 and\n",
        "    #Inception-ResNet-v2 networks. This is the input part of those\n",
        "    #networks.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=3),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1),\n",
        "            BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3_conv = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7a = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch7x7b = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpoola = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.branchpoolb = BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=x.permute(0,3,1,2)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3_conv(x),\n",
        "            self.branch3x3_pool(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branch7x7a(x),\n",
        "            self.branch7x7b(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branchpoola(x),\n",
        "            self.branchpoolb(x)\n",
        "        ]\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 4. The schema for 35  35 grid modules of the pure\n",
        "    #Inception-v4 network. This is the Inception-A block of Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branch1x1(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 7. The schema for 35  35 to 17  17 reduction module.\n",
        "    #Different variants of this blocks (with various number of filters)\n",
        "    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n",
        "    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers\n",
        "    #represent filter bank sizes which can be looked up in Table 1.\n",
        "    def __init__(self, input_channels, k, l, m, n):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, k, kernel_size=1),\n",
        "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
        "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.output_channels = input_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 5. The schema for 17  17 grid modules of the pure Inception-v4 network.\n",
        "    #This is the Inception-B block of Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch7x7stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(192, 224, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(224, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 128, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branch7x7stack(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 8. The schema for 17  17 to 8  8 grid-reduction mod- ule.\n",
        "    #This is the reduction module used by the pure Inception-v4 network in\n",
        "    #Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 256, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(256, 320, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(320, 320, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        #\"\"\"Figure 6. The schema for 88 grid modules of the pure\n",
        "        #Inceptionv4 network. This is the Inception-C block of Figure 9.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 384, kernel_size=1),\n",
        "            BasicConv2d(384, 448, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(448, 512, kernel_size=(3, 1), padding=(1, 0)),\n",
        "        )\n",
        "        self.branch3x3stacka = BasicConv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3stackb = BasicConv2d(512, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "        self.branch3x3a = BasicConv2d(384, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "        self.branch3x3b = BasicConv2d(384, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3stack_output = self.branch3x3stack(x)\n",
        "        branch3x3stack_output = [\n",
        "            self.branch3x3stacka(branch3x3stack_output),\n",
        "            self.branch3x3stackb(branch3x3stack_output)\n",
        "        ]\n",
        "        branch3x3stack_output = torch.cat(branch3x3stack_output, 1)\n",
        "\n",
        "        branch3x3_output = self.branch3x3(x)\n",
        "        branch3x3_output = [\n",
        "            self.branch3x3a(branch3x3_output),\n",
        "            self.branch3x3b(branch3x3_output)\n",
        "        ]\n",
        "        branch3x3_output = torch.cat(branch3x3_output, 1)\n",
        "\n",
        "        branch1x1_output = self.branch1x1(x)\n",
        "\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        output = [\n",
        "            branch1x1_output,\n",
        "            branch3x3_output,\n",
        "            branch3x3stack_output,\n",
        "            branchpool\n",
        "        ]\n",
        "\n",
        "        return torch.cat(output, 1)\n",
        "\n",
        "class InceptionV4(nn.Module):\n",
        "\n",
        "    def __init__(self, A, B, C, k=192, l=224, m=256, n=384, class_nums=100):\n",
        "\n",
        "        super().__init__()\n",
        "        self.stem = Inception_Stem(3)\n",
        "        self.inception_a = self._generate_inception_module(384, 384, A, InceptionA)\n",
        "        self.reduction_a = ReductionA(384, k, l, m, n)\n",
        "        output_channels = self.reduction_a.output_channels\n",
        "        self.inception_b = self._generate_inception_module(output_channels, 1024, B, InceptionB)\n",
        "        self.reduction_b = ReductionB(1024)\n",
        "        self.inception_c = self._generate_inception_module(1536, 1536, C, InceptionC)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "\n",
        "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
        "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
        "        self.linear = nn.Linear(1536, class_nums)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception_a(x)\n",
        "        x = self.reduction_a(x)\n",
        "        x = self.inception_b(x)\n",
        "        x = self.reduction_b(x)\n",
        "        x = self.inception_c(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 1536)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
        "\n",
        "        layers = nn.Sequential()\n",
        "        for l in range(block_num):\n",
        "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        return layers\n",
        "\n",
        "def inceptionv4():\n",
        "    return InceptionV4(4, 7, 3)\n",
        "\n",
        "model=inceptionv4()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traing mnist model on cpu\n",
            "============================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8500d467c6c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8500d467c6c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMZiUi4H82FP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "6f23fa35-c28e-422b-f451-1dae5fef57b1"
      },
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\" inceptionv4 in pytorch\n",
        "\n",
        "\n",
        "[1] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n",
        "\n",
        "    Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\n",
        "    https://arxiv.org/abs/1602.07261\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Inception_Stem(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 3. The schema for stem of the pure Inception-v4 and\n",
        "    #Inception-ResNet-v2 networks. This is the input part of those\n",
        "    #networks.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=3),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1),\n",
        "            BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3_conv = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7a = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch7x7b = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpoola = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.branchpoolb = BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3_conv(x),\n",
        "            self.branch3x3_pool(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branch7x7a(x),\n",
        "            self.branch7x7b(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branchpoola(x),\n",
        "            self.branchpoolb(x)\n",
        "        ]\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 4. The schema for 35  35 grid modules of the pure\n",
        "    #Inception-v4 network. This is the Inception-A block of Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branch1x1(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 7. The schema for 35  35 to 17  17 reduction module.\n",
        "    #Different variants of this blocks (with various number of filters)\n",
        "    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n",
        "    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers\n",
        "    #represent filter bank sizes which can be looked up in Table 1.\n",
        "    def __init__(self, input_channels, k, l, m, n):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, k, kernel_size=1),\n",
        "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
        "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.output_channels = input_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 5. The schema for 17  17 grid modules of the pure Inception-v4 network.\n",
        "    #This is the Inception-B block of Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch7x7stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(192, 224, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(224, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 128, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branch7x7stack(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 8. The schema for 17  17 to 8  8 grid-reduction mod- ule.\n",
        "    #This is the reduction module used by the pure Inception-v4 network in\n",
        "    #Figure 9.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 256, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(256, 320, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(320, 320, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        #\"\"\"Figure 6. The schema for 88 grid modules of the pure\n",
        "        #Inceptionv4 network. This is the Inception-C block of Figure 9.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 384, kernel_size=1),\n",
        "            BasicConv2d(384, 448, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(448, 512, kernel_size=(3, 1), padding=(1, 0)),\n",
        "        )\n",
        "        self.branch3x3stacka = BasicConv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3stackb = BasicConv2d(512, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "        self.branch3x3a = BasicConv2d(384, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "        self.branch3x3b = BasicConv2d(384, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3stack_output = self.branch3x3stack(x)\n",
        "        branch3x3stack_output = [\n",
        "            self.branch3x3stacka(branch3x3stack_output),\n",
        "            self.branch3x3stackb(branch3x3stack_output)\n",
        "        ]\n",
        "        branch3x3stack_output = torch.cat(branch3x3stack_output, 1)\n",
        "\n",
        "        branch3x3_output = self.branch3x3(x)\n",
        "        branch3x3_output = [\n",
        "            self.branch3x3a(branch3x3_output),\n",
        "            self.branch3x3b(branch3x3_output)\n",
        "        ]\n",
        "        branch3x3_output = torch.cat(branch3x3_output, 1)\n",
        "\n",
        "        branch1x1_output = self.branch1x1(x)\n",
        "\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        output = [\n",
        "            branch1x1_output,\n",
        "            branch3x3_output,\n",
        "            branch3x3stack_output,\n",
        "            branchpool\n",
        "        ]\n",
        "\n",
        "        return torch.cat(output, 1)\n",
        "\n",
        "class InceptionV4(nn.Module):\n",
        "\n",
        "    def __init__(self, A, B, C, k=192, l=224, m=256, n=384, class_nums=100):\n",
        "\n",
        "        super().__init__()\n",
        "        self.stem = Inception_Stem(3)\n",
        "        self.inception_a = self._generate_inception_module(384, 384, A, InceptionA)\n",
        "        self.reduction_a = ReductionA(384, k, l, m, n)\n",
        "        output_channels = self.reduction_a.output_channels\n",
        "        self.inception_b = self._generate_inception_module(output_channels, 1024, B, InceptionB)\n",
        "        self.reduction_b = ReductionB(1024)\n",
        "        self.inception_c = self._generate_inception_module(1536, 1536, C, InceptionC)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "\n",
        "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
        "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
        "        self.linear = nn.Linear(1536, class_nums)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception_a(x)\n",
        "        x = self.reduction_a(x)\n",
        "        x = self.inception_b(x)\n",
        "        x = self.reduction_b(x)\n",
        "        x = self.inception_c(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 1536)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
        "\n",
        "        layers = nn.Sequential()\n",
        "        for l in range(block_num):\n",
        "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        return layers\n",
        "\n",
        "class InceptionResNetA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 16. The schema for 35  35 grid (Inception-ResNet-A)\n",
        "    #module of the Inception-ResNet-v2 network.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 48, kernel_size=3, padding=1),\n",
        "            BasicConv2d(48, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 32, kernel_size=1)\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(128, 384, kernel_size=1)\n",
        "        self.shortcut = nn.Conv2d(input_channels, 384, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(384)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branch3x3stack(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "        residual = self.reduction1x1(residual)\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        output = self.bn(shortcut + residual)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class InceptionResNetB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 17. The schema for 17  17 grid (Inception-ResNet-B) module of\n",
        "    #the Inception-ResNet-v2 network.\"\"\"\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 128, kernel_size=1),\n",
        "            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(384, 1154, kernel_size=1)\n",
        "        self.shortcut = nn.Conv2d(input_channels, 1154, kernel_size=1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(1154)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch7x7(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "\n",
        "        #\"\"\"In general we picked some scaling factors between 0.1 and 0.3 to scale the residuals\n",
        "        #before their being added to the accumulated layer activations (cf. Figure 20).\"\"\"\n",
        "        residual = self.reduction1x1(residual) * 0.1\n",
        "\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        output = self.bn(residual + shortcut)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class InceptionResNetC(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        #Figure 19. The schema for 88 grid (Inception-ResNet-C)\n",
        "        #module of the Inception-ResNet-v2 network.\"\"\"\n",
        "        super().__init__()\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "        self.reduction1x1 = nn.Conv2d(448, 2048, kernel_size=1)\n",
        "        self.shorcut = nn.Conv2d(input_channels, 2048, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(2048)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch3x3(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "        residual = self.reduction1x1(residual) * 0.1\n",
        "\n",
        "        shorcut = self.shorcut(x)\n",
        "\n",
        "        output = self.bn(shorcut + residual)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class InceptionResNetReductionA(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 7. The schema for 35  35 to 17  17 reduction module.\n",
        "    #Different variants of this blocks (with various number of filters)\n",
        "    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n",
        "    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers\n",
        "    #represent filter bank sizes which can be looked up in Table 1.\n",
        "    def __init__(self, input_channels, k, l, m, n):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, k, kernel_size=1),\n",
        "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
        "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.output_channels = input_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionResNetReductionB(nn.Module):\n",
        "\n",
        "    #\"\"\"Figure 18. The schema for 17  17 to 8  8 grid-reduction module.\n",
        "    #Reduction-B module used by the wider Inception-ResNet-v1 network in\n",
        "    #Figure 15.\"\"\"\n",
        "    #I believe it was a typo(Inception-ResNet-v1 should be Inception-ResNet-v2)\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branchpool = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "        self.branch3x3a = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3b = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 288, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 288, kernel_size=3, padding=1),\n",
        "            BasicConv2d(288, 320, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch3x3a(x),\n",
        "            self.branch3x3b(x),\n",
        "            self.branch3x3stack(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "        return x\n",
        "\n",
        "class InceptionResNetV2(nn.Module):\n",
        "\n",
        "    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, class_nums=100):\n",
        "        super().__init__()\n",
        "        self.stem = Inception_Stem(3)\n",
        "        self.inception_resnet_a = self._generate_inception_module(384, 384, A, InceptionResNetA)\n",
        "        self.reduction_a = InceptionResNetReductionA(384, k, l, m, n)\n",
        "        output_channels = self.reduction_a.output_channels\n",
        "        self.inception_resnet_b = self._generate_inception_module(output_channels, 1154, B, InceptionResNetB)\n",
        "        self.reduction_b = InceptionResNetReductionB(1154)\n",
        "        self.inception_resnet_c = self._generate_inception_module(2146, 2048, C, InceptionResNetC)\n",
        "\n",
        "        #6x6 featuresize\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
        "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
        "        self.linear = nn.Linear(2048, class_nums)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception_resnet_a(x)\n",
        "        x = self.reduction_a(x)\n",
        "        x = self.inception_resnet_b(x)\n",
        "        x = self.reduction_b(x)\n",
        "        x = self.inception_resnet_c(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
        "\n",
        "        layers = nn.Sequential()\n",
        "        for l in range(block_num):\n",
        "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        return layers\n",
        "\n",
        "def inceptionv4():\n",
        "    return InceptionV4(4, 7, 3)\n",
        "\n",
        "def inception_resnet_v2():\n",
        "    return InceptionResNetV2(5, 10, 5)\n",
        "\n",
        "\n",
        "model=inception_resnet_v2()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b9f4f6733b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b9f4f6733b21>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0mouput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b9f4f6733b21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_resnet_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b9f4f6733b21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         x = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b9f4f6733b21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znoRzYCWF1xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "26fef7d1-946a-4440-de1f-a9f41c7337ed"
      },
      "source": [
        "\"\"\"resnet in pytorch\n",
        "\n",
        "\n",
        "\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
        "\n",
        "    Deep Residual Learning for Image Recognition\n",
        "    https://arxiv.org/abs/1512.03385v1\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #BasicBlock and BottleNeck block\n",
        "    #have different output size\n",
        "    #we use class attribute expansion\n",
        "    #to distinct\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        #residual function\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        #shortcut\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        #the shortcut output dimension is not the same with residual function\n",
        "        #use 1*1 convolution to match the dimension\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"Residual block for resnet over 50 layers\n",
        "\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "        #we use a different inputsize than the original paper\n",
        "        #so conv2_x's stride is 1\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
        "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
        "        contain more than one residual block\n",
        "\n",
        "        Args:\n",
        "            block: block type, basic block or bottle neck block\n",
        "            out_channels: output depth channel number of this layer\n",
        "            num_blocks: how many blocks per layer\n",
        "            stride: the stride of the first block of this layer\n",
        "\n",
        "        Return:\n",
        "            return a resnet layer\n",
        "        \"\"\"\n",
        "\n",
        "        # we have num_block blocks per layer, the first block\n",
        "        # could be 1 or 2, other blocks would always be 1\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def resnet18():\n",
        "    \"\"\" return a ResNet 18 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def resnet34():\n",
        "    \"\"\" return a ResNet 34 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def resnet50():\n",
        "    \"\"\" return a ResNet 50 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
        "\n",
        "def resnet101():\n",
        "    \"\"\" return a ResNet 101 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
        "\n",
        "def resnet152():\n",
        "    \"\"\" return a ResNet 152 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 8, 36, 3])\n",
        "\n",
        "model=resnet152()\n",
        "model.to(device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data,target=data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    ouput=model(data)\n",
        "    loss=criterion(ouput,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10==0:\n",
        "      print('train epoch: {} | batch staus: {}/{} ({: .0f}%) | Loss: {: .6f}'.format(epoch,batch_idx*len(data), len(train_loader.dataset),\n",
        "                                                                                     100.*batch_idx / len(train_loader), loss.item()))\n",
        "  \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss=0\n",
        "  correct=0\n",
        "  for data, target in test_loader:\n",
        "    data, target=data.to(device), target.to(device)\n",
        "    output=model(data)\n",
        "    test_loss += criterion(output,target).item()\n",
        "    pred=output.data.max(1,keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss/=len(test_loader.dataset)\n",
        "  print(f'=======\\n test set: average loss: {test_loss: .4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "  f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "    m,s=divmod(time.time()-since,60)\n",
        "    print(f'total time: {m:.0f}m {s: .0f}s \\n model was trained on {device}!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1517ab601b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1517ab601b71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mouput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1517ab601b71>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICmv3VCuGOUd"
      },
      "source": [
        "\"\"\"dense net in pytorch\n",
        "\n",
        "\n",
        "\n",
        "[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\n",
        "\n",
        "    Densely Connected Convolutional Networks\n",
        "    https://arxiv.org/abs/1608.06993v5\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "#\"\"\"Bottleneck layers. Although each layer only produces k\n",
        "#output feature-maps, it typically has many more inputs. It\n",
        "#has been noted in [37, 11] that a 11 convolution can be in-\n",
        "#troduced as bottleneck layer before each 33 convolution\n",
        "#to reduce the number of input feature-maps, and thus to\n",
        "#improve computational efficiency.\"\"\"\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        #\"\"\"In  our experiments, we let each 11 convolution\n",
        "        #produce 4k feature-maps.\"\"\"\n",
        "        inner_channel = 4 * growth_rate\n",
        "\n",
        "        #\"\"\"We find this design especially effective for DenseNet and\n",
        "        #we refer to our network with such a bottleneck layer, i.e.,\n",
        "        #to the BN-ReLU-Conv(11)-BN-ReLU-Conv(33) version of H ` ,\n",
        "        #as DenseNet-B.\"\"\"\n",
        "        self.bottle_neck = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(inner_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([x, self.bottle_neck(x)], 1)\n",
        "\n",
        "#\"\"\"We refer to layers between blocks as transition\n",
        "#layers, which do convolution and pooling.\"\"\"\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        #\"\"\"The transition layers used in our experiments\n",
        "        #consist of a batch normalization layer and an 11\n",
        "        #convolutional layer followed by a 22 average pooling\n",
        "        #layer\"\"\".\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "\n",
        "#DesneNet-BC\n",
        "#B stands for bottleneck layer(BN-RELU-CONV(1x1)-BN-RELU-CONV(3x3))\n",
        "#C stands for compression factor(0<=theta<=1)\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=100):\n",
        "        super().__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        #\"\"\"Before entering the first dense block, a convolution\n",
        "        #with 16 (or twice the growth rate for DenseNet-BC)\n",
        "        #output channels is performed on the input images.\"\"\"\n",
        "        inner_channels = 2 * growth_rate\n",
        "\n",
        "        #For convolutional layers with kernel size 33, each\n",
        "        #side of the inputs is zero-padded by one pixel to keep\n",
        "        #the feature-map size fixed.\n",
        "        self.conv1 = nn.Conv2d(3, inner_channels, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for index in range(len(nblocks) - 1):\n",
        "            self.features.add_module(\"dense_block_layer_{}\".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n",
        "            inner_channels += growth_rate * nblocks[index]\n",
        "\n",
        "            #\"\"\"If a dense block contains m feature-maps, we let the\n",
        "            #following transition layer generate m output feature-\n",
        "            #maps, where 0 <   1 is referred to as the compression\n",
        "            #fac-tor.\n",
        "            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n",
        "            self.features.add_module(\"transition_layer_{}\".format(index), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels\n",
        "\n",
        "        self.features.add_module(\"dense_block{}\".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('relu', nn.ReLU(inplace=True))\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.linear = nn.Linear(inner_channels, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.features(output)\n",
        "        output = self.avgpool(output)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "    def _make_dense_layers(self, block, in_channels, nblocks):\n",
        "        dense_block = nn.Sequential()\n",
        "        for index in range(nblocks):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(index), block(in_channels, self.growth_rate))\n",
        "            in_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "def densenet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
        "\n",
        "def densenet169():\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
        "\n",
        "def densenet201():\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
        "\n",
        "def densenet161():\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dKPo4FoovEZ",
        "outputId": "a471599d-4960-4410-a1a1-f8fb1606c7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "\"\"\" inceptionv3 in pytorch\n",
        "\n",
        "\n",
        "[1] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna\n",
        "\n",
        "    Rethinking the Inception Architecture for Computer Vision\n",
        "    https://arxiv.org/abs/1512.00567v3\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\"\"\" train and test dataset\n",
        "\n",
        "author baiyu\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import argparse\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "from datetime import datetime\n",
        "import easydict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import get_network\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "class CIFAR100Train(Dataset):\n",
        "    \"\"\"cifar100 test dataset, derived from\n",
        "    torch.utils.data.DataSet\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, transform=None):\n",
        "        #if transform is given, we transoform data using\n",
        "        with open(os.path.join(path, 'train'), 'rb') as cifar100:\n",
        "            self.data = pickle.load(cifar100, encoding='bytes')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['fine_labels'.encode()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.data['fine_labels'.encode()][index]\n",
        "        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n",
        "        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n",
        "        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n",
        "        image = numpy.dstack((r, g, b))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return label, image\n",
        "\n",
        "class CIFAR100Test(Dataset):\n",
        "    \"\"\"cifar100 test dataset, derived from\n",
        "    torch.utils.data.DataSet\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, transform=None):\n",
        "        with open(os.path.join(path, 'test'), 'rb') as cifar100:\n",
        "            self.data = pickle.load(cifar100, encoding='bytes')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['data'.encode()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.data['fine_labels'.encode()][index]\n",
        "        r = self.data['data'.encode()][index, :1024].reshape(32, 32)\n",
        "        g = self.data['data'.encode()][index, 1024:2048].reshape(32, 32)\n",
        "        b = self.data['data'.encode()][index, 2048:].reshape(32, 32)\n",
        "        image = numpy.dstack((r, g, b))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return label, image\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "#same naive inception module\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, pool_features):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 48, kernel_size=1),\n",
        "            BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, pool_features, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1(same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        #x -> 1x1 -> 5x5(same)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        #branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        #x -> 1x1 -> 3x3 -> 3x3(same)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> pool -> 1x1(same)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "#downsample\n",
        "#Factorization into smaller convolutions\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x - > 3x3(downsample)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> 3x3 -> 3x3(downsample)\n",
        "        branch3x3stack = self.branch3x3stack(x)\n",
        "\n",
        "        #x -> avgpool(downsample)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        #\"\"\"We can use two parallel stride 2 blocks: P and C. P is a pooling\n",
        "        #layer (either average or maximum pooling) the activation, both of\n",
        "        #them are stride 2 the filter banks of which are concatenated as in\n",
        "        #figure 10.\"\"\"\n",
        "        outputs = [branch3x3, branch3x3stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "#Factorizing Convolutions with Large Filter Size\n",
        "class InceptionC(nn.Module):\n",
        "    def __init__(self, input_channels, channels_7x7):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "\n",
        "        #In theory, we could go even further and argue that one can replace any n  n\n",
        "        #convolution by a 1  n convolution followed by a n  1 convolution and the\n",
        "        #computational cost saving increases dramatically as n grows (see figure 6).\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, c7, kernel_size=1),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        )\n",
        "\n",
        "        self.branch7x7stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, c7, kernel_size=1),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1(same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        #x -> 1layer 1*7 and 7*1 (same)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "\n",
        "        #x-> 2layer 1*7 and 7*1(same)\n",
        "        branch7x7stack = self.branch7x7stack(x)\n",
        "\n",
        "        #x-> avgpool (same)\n",
        "        branchpool = self.branch_pool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1 -> 3x3(downsample)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        #x -> 1x1 -> 1x7 -> 7x1 -> 3x3 (downsample)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "\n",
        "        #x -> avgpool (downsample)\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch7x7, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "#same\n",
        "class InceptionE(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3stack_1 = BasicConv2d(input_channels, 448, kernel_size=1)\n",
        "        self.branch3x3stack_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3stack_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3stack_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x -> 1x1 (same)\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        # x -> 1x1 -> 3x1\n",
        "        # x -> 1x1 -> 1x3\n",
        "        # concatenate(3x1, 1x3)\n",
        "        #\"\"\"7. Inception modules with expanded the filter bank outputs.\n",
        "        #This architecture is used on the coarsest (8  8) grids to promote\n",
        "        #high dimensional representations, as suggested by principle\n",
        "        #2 of Section 2.\"\"\"\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3)\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        # x -> 1x1 -> 3x3 -> 1x3\n",
        "        # x -> 1x1 -> 3x3 -> 3x1\n",
        "        #concatenate(1x3, 3x1)\n",
        "        branch3x3stack = self.branch3x3stack_1(x)\n",
        "        branch3x3stack = self.branch3x3stack_2(branch3x3stack)\n",
        "        branch3x3stack = [\n",
        "            self.branch3x3stack_3a(branch3x3stack),\n",
        "            self.branch3x3stack_3b(branch3x3stack)\n",
        "        ]\n",
        "        branch3x3stack = torch.cat(branch3x3stack, 1)\n",
        "\n",
        "        branchpool = self.branch_pool(x)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3stack, branchpool]\n",
        "\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
        "\n",
        "        #naive inception module\n",
        "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
        "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
        "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
        "\n",
        "        #downsample\n",
        "        self.Mixed_6a = InceptionB(288)\n",
        "\n",
        "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
        "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
        "\n",
        "        #downsample\n",
        "        self.Mixed_7a = InceptionD(768)\n",
        "\n",
        "        self.Mixed_7b = InceptionE(1280)\n",
        "        self.Mixed_7c = InceptionE(2048)\n",
        "\n",
        "        #6*6 feature size\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout2d()\n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #32 -> 30\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "\n",
        "        #30 -> 30\n",
        "        x = self.Mixed_5b(x)\n",
        "        x = self.Mixed_5c(x)\n",
        "        x = self.Mixed_5d(x)\n",
        "\n",
        "        #30 -> 14\n",
        "        #Efficient Grid Size Reduction to avoid representation\n",
        "        #bottleneck\n",
        "        x = self.Mixed_6a(x)\n",
        "\n",
        "        #14 -> 14\n",
        "        #\"\"\"In practice, we have found that employing this factorization does not\n",
        "        #work well on early layers, but it gives very good results on medium\n",
        "        #grid-sizes (On m  m feature maps, where m ranges between 12 and 20).\n",
        "        #On that level, very good results can be achieved by using 1  7 convolutions\n",
        "        #followed by 7  1 convolutions.\"\"\"\n",
        "        x = self.Mixed_6b(x)\n",
        "        x = self.Mixed_6c(x)\n",
        "        x = self.Mixed_6d(x)\n",
        "        x = self.Mixed_6e(x)\n",
        "\n",
        "        #14 -> 6\n",
        "        #Efficient Grid Size Reduction\n",
        "        x = self.Mixed_7a(x)\n",
        "\n",
        "        #6 -> 6\n",
        "        #We are using this solution only on the coarsest grid,\n",
        "        #since that is the place where producing high dimensional\n",
        "        #sparse representation is the most critical as the ratio of\n",
        "        #local processing (by 1  1 convolutions) is increased compared\n",
        "        #to the spatial aggregation.\"\"\"\n",
        "        x = self.Mixed_7b(x)\n",
        "        x = self.Mixed_7c(x)\n",
        "\n",
        "        #6 -> 1\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def inceptionv3():\n",
        "    return InceptionV3()\n",
        "\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "\n",
        "    start = time.time()\n",
        "    net.train()\n",
        "    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n",
        "\n",
        "        if args.gpu:\n",
        "            labels = labels.cuda()\n",
        "            images = images.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n",
        "\n",
        "        last_layer = list(net.children())[-1]\n",
        "        for name, para in last_layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
        "            if 'bias' in name:\n",
        "                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
        "\n",
        "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
        "            loss.item(),\n",
        "            optimizer.param_groups[0]['lr'],\n",
        "            epoch=epoch,\n",
        "            trained_samples=batch_index * args.b + len(images),\n",
        "            total_samples=len(cifar100_training_loader.dataset)\n",
        "        ))\n",
        "\n",
        "        #update training loss for each iteration\n",
        "        writer.add_scalar('Train/loss', loss.item(), n_iter)\n",
        "\n",
        "        if epoch <= args.warm:\n",
        "            warmup_scheduler.step()\n",
        "\n",
        "    for name, param in net.named_parameters():\n",
        "        layer, attr = os.path.splitext(name)\n",
        "        attr = attr[1:]\n",
        "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
        "\n",
        "    finish = time.time()\n",
        "\n",
        "    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_training(epoch=0, tb=True):\n",
        "\n",
        "    start = time.time()\n",
        "    net.eval()\n",
        "\n",
        "    test_loss = 0.0 # cost function error\n",
        "    correct = 0.0\n",
        "\n",
        "    for (images, labels) in cifar100_test_loader:\n",
        "\n",
        "        if args.gpu:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        outputs = net(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum()\n",
        "\n",
        "    finish = time.time()\n",
        "    if args.gpu:\n",
        "        print('GPU INFO.....')\n",
        "        print(torch.cuda.memory_summary(), end='')\n",
        "    print('Evaluating Network.....')\n",
        "    print('Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}, Time consumed:{:.2f}s'.format(\n",
        "        epoch,\n",
        "        test_loss / len(cifar100_test_loader.dataset),\n",
        "        correct.float() / len(cifar100_test_loader.dataset),\n",
        "        finish - start\n",
        "    ))\n",
        "    print()\n",
        "\n",
        "    #add informations to tensorboard\n",
        "    if tb:\n",
        "        writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n",
        "        writer.add_scalar('Test/Accuracy', correct.float() / len(cifar100_test_loader.dataset), epoch)\n",
        "\n",
        "    return correct.float() / len(cifar100_test_loader.dataset)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = easydict.EasyDict({ \n",
        "        \"batchsize\": 100,\n",
        "        \"epoch\": 20,\n",
        "        \"gpu\": 0,\n",
        "        \"out\": \"result\",\n",
        "        \"resume\": False,\n",
        "        \"unit\": 1000})\n",
        "    net = get_network(args)\n",
        "\n",
        "    #data preprocessing:\n",
        "    cifar100_training_loader = get_training_dataloader(\n",
        "        settings.CIFAR100_TRAIN_MEAN,\n",
        "        settings.CIFAR100_TRAIN_STD,\n",
        "        num_workers=4,\n",
        "        batch_size=args.b,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    cifar100_test_loader = get_test_dataloader(\n",
        "        settings.CIFAR100_TRAIN_MEAN,\n",
        "        settings.CIFAR100_TRAIN_STD,\n",
        "        num_workers=4,\n",
        "        batch_size=args.b,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n",
        "    iter_per_epoch = len(cifar100_training_loader)\n",
        "    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm)\n",
        "\n",
        "    if args.resume:\n",
        "        recent_folder = most_recent_folder(os.path.join(settings.CHECKPOINT_PATH, args.net), fmt=settings.DATE_FORMAT)\n",
        "        if not recent_folder:\n",
        "            raise Exception('no recent folder were found')\n",
        "\n",
        "        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder)\n",
        "\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)\n",
        "\n",
        "    #use tensorboard\n",
        "    if not os.path.exists(settings.LOG_DIR):\n",
        "        os.mkdir(settings.LOG_DIR)\n",
        "\n",
        "    #since tensorboard can't overwrite old values\n",
        "    #so the only way is to create a new tensorboard log\n",
        "    writer = SummaryWriter(log_dir=os.path.join(\n",
        "            settings.LOG_DIR, args.net, settings.TIME_NOW))\n",
        "    input_tensor = torch.Tensor(1, 3, 32, 32)\n",
        "    if args.gpu:\n",
        "        input_tensor = input_tensor.cuda()\n",
        "    writer.add_graph(net, input_tensor)\n",
        "\n",
        "    #create checkpoint folder to save model\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        os.makedirs(checkpoint_path)\n",
        "    checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
        "\n",
        "    best_acc = 0.0\n",
        "    if args.resume:\n",
        "        best_weights = best_acc_weights(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n",
        "        if best_weights:\n",
        "            weights_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder, best_weights)\n",
        "            print('found best acc weights file:{}'.format(weights_path))\n",
        "            print('load best training file to test acc...')\n",
        "            net.load_state_dict(torch.load(weights_path))\n",
        "            best_acc = eval_training(tb=False)\n",
        "            print('best acc is {:0.2f}'.format(best_acc))\n",
        "\n",
        "        recent_weights_file = most_recent_weights(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n",
        "        if not recent_weights_file:\n",
        "            raise Exception('no recent weights file were found')\n",
        "        weights_path = os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder, recent_weights_file)\n",
        "        print('loading weights file {} to resume training.....'.format(weights_path))\n",
        "        net.load_state_dict(torch.load(weights_path))\n",
        "\n",
        "        resume_epoch = last_epoch(os.path.join(settings.CHECKPOINT_PATH, args.net, recent_folder))\n",
        "\n",
        "\n",
        "    for epoch in range(1, settings.EPOCH + 1):\n",
        "        if epoch > args.warm:\n",
        "            train_scheduler.step(epoch)\n",
        "\n",
        "        if args.resume:\n",
        "            if epoch <= resume_epoch:\n",
        "                continue\n",
        "\n",
        "        train(epoch)\n",
        "        acc = eval_training(epoch)\n",
        "\n",
        "        #start to save best performance model after learning rate decay to 0.01\n",
        "        if epoch > settings.MILESTONES[1] and best_acc < acc:\n",
        "            weights_path = checkpoint_path.format(net=args.net, epoch=epoch, type='best')\n",
        "            print('saving weights file to {}'.format(weights_path))\n",
        "            torch.save(net.state_dict(), weights_path)\n",
        "            best_acc = acc\n",
        "            continue\n",
        "\n",
        "        if not epoch % settings.SAVE_EPOCH:\n",
        "            weights_path = checkpoint_path.format(net=args.net, epoch=epoch, type='regular')\n",
        "            print('saving weights file to {}'.format(weights_path))\n",
        "            torch.save(net.state_dict(), weights_path)\n",
        "\n",
        "    writer.close()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-50a1a0389de7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_network' from 'torch.utils' (/usr/local/lib/python3.7/dist-packages/torch/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}