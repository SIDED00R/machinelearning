{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoquIfGi3V7y4BIww/DKun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/deep3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVKIRdB7c1y8",
        "outputId": "d19a65ed-f70e-446f-dcd3-f4c0c7a10116"
      },
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "x_data=tensor([[1.0],[2.0],[3.0], [4.0]])\n",
        "y_data=tensor([[0.0],[0.0],[1.0], [1.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(1, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred=sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "model=Model()\n",
        "\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1000):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print(f'Epoch: {epoch+1} | Loss: {loss.item(): .4f}')\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() # w=w-0.01*w.grad().item()\n",
        "#After training\n",
        "hour_var=model(tensor([1.0]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([7.0]))\n",
        "print(f'Prediction after 7 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n",
        "hour_var=model(tensor([20.0]))\n",
        "print(f'Prediction after 20 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() >0.5}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss:  1.6786\n",
            "Epoch: 2 | Loss:  1.6550\n",
            "Epoch: 3 | Loss:  1.6317\n",
            "Epoch: 4 | Loss:  1.6087\n",
            "Epoch: 5 | Loss:  1.5860\n",
            "Epoch: 6 | Loss:  1.5635\n",
            "Epoch: 7 | Loss:  1.5413\n",
            "Epoch: 8 | Loss:  1.5194\n",
            "Epoch: 9 | Loss:  1.4978\n",
            "Epoch: 10 | Loss:  1.4765\n",
            "Epoch: 11 | Loss:  1.4554\n",
            "Epoch: 12 | Loss:  1.4347\n",
            "Epoch: 13 | Loss:  1.4144\n",
            "Epoch: 14 | Loss:  1.3943\n",
            "Epoch: 15 | Loss:  1.3746\n",
            "Epoch: 16 | Loss:  1.3552\n",
            "Epoch: 17 | Loss:  1.3361\n",
            "Epoch: 18 | Loss:  1.3174\n",
            "Epoch: 19 | Loss:  1.2990\n",
            "Epoch: 20 | Loss:  1.2809\n",
            "Epoch: 21 | Loss:  1.2632\n",
            "Epoch: 22 | Loss:  1.2459\n",
            "Epoch: 23 | Loss:  1.2289\n",
            "Epoch: 24 | Loss:  1.2123\n",
            "Epoch: 25 | Loss:  1.1960\n",
            "Epoch: 26 | Loss:  1.1801\n",
            "Epoch: 27 | Loss:  1.1646\n",
            "Epoch: 28 | Loss:  1.1494\n",
            "Epoch: 29 | Loss:  1.1346\n",
            "Epoch: 30 | Loss:  1.1201\n",
            "Epoch: 31 | Loss:  1.1060\n",
            "Epoch: 32 | Loss:  1.0923\n",
            "Epoch: 33 | Loss:  1.0789\n",
            "Epoch: 34 | Loss:  1.0659\n",
            "Epoch: 35 | Loss:  1.0532\n",
            "Epoch: 36 | Loss:  1.0409\n",
            "Epoch: 37 | Loss:  1.0290\n",
            "Epoch: 38 | Loss:  1.0173\n",
            "Epoch: 39 | Loss:  1.0061\n",
            "Epoch: 40 | Loss:  0.9951\n",
            "Epoch: 41 | Loss:  0.9845\n",
            "Epoch: 42 | Loss:  0.9742\n",
            "Epoch: 43 | Loss:  0.9643\n",
            "Epoch: 44 | Loss:  0.9546\n",
            "Epoch: 45 | Loss:  0.9453\n",
            "Epoch: 46 | Loss:  0.9362\n",
            "Epoch: 47 | Loss:  0.9275\n",
            "Epoch: 48 | Loss:  0.9190\n",
            "Epoch: 49 | Loss:  0.9109\n",
            "Epoch: 50 | Loss:  0.9030\n",
            "Epoch: 51 | Loss:  0.8954\n",
            "Epoch: 52 | Loss:  0.8880\n",
            "Epoch: 53 | Loss:  0.8809\n",
            "Epoch: 54 | Loss:  0.8741\n",
            "Epoch: 55 | Loss:  0.8675\n",
            "Epoch: 56 | Loss:  0.8611\n",
            "Epoch: 57 | Loss:  0.8550\n",
            "Epoch: 58 | Loss:  0.8491\n",
            "Epoch: 59 | Loss:  0.8434\n",
            "Epoch: 60 | Loss:  0.8379\n",
            "Epoch: 61 | Loss:  0.8326\n",
            "Epoch: 62 | Loss:  0.8275\n",
            "Epoch: 63 | Loss:  0.8226\n",
            "Epoch: 64 | Loss:  0.8178\n",
            "Epoch: 65 | Loss:  0.8133\n",
            "Epoch: 66 | Loss:  0.8089\n",
            "Epoch: 67 | Loss:  0.8047\n",
            "Epoch: 68 | Loss:  0.8006\n",
            "Epoch: 69 | Loss:  0.7967\n",
            "Epoch: 70 | Loss:  0.7929\n",
            "Epoch: 71 | Loss:  0.7893\n",
            "Epoch: 72 | Loss:  0.7858\n",
            "Epoch: 73 | Loss:  0.7824\n",
            "Epoch: 74 | Loss:  0.7792\n",
            "Epoch: 75 | Loss:  0.7761\n",
            "Epoch: 76 | Loss:  0.7731\n",
            "Epoch: 77 | Loss:  0.7702\n",
            "Epoch: 78 | Loss:  0.7674\n",
            "Epoch: 79 | Loss:  0.7647\n",
            "Epoch: 80 | Loss:  0.7621\n",
            "Epoch: 81 | Loss:  0.7596\n",
            "Epoch: 82 | Loss:  0.7572\n",
            "Epoch: 83 | Loss:  0.7549\n",
            "Epoch: 84 | Loss:  0.7527\n",
            "Epoch: 85 | Loss:  0.7505\n",
            "Epoch: 86 | Loss:  0.7485\n",
            "Epoch: 87 | Loss:  0.7465\n",
            "Epoch: 88 | Loss:  0.7445\n",
            "Epoch: 89 | Loss:  0.7427\n",
            "Epoch: 90 | Loss:  0.7408\n",
            "Epoch: 91 | Loss:  0.7391\n",
            "Epoch: 92 | Loss:  0.7374\n",
            "Epoch: 93 | Loss:  0.7358\n",
            "Epoch: 94 | Loss:  0.7342\n",
            "Epoch: 95 | Loss:  0.7327\n",
            "Epoch: 96 | Loss:  0.7312\n",
            "Epoch: 97 | Loss:  0.7298\n",
            "Epoch: 98 | Loss:  0.7284\n",
            "Epoch: 99 | Loss:  0.7271\n",
            "Epoch: 100 | Loss:  0.7258\n",
            "Epoch: 101 | Loss:  0.7246\n",
            "Epoch: 102 | Loss:  0.7234\n",
            "Epoch: 103 | Loss:  0.7222\n",
            "Epoch: 104 | Loss:  0.7211\n",
            "Epoch: 105 | Loss:  0.7199\n",
            "Epoch: 106 | Loss:  0.7189\n",
            "Epoch: 107 | Loss:  0.7178\n",
            "Epoch: 108 | Loss:  0.7168\n",
            "Epoch: 109 | Loss:  0.7158\n",
            "Epoch: 110 | Loss:  0.7149\n",
            "Epoch: 111 | Loss:  0.7140\n",
            "Epoch: 112 | Loss:  0.7131\n",
            "Epoch: 113 | Loss:  0.7122\n",
            "Epoch: 114 | Loss:  0.7113\n",
            "Epoch: 115 | Loss:  0.7105\n",
            "Epoch: 116 | Loss:  0.7097\n",
            "Epoch: 117 | Loss:  0.7089\n",
            "Epoch: 118 | Loss:  0.7081\n",
            "Epoch: 119 | Loss:  0.7074\n",
            "Epoch: 120 | Loss:  0.7066\n",
            "Epoch: 121 | Loss:  0.7059\n",
            "Epoch: 122 | Loss:  0.7052\n",
            "Epoch: 123 | Loss:  0.7045\n",
            "Epoch: 124 | Loss:  0.7039\n",
            "Epoch: 125 | Loss:  0.7032\n",
            "Epoch: 126 | Loss:  0.7026\n",
            "Epoch: 127 | Loss:  0.7019\n",
            "Epoch: 128 | Loss:  0.7013\n",
            "Epoch: 129 | Loss:  0.7007\n",
            "Epoch: 130 | Loss:  0.7001\n",
            "Epoch: 131 | Loss:  0.6996\n",
            "Epoch: 132 | Loss:  0.6990\n",
            "Epoch: 133 | Loss:  0.6984\n",
            "Epoch: 134 | Loss:  0.6979\n",
            "Epoch: 135 | Loss:  0.6974\n",
            "Epoch: 136 | Loss:  0.6968\n",
            "Epoch: 137 | Loss:  0.6963\n",
            "Epoch: 138 | Loss:  0.6958\n",
            "Epoch: 139 | Loss:  0.6953\n",
            "Epoch: 140 | Loss:  0.6948\n",
            "Epoch: 141 | Loss:  0.6943\n",
            "Epoch: 142 | Loss:  0.6939\n",
            "Epoch: 143 | Loss:  0.6934\n",
            "Epoch: 144 | Loss:  0.6929\n",
            "Epoch: 145 | Loss:  0.6925\n",
            "Epoch: 146 | Loss:  0.6920\n",
            "Epoch: 147 | Loss:  0.6916\n",
            "Epoch: 148 | Loss:  0.6911\n",
            "Epoch: 149 | Loss:  0.6907\n",
            "Epoch: 150 | Loss:  0.6903\n",
            "Epoch: 151 | Loss:  0.6898\n",
            "Epoch: 152 | Loss:  0.6894\n",
            "Epoch: 153 | Loss:  0.6890\n",
            "Epoch: 154 | Loss:  0.6886\n",
            "Epoch: 155 | Loss:  0.6882\n",
            "Epoch: 156 | Loss:  0.6878\n",
            "Epoch: 157 | Loss:  0.6874\n",
            "Epoch: 158 | Loss:  0.6870\n",
            "Epoch: 159 | Loss:  0.6866\n",
            "Epoch: 160 | Loss:  0.6863\n",
            "Epoch: 161 | Loss:  0.6859\n",
            "Epoch: 162 | Loss:  0.6855\n",
            "Epoch: 163 | Loss:  0.6851\n",
            "Epoch: 164 | Loss:  0.6848\n",
            "Epoch: 165 | Loss:  0.6844\n",
            "Epoch: 166 | Loss:  0.6840\n",
            "Epoch: 167 | Loss:  0.6837\n",
            "Epoch: 168 | Loss:  0.6833\n",
            "Epoch: 169 | Loss:  0.6829\n",
            "Epoch: 170 | Loss:  0.6826\n",
            "Epoch: 171 | Loss:  0.6822\n",
            "Epoch: 172 | Loss:  0.6819\n",
            "Epoch: 173 | Loss:  0.6815\n",
            "Epoch: 174 | Loss:  0.6812\n",
            "Epoch: 175 | Loss:  0.6809\n",
            "Epoch: 176 | Loss:  0.6805\n",
            "Epoch: 177 | Loss:  0.6802\n",
            "Epoch: 178 | Loss:  0.6798\n",
            "Epoch: 179 | Loss:  0.6795\n",
            "Epoch: 180 | Loss:  0.6792\n",
            "Epoch: 181 | Loss:  0.6788\n",
            "Epoch: 182 | Loss:  0.6785\n",
            "Epoch: 183 | Loss:  0.6782\n",
            "Epoch: 184 | Loss:  0.6779\n",
            "Epoch: 185 | Loss:  0.6775\n",
            "Epoch: 186 | Loss:  0.6772\n",
            "Epoch: 187 | Loss:  0.6769\n",
            "Epoch: 188 | Loss:  0.6766\n",
            "Epoch: 189 | Loss:  0.6763\n",
            "Epoch: 190 | Loss:  0.6759\n",
            "Epoch: 191 | Loss:  0.6756\n",
            "Epoch: 192 | Loss:  0.6753\n",
            "Epoch: 193 | Loss:  0.6750\n",
            "Epoch: 194 | Loss:  0.6747\n",
            "Epoch: 195 | Loss:  0.6744\n",
            "Epoch: 196 | Loss:  0.6741\n",
            "Epoch: 197 | Loss:  0.6738\n",
            "Epoch: 198 | Loss:  0.6734\n",
            "Epoch: 199 | Loss:  0.6731\n",
            "Epoch: 200 | Loss:  0.6728\n",
            "Epoch: 201 | Loss:  0.6725\n",
            "Epoch: 202 | Loss:  0.6722\n",
            "Epoch: 203 | Loss:  0.6719\n",
            "Epoch: 204 | Loss:  0.6716\n",
            "Epoch: 205 | Loss:  0.6713\n",
            "Epoch: 206 | Loss:  0.6710\n",
            "Epoch: 207 | Loss:  0.6707\n",
            "Epoch: 208 | Loss:  0.6704\n",
            "Epoch: 209 | Loss:  0.6701\n",
            "Epoch: 210 | Loss:  0.6698\n",
            "Epoch: 211 | Loss:  0.6695\n",
            "Epoch: 212 | Loss:  0.6692\n",
            "Epoch: 213 | Loss:  0.6689\n",
            "Epoch: 214 | Loss:  0.6686\n",
            "Epoch: 215 | Loss:  0.6683\n",
            "Epoch: 216 | Loss:  0.6680\n",
            "Epoch: 217 | Loss:  0.6677\n",
            "Epoch: 218 | Loss:  0.6674\n",
            "Epoch: 219 | Loss:  0.6672\n",
            "Epoch: 220 | Loss:  0.6669\n",
            "Epoch: 221 | Loss:  0.6666\n",
            "Epoch: 222 | Loss:  0.6663\n",
            "Epoch: 223 | Loss:  0.6660\n",
            "Epoch: 224 | Loss:  0.6657\n",
            "Epoch: 225 | Loss:  0.6654\n",
            "Epoch: 226 | Loss:  0.6651\n",
            "Epoch: 227 | Loss:  0.6648\n",
            "Epoch: 228 | Loss:  0.6645\n",
            "Epoch: 229 | Loss:  0.6642\n",
            "Epoch: 230 | Loss:  0.6640\n",
            "Epoch: 231 | Loss:  0.6637\n",
            "Epoch: 232 | Loss:  0.6634\n",
            "Epoch: 233 | Loss:  0.6631\n",
            "Epoch: 234 | Loss:  0.6628\n",
            "Epoch: 235 | Loss:  0.6625\n",
            "Epoch: 236 | Loss:  0.6622\n",
            "Epoch: 237 | Loss:  0.6620\n",
            "Epoch: 238 | Loss:  0.6617\n",
            "Epoch: 239 | Loss:  0.6614\n",
            "Epoch: 240 | Loss:  0.6611\n",
            "Epoch: 241 | Loss:  0.6608\n",
            "Epoch: 242 | Loss:  0.6605\n",
            "Epoch: 243 | Loss:  0.6603\n",
            "Epoch: 244 | Loss:  0.6600\n",
            "Epoch: 245 | Loss:  0.6597\n",
            "Epoch: 246 | Loss:  0.6594\n",
            "Epoch: 247 | Loss:  0.6591\n",
            "Epoch: 248 | Loss:  0.6588\n",
            "Epoch: 249 | Loss:  0.6586\n",
            "Epoch: 250 | Loss:  0.6583\n",
            "Epoch: 251 | Loss:  0.6580\n",
            "Epoch: 252 | Loss:  0.6577\n",
            "Epoch: 253 | Loss:  0.6574\n",
            "Epoch: 254 | Loss:  0.6572\n",
            "Epoch: 255 | Loss:  0.6569\n",
            "Epoch: 256 | Loss:  0.6566\n",
            "Epoch: 257 | Loss:  0.6563\n",
            "Epoch: 258 | Loss:  0.6561\n",
            "Epoch: 259 | Loss:  0.6558\n",
            "Epoch: 260 | Loss:  0.6555\n",
            "Epoch: 261 | Loss:  0.6552\n",
            "Epoch: 262 | Loss:  0.6549\n",
            "Epoch: 263 | Loss:  0.6547\n",
            "Epoch: 264 | Loss:  0.6544\n",
            "Epoch: 265 | Loss:  0.6541\n",
            "Epoch: 266 | Loss:  0.6538\n",
            "Epoch: 267 | Loss:  0.6536\n",
            "Epoch: 268 | Loss:  0.6533\n",
            "Epoch: 269 | Loss:  0.6530\n",
            "Epoch: 270 | Loss:  0.6527\n",
            "Epoch: 271 | Loss:  0.6525\n",
            "Epoch: 272 | Loss:  0.6522\n",
            "Epoch: 273 | Loss:  0.6519\n",
            "Epoch: 274 | Loss:  0.6516\n",
            "Epoch: 275 | Loss:  0.6514\n",
            "Epoch: 276 | Loss:  0.6511\n",
            "Epoch: 277 | Loss:  0.6508\n",
            "Epoch: 278 | Loss:  0.6505\n",
            "Epoch: 279 | Loss:  0.6503\n",
            "Epoch: 280 | Loss:  0.6500\n",
            "Epoch: 281 | Loss:  0.6497\n",
            "Epoch: 282 | Loss:  0.6495\n",
            "Epoch: 283 | Loss:  0.6492\n",
            "Epoch: 284 | Loss:  0.6489\n",
            "Epoch: 285 | Loss:  0.6486\n",
            "Epoch: 286 | Loss:  0.6484\n",
            "Epoch: 287 | Loss:  0.6481\n",
            "Epoch: 288 | Loss:  0.6478\n",
            "Epoch: 289 | Loss:  0.6476\n",
            "Epoch: 290 | Loss:  0.6473\n",
            "Epoch: 291 | Loss:  0.6470\n",
            "Epoch: 292 | Loss:  0.6467\n",
            "Epoch: 293 | Loss:  0.6465\n",
            "Epoch: 294 | Loss:  0.6462\n",
            "Epoch: 295 | Loss:  0.6459\n",
            "Epoch: 296 | Loss:  0.6457\n",
            "Epoch: 297 | Loss:  0.6454\n",
            "Epoch: 298 | Loss:  0.6451\n",
            "Epoch: 299 | Loss:  0.6449\n",
            "Epoch: 300 | Loss:  0.6446\n",
            "Epoch: 301 | Loss:  0.6443\n",
            "Epoch: 302 | Loss:  0.6440\n",
            "Epoch: 303 | Loss:  0.6438\n",
            "Epoch: 304 | Loss:  0.6435\n",
            "Epoch: 305 | Loss:  0.6432\n",
            "Epoch: 306 | Loss:  0.6430\n",
            "Epoch: 307 | Loss:  0.6427\n",
            "Epoch: 308 | Loss:  0.6424\n",
            "Epoch: 309 | Loss:  0.6422\n",
            "Epoch: 310 | Loss:  0.6419\n",
            "Epoch: 311 | Loss:  0.6416\n",
            "Epoch: 312 | Loss:  0.6414\n",
            "Epoch: 313 | Loss:  0.6411\n",
            "Epoch: 314 | Loss:  0.6408\n",
            "Epoch: 315 | Loss:  0.6406\n",
            "Epoch: 316 | Loss:  0.6403\n",
            "Epoch: 317 | Loss:  0.6401\n",
            "Epoch: 318 | Loss:  0.6398\n",
            "Epoch: 319 | Loss:  0.6395\n",
            "Epoch: 320 | Loss:  0.6393\n",
            "Epoch: 321 | Loss:  0.6390\n",
            "Epoch: 322 | Loss:  0.6387\n",
            "Epoch: 323 | Loss:  0.6385\n",
            "Epoch: 324 | Loss:  0.6382\n",
            "Epoch: 325 | Loss:  0.6379\n",
            "Epoch: 326 | Loss:  0.6377\n",
            "Epoch: 327 | Loss:  0.6374\n",
            "Epoch: 328 | Loss:  0.6371\n",
            "Epoch: 329 | Loss:  0.6369\n",
            "Epoch: 330 | Loss:  0.6366\n",
            "Epoch: 331 | Loss:  0.6364\n",
            "Epoch: 332 | Loss:  0.6361\n",
            "Epoch: 333 | Loss:  0.6358\n",
            "Epoch: 334 | Loss:  0.6356\n",
            "Epoch: 335 | Loss:  0.6353\n",
            "Epoch: 336 | Loss:  0.6351\n",
            "Epoch: 337 | Loss:  0.6348\n",
            "Epoch: 338 | Loss:  0.6345\n",
            "Epoch: 339 | Loss:  0.6343\n",
            "Epoch: 340 | Loss:  0.6340\n",
            "Epoch: 341 | Loss:  0.6337\n",
            "Epoch: 342 | Loss:  0.6335\n",
            "Epoch: 343 | Loss:  0.6332\n",
            "Epoch: 344 | Loss:  0.6330\n",
            "Epoch: 345 | Loss:  0.6327\n",
            "Epoch: 346 | Loss:  0.6324\n",
            "Epoch: 347 | Loss:  0.6322\n",
            "Epoch: 348 | Loss:  0.6319\n",
            "Epoch: 349 | Loss:  0.6317\n",
            "Epoch: 350 | Loss:  0.6314\n",
            "Epoch: 351 | Loss:  0.6311\n",
            "Epoch: 352 | Loss:  0.6309\n",
            "Epoch: 353 | Loss:  0.6306\n",
            "Epoch: 354 | Loss:  0.6304\n",
            "Epoch: 355 | Loss:  0.6301\n",
            "Epoch: 356 | Loss:  0.6299\n",
            "Epoch: 357 | Loss:  0.6296\n",
            "Epoch: 358 | Loss:  0.6293\n",
            "Epoch: 359 | Loss:  0.6291\n",
            "Epoch: 360 | Loss:  0.6288\n",
            "Epoch: 361 | Loss:  0.6286\n",
            "Epoch: 362 | Loss:  0.6283\n",
            "Epoch: 363 | Loss:  0.6281\n",
            "Epoch: 364 | Loss:  0.6278\n",
            "Epoch: 365 | Loss:  0.6275\n",
            "Epoch: 366 | Loss:  0.6273\n",
            "Epoch: 367 | Loss:  0.6270\n",
            "Epoch: 368 | Loss:  0.6268\n",
            "Epoch: 369 | Loss:  0.6265\n",
            "Epoch: 370 | Loss:  0.6263\n",
            "Epoch: 371 | Loss:  0.6260\n",
            "Epoch: 372 | Loss:  0.6258\n",
            "Epoch: 373 | Loss:  0.6255\n",
            "Epoch: 374 | Loss:  0.6252\n",
            "Epoch: 375 | Loss:  0.6250\n",
            "Epoch: 376 | Loss:  0.6247\n",
            "Epoch: 377 | Loss:  0.6245\n",
            "Epoch: 378 | Loss:  0.6242\n",
            "Epoch: 379 | Loss:  0.6240\n",
            "Epoch: 380 | Loss:  0.6237\n",
            "Epoch: 381 | Loss:  0.6235\n",
            "Epoch: 382 | Loss:  0.6232\n",
            "Epoch: 383 | Loss:  0.6230\n",
            "Epoch: 384 | Loss:  0.6227\n",
            "Epoch: 385 | Loss:  0.6225\n",
            "Epoch: 386 | Loss:  0.6222\n",
            "Epoch: 387 | Loss:  0.6220\n",
            "Epoch: 388 | Loss:  0.6217\n",
            "Epoch: 389 | Loss:  0.6215\n",
            "Epoch: 390 | Loss:  0.6212\n",
            "Epoch: 391 | Loss:  0.6209\n",
            "Epoch: 392 | Loss:  0.6207\n",
            "Epoch: 393 | Loss:  0.6204\n",
            "Epoch: 394 | Loss:  0.6202\n",
            "Epoch: 395 | Loss:  0.6199\n",
            "Epoch: 396 | Loss:  0.6197\n",
            "Epoch: 397 | Loss:  0.6194\n",
            "Epoch: 398 | Loss:  0.6192\n",
            "Epoch: 399 | Loss:  0.6189\n",
            "Epoch: 400 | Loss:  0.6187\n",
            "Epoch: 401 | Loss:  0.6184\n",
            "Epoch: 402 | Loss:  0.6182\n",
            "Epoch: 403 | Loss:  0.6179\n",
            "Epoch: 404 | Loss:  0.6177\n",
            "Epoch: 405 | Loss:  0.6174\n",
            "Epoch: 406 | Loss:  0.6172\n",
            "Epoch: 407 | Loss:  0.6169\n",
            "Epoch: 408 | Loss:  0.6167\n",
            "Epoch: 409 | Loss:  0.6164\n",
            "Epoch: 410 | Loss:  0.6162\n",
            "Epoch: 411 | Loss:  0.6160\n",
            "Epoch: 412 | Loss:  0.6157\n",
            "Epoch: 413 | Loss:  0.6155\n",
            "Epoch: 414 | Loss:  0.6152\n",
            "Epoch: 415 | Loss:  0.6150\n",
            "Epoch: 416 | Loss:  0.6147\n",
            "Epoch: 417 | Loss:  0.6145\n",
            "Epoch: 418 | Loss:  0.6142\n",
            "Epoch: 419 | Loss:  0.6140\n",
            "Epoch: 420 | Loss:  0.6137\n",
            "Epoch: 421 | Loss:  0.6135\n",
            "Epoch: 422 | Loss:  0.6132\n",
            "Epoch: 423 | Loss:  0.6130\n",
            "Epoch: 424 | Loss:  0.6127\n",
            "Epoch: 425 | Loss:  0.6125\n",
            "Epoch: 426 | Loss:  0.6123\n",
            "Epoch: 427 | Loss:  0.6120\n",
            "Epoch: 428 | Loss:  0.6118\n",
            "Epoch: 429 | Loss:  0.6115\n",
            "Epoch: 430 | Loss:  0.6113\n",
            "Epoch: 431 | Loss:  0.6110\n",
            "Epoch: 432 | Loss:  0.6108\n",
            "Epoch: 433 | Loss:  0.6105\n",
            "Epoch: 434 | Loss:  0.6103\n",
            "Epoch: 435 | Loss:  0.6101\n",
            "Epoch: 436 | Loss:  0.6098\n",
            "Epoch: 437 | Loss:  0.6096\n",
            "Epoch: 438 | Loss:  0.6093\n",
            "Epoch: 439 | Loss:  0.6091\n",
            "Epoch: 440 | Loss:  0.6088\n",
            "Epoch: 441 | Loss:  0.6086\n",
            "Epoch: 442 | Loss:  0.6083\n",
            "Epoch: 443 | Loss:  0.6081\n",
            "Epoch: 444 | Loss:  0.6079\n",
            "Epoch: 445 | Loss:  0.6076\n",
            "Epoch: 446 | Loss:  0.6074\n",
            "Epoch: 447 | Loss:  0.6071\n",
            "Epoch: 448 | Loss:  0.6069\n",
            "Epoch: 449 | Loss:  0.6067\n",
            "Epoch: 450 | Loss:  0.6064\n",
            "Epoch: 451 | Loss:  0.6062\n",
            "Epoch: 452 | Loss:  0.6059\n",
            "Epoch: 453 | Loss:  0.6057\n",
            "Epoch: 454 | Loss:  0.6054\n",
            "Epoch: 455 | Loss:  0.6052\n",
            "Epoch: 456 | Loss:  0.6050\n",
            "Epoch: 457 | Loss:  0.6047\n",
            "Epoch: 458 | Loss:  0.6045\n",
            "Epoch: 459 | Loss:  0.6042\n",
            "Epoch: 460 | Loss:  0.6040\n",
            "Epoch: 461 | Loss:  0.6038\n",
            "Epoch: 462 | Loss:  0.6035\n",
            "Epoch: 463 | Loss:  0.6033\n",
            "Epoch: 464 | Loss:  0.6031\n",
            "Epoch: 465 | Loss:  0.6028\n",
            "Epoch: 466 | Loss:  0.6026\n",
            "Epoch: 467 | Loss:  0.6023\n",
            "Epoch: 468 | Loss:  0.6021\n",
            "Epoch: 469 | Loss:  0.6019\n",
            "Epoch: 470 | Loss:  0.6016\n",
            "Epoch: 471 | Loss:  0.6014\n",
            "Epoch: 472 | Loss:  0.6011\n",
            "Epoch: 473 | Loss:  0.6009\n",
            "Epoch: 474 | Loss:  0.6007\n",
            "Epoch: 475 | Loss:  0.6004\n",
            "Epoch: 476 | Loss:  0.6002\n",
            "Epoch: 477 | Loss:  0.6000\n",
            "Epoch: 478 | Loss:  0.5997\n",
            "Epoch: 479 | Loss:  0.5995\n",
            "Epoch: 480 | Loss:  0.5992\n",
            "Epoch: 481 | Loss:  0.5990\n",
            "Epoch: 482 | Loss:  0.5988\n",
            "Epoch: 483 | Loss:  0.5985\n",
            "Epoch: 484 | Loss:  0.5983\n",
            "Epoch: 485 | Loss:  0.5981\n",
            "Epoch: 486 | Loss:  0.5978\n",
            "Epoch: 487 | Loss:  0.5976\n",
            "Epoch: 488 | Loss:  0.5974\n",
            "Epoch: 489 | Loss:  0.5971\n",
            "Epoch: 490 | Loss:  0.5969\n",
            "Epoch: 491 | Loss:  0.5967\n",
            "Epoch: 492 | Loss:  0.5964\n",
            "Epoch: 493 | Loss:  0.5962\n",
            "Epoch: 494 | Loss:  0.5960\n",
            "Epoch: 495 | Loss:  0.5957\n",
            "Epoch: 496 | Loss:  0.5955\n",
            "Epoch: 497 | Loss:  0.5953\n",
            "Epoch: 498 | Loss:  0.5950\n",
            "Epoch: 499 | Loss:  0.5948\n",
            "Epoch: 500 | Loss:  0.5946\n",
            "Epoch: 501 | Loss:  0.5943\n",
            "Epoch: 502 | Loss:  0.5941\n",
            "Epoch: 503 | Loss:  0.5939\n",
            "Epoch: 504 | Loss:  0.5936\n",
            "Epoch: 505 | Loss:  0.5934\n",
            "Epoch: 506 | Loss:  0.5932\n",
            "Epoch: 507 | Loss:  0.5929\n",
            "Epoch: 508 | Loss:  0.5927\n",
            "Epoch: 509 | Loss:  0.5925\n",
            "Epoch: 510 | Loss:  0.5922\n",
            "Epoch: 511 | Loss:  0.5920\n",
            "Epoch: 512 | Loss:  0.5918\n",
            "Epoch: 513 | Loss:  0.5915\n",
            "Epoch: 514 | Loss:  0.5913\n",
            "Epoch: 515 | Loss:  0.5911\n",
            "Epoch: 516 | Loss:  0.5908\n",
            "Epoch: 517 | Loss:  0.5906\n",
            "Epoch: 518 | Loss:  0.5904\n",
            "Epoch: 519 | Loss:  0.5902\n",
            "Epoch: 520 | Loss:  0.5899\n",
            "Epoch: 521 | Loss:  0.5897\n",
            "Epoch: 522 | Loss:  0.5895\n",
            "Epoch: 523 | Loss:  0.5892\n",
            "Epoch: 524 | Loss:  0.5890\n",
            "Epoch: 525 | Loss:  0.5888\n",
            "Epoch: 526 | Loss:  0.5886\n",
            "Epoch: 527 | Loss:  0.5883\n",
            "Epoch: 528 | Loss:  0.5881\n",
            "Epoch: 529 | Loss:  0.5879\n",
            "Epoch: 530 | Loss:  0.5876\n",
            "Epoch: 531 | Loss:  0.5874\n",
            "Epoch: 532 | Loss:  0.5872\n",
            "Epoch: 533 | Loss:  0.5870\n",
            "Epoch: 534 | Loss:  0.5867\n",
            "Epoch: 535 | Loss:  0.5865\n",
            "Epoch: 536 | Loss:  0.5863\n",
            "Epoch: 537 | Loss:  0.5860\n",
            "Epoch: 538 | Loss:  0.5858\n",
            "Epoch: 539 | Loss:  0.5856\n",
            "Epoch: 540 | Loss:  0.5854\n",
            "Epoch: 541 | Loss:  0.5851\n",
            "Epoch: 542 | Loss:  0.5849\n",
            "Epoch: 543 | Loss:  0.5847\n",
            "Epoch: 544 | Loss:  0.5845\n",
            "Epoch: 545 | Loss:  0.5842\n",
            "Epoch: 546 | Loss:  0.5840\n",
            "Epoch: 547 | Loss:  0.5838\n",
            "Epoch: 548 | Loss:  0.5836\n",
            "Epoch: 549 | Loss:  0.5833\n",
            "Epoch: 550 | Loss:  0.5831\n",
            "Epoch: 551 | Loss:  0.5829\n",
            "Epoch: 552 | Loss:  0.5827\n",
            "Epoch: 553 | Loss:  0.5824\n",
            "Epoch: 554 | Loss:  0.5822\n",
            "Epoch: 555 | Loss:  0.5820\n",
            "Epoch: 556 | Loss:  0.5818\n",
            "Epoch: 557 | Loss:  0.5815\n",
            "Epoch: 558 | Loss:  0.5813\n",
            "Epoch: 559 | Loss:  0.5811\n",
            "Epoch: 560 | Loss:  0.5809\n",
            "Epoch: 561 | Loss:  0.5806\n",
            "Epoch: 562 | Loss:  0.5804\n",
            "Epoch: 563 | Loss:  0.5802\n",
            "Epoch: 564 | Loss:  0.5800\n",
            "Epoch: 565 | Loss:  0.5798\n",
            "Epoch: 566 | Loss:  0.5795\n",
            "Epoch: 567 | Loss:  0.5793\n",
            "Epoch: 568 | Loss:  0.5791\n",
            "Epoch: 569 | Loss:  0.5789\n",
            "Epoch: 570 | Loss:  0.5786\n",
            "Epoch: 571 | Loss:  0.5784\n",
            "Epoch: 572 | Loss:  0.5782\n",
            "Epoch: 573 | Loss:  0.5780\n",
            "Epoch: 574 | Loss:  0.5778\n",
            "Epoch: 575 | Loss:  0.5775\n",
            "Epoch: 576 | Loss:  0.5773\n",
            "Epoch: 577 | Loss:  0.5771\n",
            "Epoch: 578 | Loss:  0.5769\n",
            "Epoch: 579 | Loss:  0.5767\n",
            "Epoch: 580 | Loss:  0.5764\n",
            "Epoch: 581 | Loss:  0.5762\n",
            "Epoch: 582 | Loss:  0.5760\n",
            "Epoch: 583 | Loss:  0.5758\n",
            "Epoch: 584 | Loss:  0.5755\n",
            "Epoch: 585 | Loss:  0.5753\n",
            "Epoch: 586 | Loss:  0.5751\n",
            "Epoch: 587 | Loss:  0.5749\n",
            "Epoch: 588 | Loss:  0.5747\n",
            "Epoch: 589 | Loss:  0.5745\n",
            "Epoch: 590 | Loss:  0.5742\n",
            "Epoch: 591 | Loss:  0.5740\n",
            "Epoch: 592 | Loss:  0.5738\n",
            "Epoch: 593 | Loss:  0.5736\n",
            "Epoch: 594 | Loss:  0.5734\n",
            "Epoch: 595 | Loss:  0.5731\n",
            "Epoch: 596 | Loss:  0.5729\n",
            "Epoch: 597 | Loss:  0.5727\n",
            "Epoch: 598 | Loss:  0.5725\n",
            "Epoch: 599 | Loss:  0.5723\n",
            "Epoch: 600 | Loss:  0.5721\n",
            "Epoch: 601 | Loss:  0.5718\n",
            "Epoch: 602 | Loss:  0.5716\n",
            "Epoch: 603 | Loss:  0.5714\n",
            "Epoch: 604 | Loss:  0.5712\n",
            "Epoch: 605 | Loss:  0.5710\n",
            "Epoch: 606 | Loss:  0.5708\n",
            "Epoch: 607 | Loss:  0.5705\n",
            "Epoch: 608 | Loss:  0.5703\n",
            "Epoch: 609 | Loss:  0.5701\n",
            "Epoch: 610 | Loss:  0.5699\n",
            "Epoch: 611 | Loss:  0.5697\n",
            "Epoch: 612 | Loss:  0.5695\n",
            "Epoch: 613 | Loss:  0.5692\n",
            "Epoch: 614 | Loss:  0.5690\n",
            "Epoch: 615 | Loss:  0.5688\n",
            "Epoch: 616 | Loss:  0.5686\n",
            "Epoch: 617 | Loss:  0.5684\n",
            "Epoch: 618 | Loss:  0.5682\n",
            "Epoch: 619 | Loss:  0.5680\n",
            "Epoch: 620 | Loss:  0.5677\n",
            "Epoch: 621 | Loss:  0.5675\n",
            "Epoch: 622 | Loss:  0.5673\n",
            "Epoch: 623 | Loss:  0.5671\n",
            "Epoch: 624 | Loss:  0.5669\n",
            "Epoch: 625 | Loss:  0.5667\n",
            "Epoch: 626 | Loss:  0.5665\n",
            "Epoch: 627 | Loss:  0.5662\n",
            "Epoch: 628 | Loss:  0.5660\n",
            "Epoch: 629 | Loss:  0.5658\n",
            "Epoch: 630 | Loss:  0.5656\n",
            "Epoch: 631 | Loss:  0.5654\n",
            "Epoch: 632 | Loss:  0.5652\n",
            "Epoch: 633 | Loss:  0.5650\n",
            "Epoch: 634 | Loss:  0.5648\n",
            "Epoch: 635 | Loss:  0.5645\n",
            "Epoch: 636 | Loss:  0.5643\n",
            "Epoch: 637 | Loss:  0.5641\n",
            "Epoch: 638 | Loss:  0.5639\n",
            "Epoch: 639 | Loss:  0.5637\n",
            "Epoch: 640 | Loss:  0.5635\n",
            "Epoch: 641 | Loss:  0.5633\n",
            "Epoch: 642 | Loss:  0.5631\n",
            "Epoch: 643 | Loss:  0.5629\n",
            "Epoch: 644 | Loss:  0.5626\n",
            "Epoch: 645 | Loss:  0.5624\n",
            "Epoch: 646 | Loss:  0.5622\n",
            "Epoch: 647 | Loss:  0.5620\n",
            "Epoch: 648 | Loss:  0.5618\n",
            "Epoch: 649 | Loss:  0.5616\n",
            "Epoch: 650 | Loss:  0.5614\n",
            "Epoch: 651 | Loss:  0.5612\n",
            "Epoch: 652 | Loss:  0.5610\n",
            "Epoch: 653 | Loss:  0.5608\n",
            "Epoch: 654 | Loss:  0.5605\n",
            "Epoch: 655 | Loss:  0.5603\n",
            "Epoch: 656 | Loss:  0.5601\n",
            "Epoch: 657 | Loss:  0.5599\n",
            "Epoch: 658 | Loss:  0.5597\n",
            "Epoch: 659 | Loss:  0.5595\n",
            "Epoch: 660 | Loss:  0.5593\n",
            "Epoch: 661 | Loss:  0.5591\n",
            "Epoch: 662 | Loss:  0.5589\n",
            "Epoch: 663 | Loss:  0.5587\n",
            "Epoch: 664 | Loss:  0.5585\n",
            "Epoch: 665 | Loss:  0.5582\n",
            "Epoch: 666 | Loss:  0.5580\n",
            "Epoch: 667 | Loss:  0.5578\n",
            "Epoch: 668 | Loss:  0.5576\n",
            "Epoch: 669 | Loss:  0.5574\n",
            "Epoch: 670 | Loss:  0.5572\n",
            "Epoch: 671 | Loss:  0.5570\n",
            "Epoch: 672 | Loss:  0.5568\n",
            "Epoch: 673 | Loss:  0.5566\n",
            "Epoch: 674 | Loss:  0.5564\n",
            "Epoch: 675 | Loss:  0.5562\n",
            "Epoch: 676 | Loss:  0.5560\n",
            "Epoch: 677 | Loss:  0.5558\n",
            "Epoch: 678 | Loss:  0.5556\n",
            "Epoch: 679 | Loss:  0.5554\n",
            "Epoch: 680 | Loss:  0.5552\n",
            "Epoch: 681 | Loss:  0.5549\n",
            "Epoch: 682 | Loss:  0.5547\n",
            "Epoch: 683 | Loss:  0.5545\n",
            "Epoch: 684 | Loss:  0.5543\n",
            "Epoch: 685 | Loss:  0.5541\n",
            "Epoch: 686 | Loss:  0.5539\n",
            "Epoch: 687 | Loss:  0.5537\n",
            "Epoch: 688 | Loss:  0.5535\n",
            "Epoch: 689 | Loss:  0.5533\n",
            "Epoch: 690 | Loss:  0.5531\n",
            "Epoch: 691 | Loss:  0.5529\n",
            "Epoch: 692 | Loss:  0.5527\n",
            "Epoch: 693 | Loss:  0.5525\n",
            "Epoch: 694 | Loss:  0.5523\n",
            "Epoch: 695 | Loss:  0.5521\n",
            "Epoch: 696 | Loss:  0.5519\n",
            "Epoch: 697 | Loss:  0.5517\n",
            "Epoch: 698 | Loss:  0.5515\n",
            "Epoch: 699 | Loss:  0.5513\n",
            "Epoch: 700 | Loss:  0.5511\n",
            "Epoch: 701 | Loss:  0.5509\n",
            "Epoch: 702 | Loss:  0.5507\n",
            "Epoch: 703 | Loss:  0.5505\n",
            "Epoch: 704 | Loss:  0.5503\n",
            "Epoch: 705 | Loss:  0.5501\n",
            "Epoch: 706 | Loss:  0.5499\n",
            "Epoch: 707 | Loss:  0.5497\n",
            "Epoch: 708 | Loss:  0.5495\n",
            "Epoch: 709 | Loss:  0.5493\n",
            "Epoch: 710 | Loss:  0.5491\n",
            "Epoch: 711 | Loss:  0.5488\n",
            "Epoch: 712 | Loss:  0.5486\n",
            "Epoch: 713 | Loss:  0.5484\n",
            "Epoch: 714 | Loss:  0.5482\n",
            "Epoch: 715 | Loss:  0.5480\n",
            "Epoch: 716 | Loss:  0.5478\n",
            "Epoch: 717 | Loss:  0.5476\n",
            "Epoch: 718 | Loss:  0.5474\n",
            "Epoch: 719 | Loss:  0.5472\n",
            "Epoch: 720 | Loss:  0.5470\n",
            "Epoch: 721 | Loss:  0.5468\n",
            "Epoch: 722 | Loss:  0.5466\n",
            "Epoch: 723 | Loss:  0.5464\n",
            "Epoch: 724 | Loss:  0.5462\n",
            "Epoch: 725 | Loss:  0.5460\n",
            "Epoch: 726 | Loss:  0.5458\n",
            "Epoch: 727 | Loss:  0.5457\n",
            "Epoch: 728 | Loss:  0.5455\n",
            "Epoch: 729 | Loss:  0.5453\n",
            "Epoch: 730 | Loss:  0.5451\n",
            "Epoch: 731 | Loss:  0.5449\n",
            "Epoch: 732 | Loss:  0.5447\n",
            "Epoch: 733 | Loss:  0.5445\n",
            "Epoch: 734 | Loss:  0.5443\n",
            "Epoch: 735 | Loss:  0.5441\n",
            "Epoch: 736 | Loss:  0.5439\n",
            "Epoch: 737 | Loss:  0.5437\n",
            "Epoch: 738 | Loss:  0.5435\n",
            "Epoch: 739 | Loss:  0.5433\n",
            "Epoch: 740 | Loss:  0.5431\n",
            "Epoch: 741 | Loss:  0.5429\n",
            "Epoch: 742 | Loss:  0.5427\n",
            "Epoch: 743 | Loss:  0.5425\n",
            "Epoch: 744 | Loss:  0.5423\n",
            "Epoch: 745 | Loss:  0.5421\n",
            "Epoch: 746 | Loss:  0.5419\n",
            "Epoch: 747 | Loss:  0.5417\n",
            "Epoch: 748 | Loss:  0.5415\n",
            "Epoch: 749 | Loss:  0.5413\n",
            "Epoch: 750 | Loss:  0.5411\n",
            "Epoch: 751 | Loss:  0.5409\n",
            "Epoch: 752 | Loss:  0.5407\n",
            "Epoch: 753 | Loss:  0.5405\n",
            "Epoch: 754 | Loss:  0.5403\n",
            "Epoch: 755 | Loss:  0.5401\n",
            "Epoch: 756 | Loss:  0.5399\n",
            "Epoch: 757 | Loss:  0.5397\n",
            "Epoch: 758 | Loss:  0.5396\n",
            "Epoch: 759 | Loss:  0.5394\n",
            "Epoch: 760 | Loss:  0.5392\n",
            "Epoch: 761 | Loss:  0.5390\n",
            "Epoch: 762 | Loss:  0.5388\n",
            "Epoch: 763 | Loss:  0.5386\n",
            "Epoch: 764 | Loss:  0.5384\n",
            "Epoch: 765 | Loss:  0.5382\n",
            "Epoch: 766 | Loss:  0.5380\n",
            "Epoch: 767 | Loss:  0.5378\n",
            "Epoch: 768 | Loss:  0.5376\n",
            "Epoch: 769 | Loss:  0.5374\n",
            "Epoch: 770 | Loss:  0.5372\n",
            "Epoch: 771 | Loss:  0.5370\n",
            "Epoch: 772 | Loss:  0.5368\n",
            "Epoch: 773 | Loss:  0.5366\n",
            "Epoch: 774 | Loss:  0.5365\n",
            "Epoch: 775 | Loss:  0.5363\n",
            "Epoch: 776 | Loss:  0.5361\n",
            "Epoch: 777 | Loss:  0.5359\n",
            "Epoch: 778 | Loss:  0.5357\n",
            "Epoch: 779 | Loss:  0.5355\n",
            "Epoch: 780 | Loss:  0.5353\n",
            "Epoch: 781 | Loss:  0.5351\n",
            "Epoch: 782 | Loss:  0.5349\n",
            "Epoch: 783 | Loss:  0.5347\n",
            "Epoch: 784 | Loss:  0.5345\n",
            "Epoch: 785 | Loss:  0.5343\n",
            "Epoch: 786 | Loss:  0.5342\n",
            "Epoch: 787 | Loss:  0.5340\n",
            "Epoch: 788 | Loss:  0.5338\n",
            "Epoch: 789 | Loss:  0.5336\n",
            "Epoch: 790 | Loss:  0.5334\n",
            "Epoch: 791 | Loss:  0.5332\n",
            "Epoch: 792 | Loss:  0.5330\n",
            "Epoch: 793 | Loss:  0.5328\n",
            "Epoch: 794 | Loss:  0.5326\n",
            "Epoch: 795 | Loss:  0.5324\n",
            "Epoch: 796 | Loss:  0.5323\n",
            "Epoch: 797 | Loss:  0.5321\n",
            "Epoch: 798 | Loss:  0.5319\n",
            "Epoch: 799 | Loss:  0.5317\n",
            "Epoch: 800 | Loss:  0.5315\n",
            "Epoch: 801 | Loss:  0.5313\n",
            "Epoch: 802 | Loss:  0.5311\n",
            "Epoch: 803 | Loss:  0.5309\n",
            "Epoch: 804 | Loss:  0.5307\n",
            "Epoch: 805 | Loss:  0.5306\n",
            "Epoch: 806 | Loss:  0.5304\n",
            "Epoch: 807 | Loss:  0.5302\n",
            "Epoch: 808 | Loss:  0.5300\n",
            "Epoch: 809 | Loss:  0.5298\n",
            "Epoch: 810 | Loss:  0.5296\n",
            "Epoch: 811 | Loss:  0.5294\n",
            "Epoch: 812 | Loss:  0.5292\n",
            "Epoch: 813 | Loss:  0.5290\n",
            "Epoch: 814 | Loss:  0.5289\n",
            "Epoch: 815 | Loss:  0.5287\n",
            "Epoch: 816 | Loss:  0.5285\n",
            "Epoch: 817 | Loss:  0.5283\n",
            "Epoch: 818 | Loss:  0.5281\n",
            "Epoch: 819 | Loss:  0.5279\n",
            "Epoch: 820 | Loss:  0.5277\n",
            "Epoch: 821 | Loss:  0.5276\n",
            "Epoch: 822 | Loss:  0.5274\n",
            "Epoch: 823 | Loss:  0.5272\n",
            "Epoch: 824 | Loss:  0.5270\n",
            "Epoch: 825 | Loss:  0.5268\n",
            "Epoch: 826 | Loss:  0.5266\n",
            "Epoch: 827 | Loss:  0.5264\n",
            "Epoch: 828 | Loss:  0.5263\n",
            "Epoch: 829 | Loss:  0.5261\n",
            "Epoch: 830 | Loss:  0.5259\n",
            "Epoch: 831 | Loss:  0.5257\n",
            "Epoch: 832 | Loss:  0.5255\n",
            "Epoch: 833 | Loss:  0.5253\n",
            "Epoch: 834 | Loss:  0.5251\n",
            "Epoch: 835 | Loss:  0.5250\n",
            "Epoch: 836 | Loss:  0.5248\n",
            "Epoch: 837 | Loss:  0.5246\n",
            "Epoch: 838 | Loss:  0.5244\n",
            "Epoch: 839 | Loss:  0.5242\n",
            "Epoch: 840 | Loss:  0.5240\n",
            "Epoch: 841 | Loss:  0.5239\n",
            "Epoch: 842 | Loss:  0.5237\n",
            "Epoch: 843 | Loss:  0.5235\n",
            "Epoch: 844 | Loss:  0.5233\n",
            "Epoch: 845 | Loss:  0.5231\n",
            "Epoch: 846 | Loss:  0.5229\n",
            "Epoch: 847 | Loss:  0.5228\n",
            "Epoch: 848 | Loss:  0.5226\n",
            "Epoch: 849 | Loss:  0.5224\n",
            "Epoch: 850 | Loss:  0.5222\n",
            "Epoch: 851 | Loss:  0.5220\n",
            "Epoch: 852 | Loss:  0.5218\n",
            "Epoch: 853 | Loss:  0.5217\n",
            "Epoch: 854 | Loss:  0.5215\n",
            "Epoch: 855 | Loss:  0.5213\n",
            "Epoch: 856 | Loss:  0.5211\n",
            "Epoch: 857 | Loss:  0.5209\n",
            "Epoch: 858 | Loss:  0.5207\n",
            "Epoch: 859 | Loss:  0.5206\n",
            "Epoch: 860 | Loss:  0.5204\n",
            "Epoch: 861 | Loss:  0.5202\n",
            "Epoch: 862 | Loss:  0.5200\n",
            "Epoch: 863 | Loss:  0.5198\n",
            "Epoch: 864 | Loss:  0.5197\n",
            "Epoch: 865 | Loss:  0.5195\n",
            "Epoch: 866 | Loss:  0.5193\n",
            "Epoch: 867 | Loss:  0.5191\n",
            "Epoch: 868 | Loss:  0.5189\n",
            "Epoch: 869 | Loss:  0.5188\n",
            "Epoch: 870 | Loss:  0.5186\n",
            "Epoch: 871 | Loss:  0.5184\n",
            "Epoch: 872 | Loss:  0.5182\n",
            "Epoch: 873 | Loss:  0.5180\n",
            "Epoch: 874 | Loss:  0.5179\n",
            "Epoch: 875 | Loss:  0.5177\n",
            "Epoch: 876 | Loss:  0.5175\n",
            "Epoch: 877 | Loss:  0.5173\n",
            "Epoch: 878 | Loss:  0.5171\n",
            "Epoch: 879 | Loss:  0.5170\n",
            "Epoch: 880 | Loss:  0.5168\n",
            "Epoch: 881 | Loss:  0.5166\n",
            "Epoch: 882 | Loss:  0.5164\n",
            "Epoch: 883 | Loss:  0.5162\n",
            "Epoch: 884 | Loss:  0.5161\n",
            "Epoch: 885 | Loss:  0.5159\n",
            "Epoch: 886 | Loss:  0.5157\n",
            "Epoch: 887 | Loss:  0.5155\n",
            "Epoch: 888 | Loss:  0.5154\n",
            "Epoch: 889 | Loss:  0.5152\n",
            "Epoch: 890 | Loss:  0.5150\n",
            "Epoch: 891 | Loss:  0.5148\n",
            "Epoch: 892 | Loss:  0.5146\n",
            "Epoch: 893 | Loss:  0.5145\n",
            "Epoch: 894 | Loss:  0.5143\n",
            "Epoch: 895 | Loss:  0.5141\n",
            "Epoch: 896 | Loss:  0.5139\n",
            "Epoch: 897 | Loss:  0.5138\n",
            "Epoch: 898 | Loss:  0.5136\n",
            "Epoch: 899 | Loss:  0.5134\n",
            "Epoch: 900 | Loss:  0.5132\n",
            "Epoch: 901 | Loss:  0.5130\n",
            "Epoch: 902 | Loss:  0.5129\n",
            "Epoch: 903 | Loss:  0.5127\n",
            "Epoch: 904 | Loss:  0.5125\n",
            "Epoch: 905 | Loss:  0.5123\n",
            "Epoch: 906 | Loss:  0.5122\n",
            "Epoch: 907 | Loss:  0.5120\n",
            "Epoch: 908 | Loss:  0.5118\n",
            "Epoch: 909 | Loss:  0.5116\n",
            "Epoch: 910 | Loss:  0.5115\n",
            "Epoch: 911 | Loss:  0.5113\n",
            "Epoch: 912 | Loss:  0.5111\n",
            "Epoch: 913 | Loss:  0.5109\n",
            "Epoch: 914 | Loss:  0.5108\n",
            "Epoch: 915 | Loss:  0.5106\n",
            "Epoch: 916 | Loss:  0.5104\n",
            "Epoch: 917 | Loss:  0.5102\n",
            "Epoch: 918 | Loss:  0.5101\n",
            "Epoch: 919 | Loss:  0.5099\n",
            "Epoch: 920 | Loss:  0.5097\n",
            "Epoch: 921 | Loss:  0.5095\n",
            "Epoch: 922 | Loss:  0.5094\n",
            "Epoch: 923 | Loss:  0.5092\n",
            "Epoch: 924 | Loss:  0.5090\n",
            "Epoch: 925 | Loss:  0.5088\n",
            "Epoch: 926 | Loss:  0.5087\n",
            "Epoch: 927 | Loss:  0.5085\n",
            "Epoch: 928 | Loss:  0.5083\n",
            "Epoch: 929 | Loss:  0.5082\n",
            "Epoch: 930 | Loss:  0.5080\n",
            "Epoch: 931 | Loss:  0.5078\n",
            "Epoch: 932 | Loss:  0.5076\n",
            "Epoch: 933 | Loss:  0.5075\n",
            "Epoch: 934 | Loss:  0.5073\n",
            "Epoch: 935 | Loss:  0.5071\n",
            "Epoch: 936 | Loss:  0.5069\n",
            "Epoch: 937 | Loss:  0.5068\n",
            "Epoch: 938 | Loss:  0.5066\n",
            "Epoch: 939 | Loss:  0.5064\n",
            "Epoch: 940 | Loss:  0.5063\n",
            "Epoch: 941 | Loss:  0.5061\n",
            "Epoch: 942 | Loss:  0.5059\n",
            "Epoch: 943 | Loss:  0.5057\n",
            "Epoch: 944 | Loss:  0.5056\n",
            "Epoch: 945 | Loss:  0.5054\n",
            "Epoch: 946 | Loss:  0.5052\n",
            "Epoch: 947 | Loss:  0.5051\n",
            "Epoch: 948 | Loss:  0.5049\n",
            "Epoch: 949 | Loss:  0.5047\n",
            "Epoch: 950 | Loss:  0.5045\n",
            "Epoch: 951 | Loss:  0.5044\n",
            "Epoch: 952 | Loss:  0.5042\n",
            "Epoch: 953 | Loss:  0.5040\n",
            "Epoch: 954 | Loss:  0.5039\n",
            "Epoch: 955 | Loss:  0.5037\n",
            "Epoch: 956 | Loss:  0.5035\n",
            "Epoch: 957 | Loss:  0.5034\n",
            "Epoch: 958 | Loss:  0.5032\n",
            "Epoch: 959 | Loss:  0.5030\n",
            "Epoch: 960 | Loss:  0.5028\n",
            "Epoch: 961 | Loss:  0.5027\n",
            "Epoch: 962 | Loss:  0.5025\n",
            "Epoch: 963 | Loss:  0.5023\n",
            "Epoch: 964 | Loss:  0.5022\n",
            "Epoch: 965 | Loss:  0.5020\n",
            "Epoch: 966 | Loss:  0.5018\n",
            "Epoch: 967 | Loss:  0.5017\n",
            "Epoch: 968 | Loss:  0.5015\n",
            "Epoch: 969 | Loss:  0.5013\n",
            "Epoch: 970 | Loss:  0.5012\n",
            "Epoch: 971 | Loss:  0.5010\n",
            "Epoch: 972 | Loss:  0.5008\n",
            "Epoch: 973 | Loss:  0.5007\n",
            "Epoch: 974 | Loss:  0.5005\n",
            "Epoch: 975 | Loss:  0.5003\n",
            "Epoch: 976 | Loss:  0.5001\n",
            "Epoch: 977 | Loss:  0.5000\n",
            "Epoch: 978 | Loss:  0.4998\n",
            "Epoch: 979 | Loss:  0.4996\n",
            "Epoch: 980 | Loss:  0.4995\n",
            "Epoch: 981 | Loss:  0.4993\n",
            "Epoch: 982 | Loss:  0.4991\n",
            "Epoch: 983 | Loss:  0.4990\n",
            "Epoch: 984 | Loss:  0.4988\n",
            "Epoch: 985 | Loss:  0.4986\n",
            "Epoch: 986 | Loss:  0.4985\n",
            "Epoch: 987 | Loss:  0.4983\n",
            "Epoch: 988 | Loss:  0.4981\n",
            "Epoch: 989 | Loss:  0.4980\n",
            "Epoch: 990 | Loss:  0.4978\n",
            "Epoch: 991 | Loss:  0.4976\n",
            "Epoch: 992 | Loss:  0.4975\n",
            "Epoch: 993 | Loss:  0.4973\n",
            "Epoch: 994 | Loss:  0.4971\n",
            "Epoch: 995 | Loss:  0.4970\n",
            "Epoch: 996 | Loss:  0.4968\n",
            "Epoch: 997 | Loss:  0.4967\n",
            "Epoch: 998 | Loss:  0.4965\n",
            "Epoch: 999 | Loss:  0.4963\n",
            "Epoch: 1000 | Loss:  0.4962\n",
            "Prediction after 1 hour of training: 0.4277 | Above 50%: False\n",
            "Prediction after 7 hour of training: 0.9538 | Above 50%: True\n",
            "Prediction after 20 hour of training: 1.0000 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmaXnPjMffEm"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "x_data=from_numpy(xy[:, 0:-1])\n",
        "y_data=from_numpy(xy[:, [-1]])\n",
        "print(y_data)\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIcIhfxTlBkk"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  y_pred=model(x_data)\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print(f'Epoch: {epoch+1}/100 | Loss: {loss.item(): .4}')\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqlcj2xglD9K"
      },
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/gdrive')\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy=np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/diabetes.csv.gz', delimiter=',', dtype=np.float32) \n",
        "    self.x_data=from_numpy(xy[:, 0:-1])\n",
        "    self.y_data=from_numpy(xy[:, [-1]])\n",
        "    self.len=xy.shape[0]\n",
        "\n",
        "    \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.len\n",
        "\n",
        "dataset=DiabetesDataset()\n",
        "train_loader=DataLoader(dataset= dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(8,6)\n",
        "    self.l2=nn.Linear(6,4)\n",
        "    self.l3=nn.Linear(4,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out1=self.relu(self.l1(x))\n",
        "    out2=self.relu(self.l2(out1))\n",
        "    y_pred=self.sigmoid(self.l3(out2))\n",
        "    return y_pred\n",
        "\n",
        "model=Model()\n",
        "criterion=nn.BCELoss(reduction='mean')\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader,0):\n",
        "    inputs,labels=data\n",
        "    y_pred=model(inputs)\n",
        "    loss=criterion(y_pred,labels)\n",
        "    print(f'Epoch{epoch+1} | Batch: {i+1} Loss: {loss.item(): .4f}')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}