{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjflogs05nrpWZ9GDf2X6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDED00R/machinelearning/blob/main/deep3\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfM6KoVwoEb-",
        "outputId": "6eb4d3b2-8f45-4034-d9c7-499be872079c"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data=[1.0,2.0,3.0]\n",
        "y_data=[2.0,4.0,6.0]\n",
        "\n",
        "w=torch.tensor([1.0], requires_grad=True)\n",
        "def forward(x):\n",
        "  return x*w\n",
        "def loss(x,y):\n",
        "  y_pred=forward(x)\n",
        "  return (y_pred-y)**2\n",
        "print(\"predic\",4,forward(4).item())\n",
        "for epoch in range(10):\n",
        "  for x_val,y_val in zip(x_data,y_data):\n",
        "    y_pred=forward(x_val)\n",
        "    l=loss(x_val,y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad\",x_val,y_val,w.grad.item())\n",
        "    w.data=w.data-0.01*w.grad.item()\n",
        "    w.grad.data.zero_()\n",
        "  print(f\"epoch: {epoch}| loss: {l.item()}\")\n",
        "print(\"prediction\",4,forward(4).item())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predic 4 4.0\n",
            "\tgrad 1.0 2.0 -2.0\n",
            "\tgrad 2.0 4.0 -60.23629379272461\n",
            "\tgrad 3.0 6.0 -33.59331130981445\n",
            "epoch: 0| loss: 2.114347219467163\n",
            "\tgrad 1.0 2.0 -0.08340811729431152\n",
            "\tgrad 2.0 4.0 -0.004369103349745274\n",
            "\tgrad 3.0 6.0 -0.0004961026716046035\n",
            "epoch: 1| loss: 3.3756609809643123e-06\n",
            "\tgrad 1.0 2.0 -0.08164238929748535\n",
            "\tgrad 2.0 4.0 -0.004097453318536282\n",
            "\tgrad 3.0 6.0 -0.0004458701878320426\n",
            "epoch: 2| loss: 2.969772140204441e-06\n",
            "\tgrad 1.0 2.0 -0.07991862297058105\n",
            "\tgrad 2.0 4.0 -0.0038433719892054796\n",
            "\tgrad 3.0 6.0 -0.00040084103238768876\n",
            "epoch: 3| loss: 2.6136028736800654e-06\n",
            "\tgrad 1.0 2.0 -0.07823538780212402\n",
            "\tgrad 2.0 4.0 -0.0036055983509868383\n",
            "\tgrad 3.0 6.0 -0.0003604372905101627\n",
            "epoch: 4| loss: 2.300745791217196e-06\n",
            "\tgrad 1.0 2.0 -0.07659149169921875\n",
            "\tgrad 2.0 4.0 -0.0033830595202744007\n",
            "\tgrad 3.0 6.0 -0.0003241899248678237\n",
            "epoch: 5| loss: 2.0259669781808043e-06\n",
            "\tgrad 1.0 2.0 -0.07498550415039062\n",
            "\tgrad 2.0 4.0 -0.003174689132720232\n",
            "\tgrad 3.0 6.0 -0.000291657168418169\n",
            "epoch: 6| loss: 1.7845147795014782e-06\n",
            "\tgrad 1.0 2.0 -0.07341670989990234\n",
            "\tgrad 2.0 4.0 -0.002979554934427142\n",
            "\tgrad 3.0 6.0 -0.00026243942556902766\n",
            "epoch: 7| loss: 1.5722004036433646e-06\n",
            "\tgrad 1.0 2.0 -0.0718834400177002\n",
            "\tgrad 2.0 4.0 -0.00279675773344934\n",
            "\tgrad 3.0 6.0 -0.0002361977967666462\n",
            "epoch: 8| loss: 1.385491941618966e-06\n",
            "\tgrad 1.0 2.0 -0.07038497924804688\n",
            "\tgrad 2.0 4.0 -0.0026254854165017605\n",
            "\tgrad 3.0 6.0 -0.0002126227191183716\n",
            "epoch: 9| loss: 1.2212499314046e-06\n",
            "prediction 4 7.86215877532959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2kr6GsEubYg",
        "outputId": "89a6f389-4048-4595-c343-824280c91cae"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "x_data=tensor([[1.0],[2.0],[3.0]])\n",
        "y_data=tensor([[2.0],[4.0],[6.0]])\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.linear=torch.nn.Linear(1,1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    y_pred=self.linear(x)\n",
        "    return y_pred\n",
        "model=Model()\n",
        "criterion=torch.nn.MSELoss(reduction='sum')\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "    y_pred=model(x_data)\n",
        "    loss=criterion(y_pred,y_data)\n",
        "    print(f\"epoch: {epoch}| loss: {loss.item()}\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "hour_var=tensor([4.0])\n",
        "y_pred=model(hour_var)\n",
        "print(\"prediction\",4,forward(hour_var).item())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0| loss: 10.273796081542969\n",
            "epoch: 1| loss: 4.678962707519531\n",
            "epoch: 2| loss: 2.1867868900299072\n",
            "epoch: 3| loss: 1.0758492946624756\n",
            "epoch: 4| loss: 0.5798197984695435\n",
            "epoch: 5| loss: 0.3575515151023865\n",
            "epoch: 6| loss: 0.2571754455566406\n",
            "epoch: 7| loss: 0.2110823392868042\n",
            "epoch: 8| loss: 0.18917477130889893\n",
            "epoch: 9| loss: 0.17805354297161102\n",
            "epoch: 10| loss: 0.17175427079200745\n",
            "epoch: 11| loss: 0.16762076318264008\n",
            "epoch: 12| loss: 0.16447049379348755\n",
            "epoch: 13| loss: 0.16177679598331451\n",
            "epoch: 14| loss: 0.15930509567260742\n",
            "epoch: 15| loss: 0.15695028007030487\n",
            "epoch: 16| loss: 0.1546655148267746\n",
            "epoch: 17| loss: 0.15242955088615417\n",
            "epoch: 18| loss: 0.1502334177494049\n",
            "epoch: 19| loss: 0.14807167649269104\n",
            "epoch: 20| loss: 0.1459425985813141\n",
            "epoch: 21| loss: 0.14384444057941437\n",
            "epoch: 22| loss: 0.1417769193649292\n",
            "epoch: 23| loss: 0.13973931968212128\n",
            "epoch: 24| loss: 0.13773107528686523\n",
            "epoch: 25| loss: 0.13575157523155212\n",
            "epoch: 26| loss: 0.13380084931850433\n",
            "epoch: 27| loss: 0.13187777996063232\n",
            "epoch: 28| loss: 0.12998251616954803\n",
            "epoch: 29| loss: 0.12811435759067535\n",
            "epoch: 30| loss: 0.12627317011356354\n",
            "epoch: 31| loss: 0.12445856630802155\n",
            "epoch: 32| loss: 0.12266954779624939\n",
            "epoch: 33| loss: 0.12090666592121124\n",
            "epoch: 34| loss: 0.11916908621788025\n",
            "epoch: 35| loss: 0.11745651811361313\n",
            "epoch: 36| loss: 0.11576839536428452\n",
            "epoch: 37| loss: 0.11410462856292725\n",
            "epoch: 38| loss: 0.11246465891599655\n",
            "epoch: 39| loss: 0.11084847152233124\n",
            "epoch: 40| loss: 0.10925543308258057\n",
            "epoch: 41| loss: 0.10768518596887589\n",
            "epoch: 42| loss: 0.10613763332366943\n",
            "epoch: 43| loss: 0.10461224615573883\n",
            "epoch: 44| loss: 0.10310877859592438\n",
            "epoch: 45| loss: 0.10162699967622757\n",
            "epoch: 46| loss: 0.10016655921936035\n",
            "epoch: 47| loss: 0.09872690588235855\n",
            "epoch: 48| loss: 0.09730793535709381\n",
            "epoch: 49| loss: 0.09590966999530792\n",
            "epoch: 50| loss: 0.09453125298023224\n",
            "epoch: 51| loss: 0.09317274391651154\n",
            "epoch: 52| loss: 0.09183353185653687\n",
            "epoch: 53| loss: 0.09051382541656494\n",
            "epoch: 54| loss: 0.08921291679143906\n",
            "epoch: 55| loss: 0.08793078362941742\n",
            "epoch: 56| loss: 0.08666728436946869\n",
            "epoch: 57| loss: 0.0854216068983078\n",
            "epoch: 58| loss: 0.08419398218393326\n",
            "epoch: 59| loss: 0.08298388868570328\n",
            "epoch: 60| loss: 0.0817914679646492\n",
            "epoch: 61| loss: 0.08061586320400238\n",
            "epoch: 62| loss: 0.07945743948221207\n",
            "epoch: 63| loss: 0.07831530272960663\n",
            "epoch: 64| loss: 0.07718975841999054\n",
            "epoch: 65| loss: 0.0760805532336235\n",
            "epoch: 66| loss: 0.07498705387115479\n",
            "epoch: 67| loss: 0.07390927523374557\n",
            "epoch: 68| loss: 0.07284732162952423\n",
            "epoch: 69| loss: 0.0718003585934639\n",
            "epoch: 70| loss: 0.0707683339715004\n",
            "epoch: 71| loss: 0.06975127011537552\n",
            "epoch: 72| loss: 0.06874901801347733\n",
            "epoch: 73| loss: 0.06776095926761627\n",
            "epoch: 74| loss: 0.06678705662488937\n",
            "epoch: 75| loss: 0.06582721322774887\n",
            "epoch: 76| loss: 0.06488116830587387\n",
            "epoch: 77| loss: 0.06394867599010468\n",
            "epoch: 78| loss: 0.0630296990275383\n",
            "epoch: 79| loss: 0.0621238648891449\n",
            "epoch: 80| loss: 0.06123103201389313\n",
            "epoch: 81| loss: 0.06035099923610687\n",
            "epoch: 82| loss: 0.05948376655578613\n",
            "epoch: 83| loss: 0.058628812432289124\n",
            "epoch: 84| loss: 0.05778630077838898\n",
            "epoch: 85| loss: 0.056955814361572266\n",
            "epoch: 86| loss: 0.056137219071388245\n",
            "epoch: 87| loss: 0.05533038079738617\n",
            "epoch: 88| loss: 0.05453524738550186\n",
            "epoch: 89| loss: 0.05375147610902786\n",
            "epoch: 90| loss: 0.05297902226448059\n",
            "epoch: 91| loss: 0.05221760645508766\n",
            "epoch: 92| loss: 0.051467232406139374\n",
            "epoch: 93| loss: 0.05072755366563797\n",
            "epoch: 94| loss: 0.04999848082661629\n",
            "epoch: 95| loss: 0.04927992820739746\n",
            "epoch: 96| loss: 0.04857168346643448\n",
            "epoch: 97| loss: 0.047873660922050476\n",
            "epoch: 98| loss: 0.047185685485601425\n",
            "epoch: 99| loss: 0.04650744050741196\n",
            "epoch: 100| loss: 0.045839130878448486\n",
            "epoch: 101| loss: 0.04518026113510132\n",
            "epoch: 102| loss: 0.04453098773956299\n",
            "epoch: 103| loss: 0.043891068547964096\n",
            "epoch: 104| loss: 0.04326026886701584\n",
            "epoch: 105| loss: 0.04263850301504135\n",
            "epoch: 106| loss: 0.04202571511268616\n",
            "epoch: 107| loss: 0.04142172262072563\n",
            "epoch: 108| loss: 0.04082643985748291\n",
            "epoch: 109| loss: 0.04023976996541023\n",
            "epoch: 110| loss: 0.039661455899477005\n",
            "epoch: 111| loss: 0.03909146785736084\n",
            "epoch: 112| loss: 0.03852967917919159\n",
            "epoch: 113| loss: 0.037975944578647614\n",
            "epoch: 114| loss: 0.03743017837405205\n",
            "epoch: 115| loss: 0.036892205476760864\n",
            "epoch: 116| loss: 0.03636201471090317\n",
            "epoch: 117| loss: 0.035839371383190155\n",
            "epoch: 118| loss: 0.035324327647686005\n",
            "epoch: 119| loss: 0.034816667437553406\n",
            "epoch: 120| loss: 0.03431630879640579\n",
            "epoch: 121| loss: 0.03382309526205063\n",
            "epoch: 122| loss: 0.03333698958158493\n",
            "epoch: 123| loss: 0.032857973128557205\n",
            "epoch: 124| loss: 0.03238571435213089\n",
            "epoch: 125| loss: 0.031920261681079865\n",
            "epoch: 126| loss: 0.03146151080727577\n",
            "epoch: 127| loss: 0.031009437516331673\n",
            "epoch: 128| loss: 0.030563702806830406\n",
            "epoch: 129| loss: 0.030124478042125702\n",
            "epoch: 130| loss: 0.029691506177186966\n",
            "epoch: 131| loss: 0.029264893382787704\n",
            "epoch: 132| loss: 0.028844298794865608\n",
            "epoch: 133| loss: 0.028429660946130753\n",
            "epoch: 134| loss: 0.028021102771162987\n",
            "epoch: 135| loss: 0.027618464082479477\n",
            "epoch: 136| loss: 0.027221467345952988\n",
            "epoch: 137| loss: 0.026830369606614113\n",
            "epoch: 138| loss: 0.02644466608762741\n",
            "epoch: 139| loss: 0.026064645498991013\n",
            "epoch: 140| loss: 0.025689994916319847\n",
            "epoch: 141| loss: 0.025320840999484062\n",
            "epoch: 142| loss: 0.024956895038485527\n",
            "epoch: 143| loss: 0.02459832839667797\n",
            "epoch: 144| loss: 0.0242447629570961\n",
            "epoch: 145| loss: 0.02389625646173954\n",
            "epoch: 146| loss: 0.023552849888801575\n",
            "epoch: 147| loss: 0.02321440353989601\n",
            "epoch: 148| loss: 0.02288077026605606\n",
            "epoch: 149| loss: 0.022551901638507843\n",
            "epoch: 150| loss: 0.022227847948670387\n",
            "epoch: 151| loss: 0.02190835401415825\n",
            "epoch: 152| loss: 0.021593531593680382\n",
            "epoch: 153| loss: 0.021283205598592758\n",
            "epoch: 154| loss: 0.020977310836315155\n",
            "epoch: 155| loss: 0.020675834268331528\n",
            "epoch: 156| loss: 0.020378658547997475\n",
            "epoch: 157| loss: 0.020085826516151428\n",
            "epoch: 158| loss: 0.01979714073240757\n",
            "epoch: 159| loss: 0.019512666389346123\n",
            "epoch: 160| loss: 0.019232265651226044\n",
            "epoch: 161| loss: 0.018955767154693604\n",
            "epoch: 162| loss: 0.01868332177400589\n",
            "epoch: 163| loss: 0.018414899706840515\n",
            "epoch: 164| loss: 0.01815022900700569\n",
            "epoch: 165| loss: 0.017889324575662613\n",
            "epoch: 166| loss: 0.01763230189681053\n",
            "epoch: 167| loss: 0.017378833144903183\n",
            "epoch: 168| loss: 0.017129147425293922\n",
            "epoch: 169| loss: 0.016882944852113724\n",
            "epoch: 170| loss: 0.016640271991491318\n",
            "epoch: 171| loss: 0.016401203349232674\n",
            "epoch: 172| loss: 0.016165513545274734\n",
            "epoch: 173| loss: 0.015933150425553322\n",
            "epoch: 174| loss: 0.015704138204455376\n",
            "epoch: 175| loss: 0.015478434041142464\n",
            "epoch: 176| loss: 0.01525597833096981\n",
            "epoch: 177| loss: 0.01503673754632473\n",
            "epoch: 178| loss: 0.01482066884636879\n",
            "epoch: 179| loss: 0.014607639983296394\n",
            "epoch: 180| loss: 0.01439770869910717\n",
            "epoch: 181| loss: 0.01419074460864067\n",
            "epoch: 182| loss: 0.01398684736341238\n",
            "epoch: 183| loss: 0.013785824179649353\n",
            "epoch: 184| loss: 0.013587740249931812\n",
            "epoch: 185| loss: 0.013392436318099499\n",
            "epoch: 186| loss: 0.013199947774410248\n",
            "epoch: 187| loss: 0.013010228052735329\n",
            "epoch: 188| loss: 0.012823320925235748\n",
            "epoch: 189| loss: 0.01263895072042942\n",
            "epoch: 190| loss: 0.01245734840631485\n",
            "epoch: 191| loss: 0.012278278358280659\n",
            "epoch: 192| loss: 0.012101887725293636\n",
            "epoch: 193| loss: 0.01192795392125845\n",
            "epoch: 194| loss: 0.011756500229239464\n",
            "epoch: 195| loss: 0.011587566696107388\n",
            "epoch: 196| loss: 0.01142096146941185\n",
            "epoch: 197| loss: 0.011256839148700237\n",
            "epoch: 198| loss: 0.011095061898231506\n",
            "epoch: 199| loss: 0.010935635305941105\n",
            "epoch: 200| loss: 0.01077849231660366\n",
            "epoch: 201| loss: 0.0106235696002841\n",
            "epoch: 202| loss: 0.010470864363014698\n",
            "epoch: 203| loss: 0.010320387780666351\n",
            "epoch: 204| loss: 0.010172118432819843\n",
            "epoch: 205| loss: 0.010025895200669765\n",
            "epoch: 206| loss: 0.009881793521344662\n",
            "epoch: 207| loss: 0.009739779867231846\n",
            "epoch: 208| loss: 0.00959980208426714\n",
            "epoch: 209| loss: 0.009461821988224983\n",
            "epoch: 210| loss: 0.009325878694653511\n",
            "epoch: 211| loss: 0.009191805496811867\n",
            "epoch: 212| loss: 0.009059727191925049\n",
            "epoch: 213| loss: 0.008929550647735596\n",
            "epoch: 214| loss: 0.00880117155611515\n",
            "epoch: 215| loss: 0.00867469608783722\n",
            "epoch: 216| loss: 0.00855001900345087\n",
            "epoch: 217| loss: 0.008427167311310768\n",
            "epoch: 218| loss: 0.00830603763461113\n",
            "epoch: 219| loss: 0.00818667747080326\n",
            "epoch: 220| loss: 0.008069012314081192\n",
            "epoch: 221| loss: 0.007953044027090073\n",
            "epoch: 222| loss: 0.007838749326765537\n",
            "epoch: 223| loss: 0.007726104464381933\n",
            "epoch: 224| loss: 0.007615091744810343\n",
            "epoch: 225| loss: 0.0075056585483253\n",
            "epoch: 226| loss: 0.007397737354040146\n",
            "epoch: 227| loss: 0.007291471119970083\n",
            "epoch: 228| loss: 0.0071866982616484165\n",
            "epoch: 229| loss: 0.0070833442732691765\n",
            "epoch: 230| loss: 0.006981580052524805\n",
            "epoch: 231| loss: 0.006881241220980883\n",
            "epoch: 232| loss: 0.006782371085137129\n",
            "epoch: 233| loss: 0.006684896536171436\n",
            "epoch: 234| loss: 0.006588819902390242\n",
            "epoch: 235| loss: 0.006494124419987202\n",
            "epoch: 236| loss: 0.0064007556065917015\n",
            "epoch: 237| loss: 0.006308793090283871\n",
            "epoch: 238| loss: 0.00621811393648386\n",
            "epoch: 239| loss: 0.0061287470161914825\n",
            "epoch: 240| loss: 0.006040660198777914\n",
            "epoch: 241| loss: 0.005953838583081961\n",
            "epoch: 242| loss: 0.00586828775703907\n",
            "epoch: 243| loss: 0.0057839518412947655\n",
            "epoch: 244| loss: 0.005700823850929737\n",
            "epoch: 245| loss: 0.005618899129331112\n",
            "epoch: 246| loss: 0.005538164637982845\n",
            "epoch: 247| loss: 0.005458589643239975\n",
            "epoch: 248| loss: 0.005380123388022184\n",
            "epoch: 249| loss: 0.005302793346345425\n",
            "epoch: 250| loss: 0.005226559937000275\n",
            "epoch: 251| loss: 0.005151480436325073\n",
            "epoch: 252| loss: 0.005077441222965717\n",
            "epoch: 253| loss: 0.00500447629019618\n",
            "epoch: 254| loss: 0.004932566545903683\n",
            "epoch: 255| loss: 0.00486165564507246\n",
            "epoch: 256| loss: 0.004791777115315199\n",
            "epoch: 257| loss: 0.004722936078906059\n",
            "epoch: 258| loss: 0.004655045457184315\n",
            "epoch: 259| loss: 0.004588142037391663\n",
            "epoch: 260| loss: 0.004522216506302357\n",
            "epoch: 261| loss: 0.004457209724932909\n",
            "epoch: 262| loss: 0.004393159411847591\n",
            "epoch: 263| loss: 0.004329997580498457\n",
            "epoch: 264| loss: 0.004267786629498005\n",
            "epoch: 265| loss: 0.004206440411508083\n",
            "epoch: 266| loss: 0.004146014340221882\n",
            "epoch: 267| loss: 0.004086407832801342\n",
            "epoch: 268| loss: 0.004027683287858963\n",
            "epoch: 269| loss: 0.003969827201217413\n",
            "epoch: 270| loss: 0.003912757150828838\n",
            "epoch: 271| loss: 0.0038565327413380146\n",
            "epoch: 272| loss: 0.0038010957650840282\n",
            "epoch: 273| loss: 0.003746483940631151\n",
            "epoch: 274| loss: 0.0036926448810845613\n",
            "epoch: 275| loss: 0.0036395746283233166\n",
            "epoch: 276| loss: 0.003587243612855673\n",
            "epoch: 277| loss: 0.0035356853622943163\n",
            "epoch: 278| loss: 0.0034848679788410664\n",
            "epoch: 279| loss: 0.003434806829318404\n",
            "epoch: 280| loss: 0.003385459538549185\n",
            "epoch: 281| loss: 0.0033368119038641453\n",
            "epoch: 282| loss: 0.003288830164819956\n",
            "epoch: 283| loss: 0.003241556463763118\n",
            "epoch: 284| loss: 0.0031949602998793125\n",
            "epoch: 285| loss: 0.0031490828841924667\n",
            "epoch: 286| loss: 0.0031038259621709585\n",
            "epoch: 287| loss: 0.0030591986142098904\n",
            "epoch: 288| loss: 0.0030152362305670977\n",
            "epoch: 289| loss: 0.002971902722492814\n",
            "epoch: 290| loss: 0.002929181791841984\n",
            "epoch: 291| loss: 0.0028871011454612017\n",
            "epoch: 292| loss: 0.0028456011787056923\n",
            "epoch: 293| loss: 0.002804697258397937\n",
            "epoch: 294| loss: 0.002764388918876648\n",
            "epoch: 295| loss: 0.0027246910613030195\n",
            "epoch: 296| loss: 0.002685509156435728\n",
            "epoch: 297| loss: 0.0026469165459275246\n",
            "epoch: 298| loss: 0.002608879003673792\n",
            "epoch: 299| loss: 0.002571370452642441\n",
            "epoch: 300| loss: 0.002534405793994665\n",
            "epoch: 301| loss: 0.002498005283996463\n",
            "epoch: 302| loss: 0.002462110249325633\n",
            "epoch: 303| loss: 0.0024267041590064764\n",
            "epoch: 304| loss: 0.0023918617516756058\n",
            "epoch: 305| loss: 0.002357471501454711\n",
            "epoch: 306| loss: 0.002323591150343418\n",
            "epoch: 307| loss: 0.002290198113769293\n",
            "epoch: 308| loss: 0.002257273066788912\n",
            "epoch: 309| loss: 0.002224855124950409\n",
            "epoch: 310| loss: 0.0021928816568106413\n",
            "epoch: 311| loss: 0.0021613528952002525\n",
            "epoch: 312| loss: 0.002130283508449793\n",
            "epoch: 313| loss: 0.0020996704697608948\n",
            "epoch: 314| loss: 0.0020695175044238567\n",
            "epoch: 315| loss: 0.0020397582557052374\n",
            "epoch: 316| loss: 0.0020104493014514446\n",
            "epoch: 317| loss: 0.001981559209525585\n",
            "epoch: 318| loss: 0.001953058410435915\n",
            "epoch: 319| loss: 0.0019250138429924846\n",
            "epoch: 320| loss: 0.0018973343539983034\n",
            "epoch: 321| loss: 0.0018700839718803763\n",
            "epoch: 322| loss: 0.001843185629695654\n",
            "epoch: 323| loss: 0.0018166926456615329\n",
            "epoch: 324| loss: 0.001790603157132864\n",
            "epoch: 325| loss: 0.001764863496646285\n",
            "epoch: 326| loss: 0.0017395138274878263\n",
            "epoch: 327| loss: 0.0017145040910691023\n",
            "epoch: 328| loss: 0.0016898524481803179\n",
            "epoch: 329| loss: 0.001665583229623735\n",
            "epoch: 330| loss: 0.0016416357830166817\n",
            "epoch: 331| loss: 0.0016180463135242462\n",
            "epoch: 332| loss: 0.0015947994543239474\n",
            "epoch: 333| loss: 0.0015718545764684677\n",
            "epoch: 334| loss: 0.0015492658130824566\n",
            "epoch: 335| loss: 0.0015270109288394451\n",
            "epoch: 336| loss: 0.0015050547663122416\n",
            "epoch: 337| loss: 0.00148345238994807\n",
            "epoch: 338| loss: 0.0014621128793805838\n",
            "epoch: 339| loss: 0.001441104570403695\n",
            "epoch: 340| loss: 0.0014203819446265697\n",
            "epoch: 341| loss: 0.0013999659568071365\n",
            "epoch: 342| loss: 0.0013798683648929\n",
            "epoch: 343| loss: 0.001360023394227028\n",
            "epoch: 344| loss: 0.001340492395684123\n",
            "epoch: 345| loss: 0.001321221119724214\n",
            "epoch: 346| loss: 0.001302225748077035\n",
            "epoch: 347| loss: 0.0012835095403715968\n",
            "epoch: 348| loss: 0.0012650631833821535\n",
            "epoch: 349| loss: 0.0012468816712498665\n",
            "epoch: 350| loss: 0.0012289786245673895\n",
            "epoch: 351| loss: 0.0012112982803955674\n",
            "epoch: 352| loss: 0.0011938965180888772\n",
            "epoch: 353| loss: 0.001176738296635449\n",
            "epoch: 354| loss: 0.0011598232667893171\n",
            "epoch: 355| loss: 0.0011431602761149406\n",
            "epoch: 356| loss: 0.0011267097434028983\n",
            "epoch: 357| loss: 0.0011105312732979655\n",
            "epoch: 358| loss: 0.0010945757385343313\n",
            "epoch: 359| loss: 0.0010788331273943186\n",
            "epoch: 360| loss: 0.0010633390629664063\n",
            "epoch: 361| loss: 0.001048045582138002\n",
            "epoch: 362| loss: 0.0010329918004572392\n",
            "epoch: 363| loss: 0.001018136041238904\n",
            "epoch: 364| loss: 0.0010035185841843486\n",
            "epoch: 365| loss: 0.0009890891378745437\n",
            "epoch: 366| loss: 0.0009748766897246242\n",
            "epoch: 367| loss: 0.0009608608670532703\n",
            "epoch: 368| loss: 0.0009470646036788821\n",
            "epoch: 369| loss: 0.0009334444766864181\n",
            "epoch: 370| loss: 0.0009200323838740587\n",
            "epoch: 371| loss: 0.0009068084182217717\n",
            "epoch: 372| loss: 0.0008937722304835916\n",
            "epoch: 373| loss: 0.0008809461141936481\n",
            "epoch: 374| loss: 0.0008682640618644655\n",
            "epoch: 375| loss: 0.0008557862602174282\n",
            "epoch: 376| loss: 0.0008435017080046237\n",
            "epoch: 377| loss: 0.0008313697762787342\n",
            "epoch: 378| loss: 0.0008194237598218024\n",
            "epoch: 379| loss: 0.0008076519006863236\n",
            "epoch: 380| loss: 0.0007960489019751549\n",
            "epoch: 381| loss: 0.0007846030639484525\n",
            "epoch: 382| loss: 0.0007733242819085717\n",
            "epoch: 383| loss: 0.0007622120901942253\n",
            "epoch: 384| loss: 0.0007512664888054132\n",
            "epoch: 385| loss: 0.0007404680363833904\n",
            "epoch: 386| loss: 0.0007298153941519558\n",
            "epoch: 387| loss: 0.0007193395867943764\n",
            "epoch: 388| loss: 0.0007089906721375883\n",
            "epoch: 389| loss: 0.0006987997330725193\n",
            "epoch: 390| loss: 0.0006887634517624974\n",
            "epoch: 391| loss: 0.0006788664613850415\n",
            "epoch: 392| loss: 0.0006690981681458652\n",
            "epoch: 393| loss: 0.0006594888400286436\n",
            "epoch: 394| loss: 0.0006500171730294824\n",
            "epoch: 395| loss: 0.000640671409200877\n",
            "epoch: 396| loss: 0.0006314663914963603\n",
            "epoch: 397| loss: 0.00062238157261163\n",
            "epoch: 398| loss: 0.0006134390132501721\n",
            "epoch: 399| loss: 0.0006046225898899138\n",
            "epoch: 400| loss: 0.0005959267728030682\n",
            "epoch: 401| loss: 0.0005873756599612534\n",
            "epoch: 402| loss: 0.0005789409624412656\n",
            "epoch: 403| loss: 0.0005706171505153179\n",
            "epoch: 404| loss: 0.0005624047480523586\n",
            "epoch: 405| loss: 0.0005543380393646657\n",
            "epoch: 406| loss: 0.0005463744746521115\n",
            "epoch: 407| loss: 0.0005385103868320584\n",
            "epoch: 408| loss: 0.0005307755782268941\n",
            "epoch: 409| loss: 0.0005231486284174025\n",
            "epoch: 410| loss: 0.0005156337283551693\n",
            "epoch: 411| loss: 0.0005082176066935062\n",
            "epoch: 412| loss: 0.0005009086453355849\n",
            "epoch: 413| loss: 0.0004937094636261463\n",
            "epoch: 414| loss: 0.00048661878099665046\n",
            "epoch: 415| loss: 0.0004796189023181796\n",
            "epoch: 416| loss: 0.0004727253399323672\n",
            "epoch: 417| loss: 0.0004659293917939067\n",
            "epoch: 418| loss: 0.0004592387122102082\n",
            "epoch: 419| loss: 0.00045264343498274684\n",
            "epoch: 420| loss: 0.00044613148202188313\n",
            "epoch: 421| loss: 0.00043971912236884236\n",
            "epoch: 422| loss: 0.0004334016120992601\n",
            "epoch: 423| loss: 0.0004271754587534815\n",
            "epoch: 424| loss: 0.00042103492887690663\n",
            "epoch: 425| loss: 0.00041498502832837403\n",
            "epoch: 426| loss: 0.0004090247966814786\n",
            "epoch: 427| loss: 0.000403140380512923\n",
            "epoch: 428| loss: 0.00039734141319058836\n",
            "epoch: 429| loss: 0.00039163746987469494\n",
            "epoch: 430| loss: 0.0003860112919937819\n",
            "epoch: 431| loss: 0.0003804552834481001\n",
            "epoch: 432| loss: 0.0003749924944713712\n",
            "epoch: 433| loss: 0.0003696005151141435\n",
            "epoch: 434| loss: 0.00036429191823117435\n",
            "epoch: 435| loss: 0.0003590526175685227\n",
            "epoch: 436| loss: 0.0003538906457833946\n",
            "epoch: 437| loss: 0.00034880946623161435\n",
            "epoch: 438| loss: 0.0003437984560150653\n",
            "epoch: 439| loss: 0.0003388601471669972\n",
            "epoch: 440| loss: 0.00033398193772882223\n",
            "epoch: 441| loss: 0.00032919502700679004\n",
            "epoch: 442| loss: 0.0003244594263378531\n",
            "epoch: 443| loss: 0.00031978724291548133\n",
            "epoch: 444| loss: 0.00031519850017502904\n",
            "epoch: 445| loss: 0.0003106695367023349\n",
            "epoch: 446| loss: 0.00030619680183008313\n",
            "epoch: 447| loss: 0.0003018039569724351\n",
            "epoch: 448| loss: 0.00029746489599347115\n",
            "epoch: 449| loss: 0.0002931930939666927\n",
            "epoch: 450| loss: 0.0002889759198296815\n",
            "epoch: 451| loss: 0.0002848187286872417\n",
            "epoch: 452| loss: 0.00028072576969861984\n",
            "epoch: 453| loss: 0.00027669916744343936\n",
            "epoch: 454| loss: 0.0002727170067373663\n",
            "epoch: 455| loss: 0.00026880091172643006\n",
            "epoch: 456| loss: 0.00026493112090975046\n",
            "epoch: 457| loss: 0.00026113350759260356\n",
            "epoch: 458| loss: 0.00025738292606547475\n",
            "epoch: 459| loss: 0.000253674341365695\n",
            "epoch: 460| loss: 0.00025003487826325\n",
            "epoch: 461| loss: 0.0002464327262714505\n",
            "epoch: 462| loss: 0.0002429001615382731\n",
            "epoch: 463| loss: 0.000239405722822994\n",
            "epoch: 464| loss: 0.0002359598147450015\n",
            "epoch: 465| loss: 0.00023256991698872298\n",
            "epoch: 466| loss: 0.00022923624783288687\n",
            "epoch: 467| loss: 0.00022593012545257807\n",
            "epoch: 468| loss: 0.00022269142209552228\n",
            "epoch: 469| loss: 0.0002194872940890491\n",
            "epoch: 470| loss: 0.00021633118740282953\n",
            "epoch: 471| loss: 0.00021322423708625138\n",
            "epoch: 472| loss: 0.00021016337268520147\n",
            "epoch: 473| loss: 0.00020713804406113923\n",
            "epoch: 474| loss: 0.00020415954350028187\n",
            "epoch: 475| loss: 0.00020122902060393244\n",
            "epoch: 476| loss: 0.00019833442638628185\n",
            "epoch: 477| loss: 0.0001954876206582412\n",
            "epoch: 478| loss: 0.00019267834431957453\n",
            "epoch: 479| loss: 0.00018990301759913564\n",
            "epoch: 480| loss: 0.00018717488273978233\n",
            "epoch: 481| loss: 0.00018448781338520348\n",
            "epoch: 482| loss: 0.00018184131477028131\n",
            "epoch: 483| loss: 0.00017922642291523516\n",
            "epoch: 484| loss: 0.0001766528730513528\n",
            "epoch: 485| loss: 0.0001741094165481627\n",
            "epoch: 486| loss: 0.00017161259893327951\n",
            "epoch: 487| loss: 0.0001691459328867495\n",
            "epoch: 488| loss: 0.00016671288176439703\n",
            "epoch: 489| loss: 0.0001643145951675251\n",
            "epoch: 490| loss: 0.0001619573449715972\n",
            "epoch: 491| loss: 0.00015963256009854376\n",
            "epoch: 492| loss: 0.00015733482723589987\n",
            "epoch: 493| loss: 0.00015506964700762182\n",
            "epoch: 494| loss: 0.00015284246183000505\n",
            "epoch: 495| loss: 0.00015064851322676986\n",
            "epoch: 496| loss: 0.00014848035061731935\n",
            "epoch: 497| loss: 0.00014634548278991133\n",
            "epoch: 498| loss: 0.0001442393404431641\n",
            "epoch: 499| loss: 0.00014216857380233705\n",
            "prediction 4 7.86215877532959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3zvxmh30hS3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
